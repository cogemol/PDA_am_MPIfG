{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('.venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "4fac973d8b48a5fcf37e7d133428a31fb47ebbd054f5d1feed8c0da486f2af46"
   }
  },
  "interpreter": {
   "hash": "0f893b9d4dd5ce6ef4bac37f57da8242e68fab57158b8f535387ec8494938c81"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Buchhandelsdaten in Vufind als Grundlage für PDA (Patron Driven Aquisition) am MPIfG\r\n",
    "\r\n",
    "Einbindung von freundlicherweise von Schweitzer Fachinformation zur Verfügung gestellten Daten.   \r\n",
    "(Auswahl über passend konfigurierte Neuerscheinungsabfragen in unserem Kundenprofil).   \r\n",
    "\r\n",
    "Das Jupyter Notebook arbeitet mit Python 3.8.10 und wurde mit Visual Studio Code 1.58.2 erstellt \r\n",
    "\r\n",
    "\r\n",
    "#### Arbeitsschritte im Code:\r\n",
    "\r\n",
    "- Vorarbeiten\r\n",
    "  - Notwendige Pandas Libraries aufrufen\r\n",
    "  - Daten abholen und einlesen  \r\n",
    "- Daten aufbereiten   \r\n",
    "     - Dublettencheck innerhalb der Buchhandelsdaten    \r\n",
    "     - Bestandsabgleich durch Abfragen auf dem Aleph-Server   \r\n",
    "     - Aufbereitung und Einspielen der Antworten   \r\n",
    "     - Erstellen der Exportdatei   \r\n",
    "\r\n",
    "\r\n",
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vorarbeiten\r\n",
    "\r\n",
    "### Pandas Libraries laden"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd                                                # für das Arbeiten mit der CSV-Datei\r\n",
    "import urllib.request                                              # für das Abrufen der URL\r\n",
    "import requests                                                    #für die Bestandsabfragen \r\n",
    "pd.options.mode.chained_assignment = None                          # default='warn' abschalten beim Beschreiben der neuen Spalten\r\n",
    "import time                                                        # für das Schreiben des Datums Logdatei und Excel-Export und Arbeiten mit dem Erscheinungsdatum\r\n",
    "import numpy as np                                                 # für das Bearbeiten von Spalten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prüfung, ob die Verbindung zum Aleph-Server für Abfragen korrekt funktioniert:\r\n",
    "\r\n",
    "    Nur zugelassene IPs können diese Schnittstelle abfragen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783482648434\"\r\n",
    "\r\n",
    "reply = requests.get(test).text\r\n",
    "a = reply.find(\"Forbidden\")\r\n",
    "b =  reply.find(\"?xml\")\r\n",
    "\r\n",
    "if (a > 50):\r\n",
    "    print(\"Es gibt ein Problem mit dem Server\")\r\n",
    "if (b == 1):\r\n",
    "    print(\"Der Server antwortet korrekt\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Der Server antwortet korrekt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "print(reply)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'reply' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16196/4069946789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'reply' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Datensätze abholen und einlesen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "url = \"https://content.schweitzer-online.de/static/content/export/mpifg/export.csv\"  # Abruf, der von Schweitzer zur Verfügung gestellten Daten\r\n",
    "checkout_file = \"./input/export.csv\"  \r\n",
    "urllib.request.urlretrieve(url, checkout_file)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./input/export.csv', <http.client.HTTPMessage at 0x2cca94383d0>)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = pd.read_csv('./input/export.csv', encoding = 'UTF-8', sep=';' , keep_default_na=False) # muss encoding angeben und Trennzeichen, NaN (= leere Werte) direkt beim Import entfernen, da sie später Probleme machen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LOG-Datei für den Prozess, zur Dokumentation des Imports und als Kontrollanzeige hier\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "x = df.shape[0]\r\n",
    "print('Anzahl der enthaltenen Datensätze:', x)\r\n",
    "print('vorhandene ISBNs:', df[\"isbn_ean\"].shape[0])\r\n",
    "\r\n",
    "timestr = time.strftime('%d.%m.%Y - %H:%M')\r\n",
    "\r\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\r\n",
    "    log.write('Logdatei PDA-Import vom ')\r\n",
    "    log.write(timestr)\r\n",
    "    log.write('\\n------------------------------------------\\n')\r\n",
    "    log.write('Gelieferte Datensätze:             ' + str(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anzahl der enthaltenen Datensätze: 3202\n",
      "vorhandene ISBNs: 3202\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Daten aufbereiten"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\r\n",
    "\r\n",
    "## Dublettencheck innerhalb der Buchhandelsdaten \r\n",
    "\r\n",
    "Aufgaben im Rahmen des Dublettencheck:\r\n",
    "1. Dublettenkontrolle anhand von Titel, Untertitel und Autor \r\n",
    "   - Zunächst Behebung der unsauberen Titel / Untertitel-Trennung für korrekteren Abgleich\r\n",
    "   - Trennung der Datensätze in Dubletten und \"Einzeltitel\"\r\n",
    "     - Einzeltitel werden direkt für Bestandsprüfung vorgemerkt\r\n",
    "     - Dubletten werden auf neueste Version reduziert und diese der Bestandsprüfungsdatei hinzugefügt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Die Dublettenkontrolle Anhand von Titel, Untertitel und Autor\r\n",
    "\r\n",
    "*Entfernen von Untertiteln aus der Titelspalte, Extrahieren von Untertiteln und Abgleich mit Untertitelspalte und Schreiben der vorhandenen Informationen in neue Untertitel-Spalte.   \r\n",
    "Durch diese Spalte werden ca. 1/3 mehr Dubletten erkannt, als ohne die Bereinigung. *"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "neu = df[\"title\"].str.split(':', n = 1, expand = True)  #Titel am 1. Doppelpunkt splitten und getrennt in neue Felder schreiben\r\n",
    "df[\"title_sep\"]= neu[0]\r\n",
    "df[\"subtitle_sep\"]= neu[1]\r\n",
    "\r\n",
    "df[\"subtitle_sep\"] = df[\"subtitle_sep\"].replace(np.nan, '', regex=True) #NaN-Werte stören, darum raus damit ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "comparison = np.where(df[\"subtitle\"] == df[\"subtitle_sep\"], '', df[\"subtitle\"])    # Abgleich - wenn in beiden das Gleiche steht, dann ursprüngliches \"Subtitle\"-Feld nehmen\r\n",
    "df[\"subtitle_comparison\"] = comparison  \r\n",
    "\r\n",
    "comparison2 = np.where(df[\"subtitle\"] < df[\"subtitle_sep\"], df[\"subtitle_sep\"], '') # Wenn nur in \"subtitle_sep\" Infos stehen, diese übernehmen, das ist noch nicht ganz sauber, da hier manchmal anderes steht als in \"subtitle\"\r\n",
    "df[\"subtitle_comparison2\"] = comparison2 \r\n",
    "\r\n",
    "df[\"subtitle_all\"] = df[\"subtitle_comparison\"]+df[\"subtitle_comparison2\"]          # Beide Informationen in neuer Subtitle-Spalte zusammenführen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df[\"short_title\"] = df[\"title_sep\"] + ' ' + df[\"subtitle_all\"] + ' / ' + df[\"contributor\"]  # aus den bereinigten Daten einen Kurztitel erzeugen, der dann für den Dublettencheck verwendet wird"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df_dubletten = df.groupby(\"short_title\").filter(lambda g: (g.nunique() >1).any()) # schreibt alle mehrfach vorhandenen Titel in ein eigenes Datenframe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "df_dubl_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep='first')   # sortiert Dubletten nach Jahr und schreibt den jeweils ersten (= neuesten) Eintrag in neues Dataframe\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_ohne_dubletten = df.drop_duplicates(\"short_title\", keep=False)       #durch \"\"keep=False\" werden alle nicht-Dubletten rausgezogen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df_einspielen = df_ohne_dubletten.append(df_dubl_einspielen)                    # die ausgewählten Dubletten und alle Nicht-Dubletten werden in ein Datenframe zusammengeführt\r\n",
    "df_einspielen.reset_index(inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datensätze am Bestand abgleichen\r\n",
    "\r\n",
    "*URLs für die Abfrage über den X-Server unseres Bibliothekssystems werden erzeugt und über die ISBN eine Abfrage auf Bestand gemacht. Die Abfrage funktioniert nur für zugelassene IPs (darum oben die Prüfung).  \r\n",
    "Für die Abfrage in unseren Bestand ist die ISBN sehr gut, da in den Titeldaten alle im Buch befindlichen ISBNs - auch die anderer Ausgabeformen - mit übernommen sind. Beim MPG-Ebooks Katalog handelt sich um Daten von Verlagen, die sich in ihrer Qualität und Informationsumfang sehr unterscheiden. Hier wird noch zu prüfen sein, inwieweit ein anderer Abfragemechansimus gewählt werden muss.*  \r\n",
    "\r\n",
    "#### URLs für unseren Bestand erzeugen\r\n",
    "\r\n",
    "*Es funktionierte nicht, dass die URLs an die vorhandenen ISBNs einfach so angefügt werden, darum der Workaround mit einem Platzhalter, der sich dann über replace vom richtigen Link überschreiben ließ.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df_einspielen[\"url_ges\"] = df_einspielen[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link','http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### URLs für Ebooks-Katalog erzeugen "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "df_einspielen[\"url_ebx\"] = df_einspielen[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Abfragen beim Server\r\n",
    "\r\n",
    "##### Zunächst für die Daten des MPIfG \r\n",
    "\r\n",
    "*Vorgehensweise: Abfrage und Sammeln der Antworten in einer Datei, diese Antworten werden dann in Ausdrücke \"übersetzt\" - \"vorhanden\" und \"neu\" und diese Daten in eine Spalte ins Dataframe zur weiteren Auswertung übertragen.   \r\n",
    "Schwierigkeit hier war, die Sammlung der Antworten zu den einzelnen Titeln, um sie in das Datenframe einzuspielen. Der störende XML-Header der Antworten wird erst gar nicht in die Datei geschrieben.   *   \r\n",
    "\r\n",
    "    Hinweis: \r\n",
    "    Vielleicht werden wir mittelfristig, um die Anzahl der Abfragen auf dem Server zu reduzieren, nach dem Abgleich mit dem eigenen Bestand, die Titel auf \"Neue\" reduzieren. Im Moment möchten wir jedoch die Abfrage prüfen und darum einen Blick auf die Bestandsabfrage machen. Aus diesem Grund wird unten auch eine Excel-Exportdatei dieser Titel erstellt. Ebenso ist die Dublettenprüfung vor den Bestandsabfragen denkbar, aber im Moment noch für die Kontrolle der Einstellungen in dieser Reihenfolge. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "with open('./input/server_responses', 'w') as fn:  \r\n",
    "    for url in df_einspielen[\"url_ges\"]:\r\n",
    "        reply = requests.get(url).text\r\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \r\n",
    "        fn.write(a)\r\n",
    "\r\n",
    "with open('./input/server_responses', 'r') as f:\r\n",
    "    with open('./input/server_responses_transfered', 'w') as fr:\r\n",
    "        for line in f:\r\n",
    "            if 'empty' in line:\r\n",
    "                fr.write('neu\\n')\r\n",
    "            elif 'no_records' in line:\r\n",
    "                fr.write('vorhanden\\n')\r\n",
    "\r\n",
    "df_fwf = pd.read_fwf('./input/server_responses_transfered', names=[\"Abfrage_ges\"])\r\n",
    "df_result = pd.concat([df_einspielen, df_fwf], axis=1)             "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Randnotiz: \r\n",
    "    Bei 2400 Titels brauchte der Abgleich ca 350 Sekunden"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\r\n",
    "x = df_einspielen.shape[0]\r\n",
    "y = df_fwf.shape[0]\r\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anzahl der eingelesenen Datensätze: 2826 \n",
      "Anzahl der Antworten vom Server:    2826\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Datenabgleich mit dem Bestand des MPG Ebooks-Katalog\r\n",
    "\r\n",
    "*Vorgehensweise analog Bestandsabfrage MPI.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "with open('./input/server_responses_ebx', 'w') as fn:  \r\n",
    "    for url in df_result[\"url_ebx\"]:\r\n",
    "        reply = requests.get(url).text\r\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \r\n",
    "        fn.write(a)\r\n",
    "\r\n",
    "with open('./input/server_responses_ebx', 'r') as f:\r\n",
    "    with open('./input/server_responses_transfered_ebx', 'w') as fr:\r\n",
    "        for line in f:\r\n",
    "            if 'empty' in line:\r\n",
    "                fr.write('neu\\n')\r\n",
    "            elif 'no_records' in line:\r\n",
    "                fr.write('vorhanden\\n')\r\n",
    "\r\n",
    "df_fwf_ebx = pd.read_fwf('./input/server_responses_transfered_ebx', names=[\"Abfrage_ebx\"])\r\n",
    "df_result2 = pd.concat([df_result, df_fwf_ebx], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\r\n",
    "x = df_result2.shape[0]\r\n",
    "y = df_fwf_ebx.shape[0]\r\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Anzahl der eingelesenen Datensätze: 2826 \n",
      "Anzahl der Antworten vom Server:    2826\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\r\n",
    "\r\n",
    "## Exportvorbereitungen\r\n",
    "\r\n",
    "### Zunächst ein Blick auf die vorhandenen Titel\r\n",
    "\r\n",
    "##### Zur Qualitätskontrolle und aus Interesse, werden die als vorhanden gekennzeichneten Titel in eine datierte Excel-Tabelle geschrieben "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df_nicht_einspielen = df_result2.drop(df_result2[(df_result2[\"Abfrage_ebx\"]== 'neu') & (df_result2[\"Abfrage_ges\"] == 'neu')].index) \r\n",
    "# alle Titel rausholen, die in einer der beiden Datenbanken vorhanden waren"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "date = time.strftime(\"%Y_%m_%d\")                                              # Zeit erfassen für Dateibenennung\r\n",
    "\r\n",
    "df_nicht_einspielen[\"object_id\"] = df_nicht_einspielen.object_id.astype(str)  # wandelt die spalte von Int64 zu Object um, so dass es in Excel korrekt eingelesen wird\r\n",
    "df_nicht_einspielen[\"isbn_ean\"] = df_nicht_einspielen.isbn_ean.astype(str)\r\n",
    "df_nicht_einspielen = df_nicht_einspielen.drop(columns=[\"url_ebx\", \"url_ges\", \"cover\", \"title_sep\", \"subtitle_comparison\", \"subtitle_comparison2\", \"subtitle_all\", \"subtitle_sep\"]) # unnötige Spalten entfernen\r\n",
    "\r\n",
    "df_nicht_einspielen.to_excel('./output/Vorhandene_Titel_'+date+'.xlsx', engine='xlsxwriter')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Jetzt Extraktion der Titel zum Einspielen:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df_aleph_einspielen = df_result2.loc[((df_result2[\"Abfrage_ebx\"]== 'neu') & (df_result2[\"Abfrage_ges\"] == 'neu'))]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Für die Logdatei Ermittlung verschiedener Zahlen und hier zur direkten Ansicht ausgegeben"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Kontrollmechanismus, ob für alle Titel auch Treffer da sind\r\n",
    "x = df_result2.shape[0]  #Ursprungszahl, eigentlich oben schon berechnet ...\r\n",
    "a = df_ohne_dubletten.shape[0]\r\n",
    "c = df_dubletten.shape[0]\r\n",
    "b = df_dubl_einspielen.shape[0] #Auswahl der neuen Treffer\r\n",
    "y = df_nicht_einspielen.shape[0] #vorhandene\r\n",
    "z = df_aleph_einspielen.shape[0]\r\n",
    "d = a + b\r\n",
    "print('Kleine Statistik:\\n=====================================',\r\n",
    "    '\\nGelieferte Datensätze:           ', x, \r\n",
    "    '\\n-------------------------------------',\r\n",
    "    '\\nSätze ohne Dubletten:            ', a,\r\n",
    "    '\\n    Dubletten:       ', c, \r\n",
    "    '\\n    Davon zum Einspielen:        ', b,\r\n",
    "    '\\nAm Bestand abzugleichen:         ', d,\r\n",
    "    '\\n----------------------------------------------------------------',\r\n",
    "    '\\nVorhandene Titel, die nicht in Aleph exportiert werden:   ', y,\r\n",
    "    '\\nTatsächlich eingespielt werden:                           ', z,\r\n",
    "    '\\n----------------------------------------------------------------',)\r\n",
    "\r\n",
    "#hier entsprechende Einträge für die Log-Datei\r\n",
    "\r\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:\r\n",
    "    log.write(\"\\nSätze ohne Dubletten:              \" + str(a))\r\n",
    "    log.write(\"\\n   Dubletten:              \" + str(c))\r\n",
    "    log.write(\"\\n   Auswahl zum Einspielen:         \" + str(b))\r\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\r\n",
    "    log.write(\"\\nAm Bestand abzugleichen:           \" + str(d))\r\n",
    "    log.write(\"\\n        davon vorhanden:           \" + str(y))\r\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\r\n",
    "    log.write(\"\\nEingespielt werden:                \" + str(z))\r\n",
    "    log.write(\"\\n============================================================\\n\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kleine Statistik:\n",
      "===================================== \n",
      "Gelieferte Datensätze:            2826 \n",
      "------------------------------------- \n",
      "Sätze ohne Dubletten:             2472 \n",
      "    Dubletten:        730 \n",
      "    Davon zum Einspielen:         354 \n",
      "Am Bestand abzugleichen:          2826 \n",
      "---------------------------------------------------------------- \n",
      "Vorhandene Titel, die nicht in Aleph exportiert werden:    104 \n",
      "Tatsächlich eingespielt werden:                            2722 \n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\r\n",
    "\r\n",
    "## Aufbereiten der Exportdatei\r\n",
    "\r\n",
    "#### Zielformat für das Einspielen in Aleph:\r\n",
    "\r\n",
    "    000000001 LDR   L -----nM2.01200024------h              \r\n",
    "    000000001 020   L $$a (object_id))\r\n",
    "    000000001 030   L $$aaz||rrrza||||\r\n",
    "    000000001 051   L $$am|||||||\r\n",
    "    000000001 070   L $$aSchweitzer\r\n",
    "    000000001 077   L $$aMonographie\r\n",
    "    000000001 078   L $$aSchweitzer\r\n",
    "    000000001 082   L $$azum Bestellen\r\n",
    "    000000001 100   L $$a (contributor_1)\r\n",
    "    000000001 104   L $$a (contributor_2)\r\n",
    "    000000001 108   L $$a (contributor_3)\r\n",
    "    000000001 331   L $$a (title_sep)\r\n",
    "    000000001 335   L $$a (subtitle_all)\r\n",
    "    000000001 403   L $$a (edition_number / edition_text)  #noch prüfen, was besser zu verwenden ist \r\n",
    "    000000001 419   L $$b (publisher) $$a (date_combined)\r\n",
    "    000000001 433   L $$a (pages)\r\n",
    "    000000001 451   L $$a (series)\r\n",
    "    000000001 520   L $$a (thesis)\r\n",
    "    000000001 540   L $$a (isbn_ean)\r\n",
    "    000000001 656   L $$a (cover)\r\n",
    "    000000001 750   L $$a (description)\r\n",
    "    000000001 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch        \r\n",
    "    \r\n",
    "Anmerkung zum Feld 655: die URL wird NACH dem Einspielen in Aleph mit der Datensatz-ID angereichert (siehe Juypter-Notebook \"Link-Anreicherung\"), um einen klaren Bestellink für den Kaufvorschlag zu haben\r\n",
    "\r\n",
    "*Hierfür werden immer die Feldbenennung und bestimmte Codierungen VOR den Inhalt - in Klammern de Bezeichnung der entsprechenden Spalte - gesetzt, bzw. erfoderliche Felder komplett neu hinzugefügt.   \r\n",
    "Am Anfang jeder Zeile braucht Aleph eine 9-Stellige eindeutige Zahl pro Titel.*   \r\n",
    "\r\n",
    "*Manchmal ließ sich der Inhalt einer Spalte direkt in die Datei schreiben, manchmal musst die Spalte zuvor über apply aufbereitet werden.* "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "df_aleph_einspielen[\"020\"] = df_aleph_einspielen[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \r\n",
    "del df_aleph_einspielen[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Besondere Aufbereitung der Personendaten\r\n",
    "\r\n",
    "*Da bis zu 3 Personen in einer Spalte zu finden sind, werden diese im Discovery nicht getrennt suchbar, darum werden sie gesplittet. Für die Dublettenkontrolle hat sich das als irrelevant erwiesen, darum erfolgt dieser Schritt erst hier.*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "person = df_aleph_einspielen[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\r\n",
    "\r\n",
    "df_aleph_einspielen[\"contributor_1\"]= person[0]\r\n",
    "df_aleph_einspielen[\"contributor_2\"]= person[1]\r\n",
    "df_aleph_einspielen[\"contributor_3\"]= person[2]\r\n",
    "\r\n",
    "df_aleph_einspielen[\"contributor_1\"]= df_aleph_einspielen[\"contributor_1\"].replace(np.nan, '', regex=True)\r\n",
    "df_aleph_einspielen[\"contributor_2\"]= df_aleph_einspielen[\"contributor_2\"].replace(np.nan, '', regex=True)\r\n",
    "df_aleph_einspielen[\"contributor_3\"]= df_aleph_einspielen[\"contributor_3\"].replace(np.nan, '', regex=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Besondere Aufbereitung des Erscheinungsdatum und Erscheinungsjahres\r\n",
    "\r\n",
    "*In der Auswahl unserer Titel befinden sich auch im Erscheinen befindliche Titel der kommenden Monate. Diese Information möchten wir gerne im Discovery sichtbar machen. Hierfür bleibt uns nur Aleph-Feld 419c, das dem Erscheinungsjahr vorbehalten ist.   \r\n",
    "Wunsch ist es: Wenn des Erscheinungsdatum weiter als 10 Tage weg vom heutigen Datum ist, soll das komplette Datum angezeigt werden, ansonsten nur das Erscheinungsjahr.*\r\n",
    "\r\n",
    "Zur Umsetzung muss die Spalte \"publication_date\" in ein Datum verwandelt werden und nach den genannten Kriterien unterschiedlich angezeigt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "today = int(time.strftime('%Y%m%d'))\r\n",
    "df_aleph_einspielen[\"coming_soon\"] = np.where(df_aleph_einspielen[\"publication_date\"].astype(int) > today+10, df_aleph_einspielen[\"publication_date\"], np.nan) #zieht die über 10 Tage raus, brauchen nan für Umwandlung in Datum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "df_aleph_einspielen[\"publication_date_soon\"] = df_aleph_einspielen[\"coming_soon\"].astype(str).str.replace('00','01')  # es gibt invalide Daten wie 20210800, darum diese erst auf den 1. des Monats geschoben.\r\n",
    "df_aleph_einspielen[\"coming_soon\"] = pd.to_datetime(df_aleph_einspielen[\"publication_date_soon\"], format='%Y%m%d.0', errors='ignore').astype(str).str.replace('NaT','') # Formatumwandlung, \"ignore\", da es weitere Fehlerhafte Daten in falscher Reihenfolge gibt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df_aleph_einspielen[\"published\"] = np.where(df_aleph_einspielen[\"coming_soon\"] == '', df_aleph_einspielen[\"publication_year\"], '') #Auslesen und Kombinieren der Daten\r\n",
    "df_aleph_einspielen[\"date_combined\"] = df_aleph_einspielen[\"published\"]+df_aleph_einspielen[\"coming_soon\"]\r\n",
    "#df_aleph_einspielen['date_combined']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "df_aleph_einspielen[\"419b\"] = df_aleph_einspielen[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \r\n",
    "df_aleph_einspielen[\"419c\"] = df_aleph_einspielen[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \r\n",
    "\r\n",
    "df_aleph_einspielen[\"419\"] = df_aleph_einspielen[\"419b\"]+df_aleph_einspielen[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "df_aleph_einspielen[\"403\"] = df_aleph_einspielen[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \r\n",
    "df_aleph_einspielen[\"433\"] = df_aleph_einspielen[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\r\n",
    "df_aleph_einspielen[\"451\"] = df_aleph_einspielen[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \r\n",
    "df_aleph_einspielen[\"520\"] = df_aleph_einspielen[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \r\n",
    "df_aleph_einspielen[\"540\"] = df_aleph_einspielen[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \r\n",
    "df_aleph_einspielen[\"656\"] = df_aleph_einspielen[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \r\n",
    "df_aleph_einspielen[\"750\"] = df_aleph_einspielen[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Für das Durchzählen der Titel braucht es eine neue Spalte"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# hier entsteht eine neue Spalte mit Zahlen ab 1 durchgehend gezählt, die für den korrekten Import der Daten in Aleph nötig ist\r\n",
    "x = df_aleph_einspielen.shape[0]   \r\n",
    "df_aleph_einspielen[\"id\"] = range(1,x+1)                                                       #Notwendig ist die Zählung ab 1, da Aleph sonst nicht korrekt einließt\r\n",
    "df_aleph_einspielen[\"id\"] = df_aleph_einspielen[\"id\"].apply(lambda x: f\"{x:09d}\")              #Die Zahl muss 9-Stellig aufgefüllt werden"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>\r\n",
    "\r\n",
    "### Vorbereitungen abgschlossen, jetzt das Schreiben der Datei im Aleph-Sequential-Format:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "with open(\"./output/pda_ges02\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\r\n",
    "    for i in df_aleph_einspielen.index:\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' LDR   L -----nM2.01200024------h'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"020\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 030   L $$aaz||rrrza||||'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 051   L $$am|||||||m|||||||'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 077   L $$aMonographie'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 078   L $$aSchweitzer'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 082   L $$azum Bestellen'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 100   L $$a'+df_aleph_einspielen[\"contributor_1\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 104   L $$a'+df_aleph_einspielen[\"contributor_2\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 108   L $$a'+df_aleph_einspielen[\"contributor_3\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 331   L $$a'+df_aleph_einspielen[\"title_sep\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 335   L $$a'+df_aleph_einspielen[\"subtitle_all\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"403\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"419\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"433\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"451\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"520\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"540\"][i]+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch'+'\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"656\"][i]+'$$3Cover\\n')\r\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"750\"][i]+'\\n')"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Anschließend Import der Datei \"pda_ges02\" in Aleph und Anreicherung mit den Bestelllinks über das Skript in Aleph_Link-Anreicherung.ipynb*"
   ],
   "metadata": {}
  }
 ]
}