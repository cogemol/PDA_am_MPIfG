{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buchhandelsdaten in Vufind als Grundlage für PDA (Patron Driven Aquisition) am MPIfG / Version 2: Updates laden statt Vollimport\n",
    "\n",
    "Einbindung von freundlicherweise von Schweitzer Fachinformation zur Verfügung gestellten Daten.   \n",
    "(Auswahl über passend konfigurierte Neuerscheinungsabfragen in unserem Kundenprofil).   \n",
    "\n",
    "#### Gründe für die Umstellung auf Updates: Durch Einspielen der Daten in Vufind ergibt sich eine zeitliche Diskrepanz, in der Titel bereits in Aleph gelöscht sind, aber in Vufind noch bestellbar. Zudem werden die Aleph-IDs hochgezähl, denn bei jedem Einspielen werden neue Nummern vergeben. Durch die Updates bleiben die Titel weiterhin verfügbar und das Hochzählen der IDs hält sich im Rahmen. Das Skript beschleunigt sich durch Reduzierung der Abfragen am Server.\n",
    "*Kleiner Nachteil: der manuelle Aufwand vergrößert sich. Mehrere Dateien müssen in Aleph eingespielt und verarbeitet werden, aber hält sich nach wie vor im vertretbaren Rahmen.*\n",
    "\n",
    "Das Jupyter Notebook arbeitet mit Python 3.8.1 und wurde mit Visual Studio Code 1.62.3 erstellt \n",
    "\n",
    "\n",
    "#### Arbeitsschritte im Code:\n",
    "\n",
    "> Vorarbeiten:   \n",
    "  - Notwendige Pandas Libraries aufrufen\n",
    "  - Serverprüfung auf funktionierende Verbindung zum Aleph-X-Server    \n",
    "\n",
    "\n",
    "> Daten abholen und einlesen:   \n",
    "  1. Buchhandelsdaten von Schweitzer \n",
    "  2. Aleph-Konkordanz Aleph-ID /Schweitzer ID  (erzeugt tagesaktuell per p-print-03 in Aleph)\n",
    "     - Aufbereiten der Daten: Schweitzer ID extrahieren und Aleph-ID mit Nullen auffüllen\n",
    "  3. Daten zusammenführen in einem df    \n",
    "\n",
    "\n",
    "> Daten aufbereiten:   \n",
    "  1. Buchhandelsdaten prüfen und vorbereiten\n",
    "     1. Identifizierung von Titeln in Aleph, die nicht mehr im Datensatz sind und schreiben in Datei \"ges02_loeschen_1\"\n",
    "     2. Dublettencheck innerhalb der Buchhandelsdaten\n",
    "     3. Trennung der Daten in \"in Aleph vorhanden\" und \"neu\"  \n",
    "  2. Bestandsabfragen:\n",
    "     1. Ganz neue Titel   \n",
    "        - Bestandsabgleich durch Abfragen (GES und EBX) auf dem Aleph-Server   \n",
    "     2. In Aleph vorhanden\n",
    "        - Bestandsabgleich durch Abfrage EBX auf dem Aleph-Server  \n",
    "     3. Exportvorbereitungen:\n",
    "        1. Neue Titel\n",
    "        2. In Aleph vorhandene Titel   \n",
    "            Identifizierung von zu löschenden Titeln \"ges02_loeschen_2\" und der zu aktualisierenden\n",
    "  3. Exportdateien aufbereiten:\n",
    "     1. Aufbereiten neuer Titel\n",
    "     2. Aufbereiten der vorhandenen Titel   \n",
    "\n",
    "\n",
    "> Informationssammlung\n",
    "   1. Log-Datei mit Rahmendaten wird fortlaufend geschrieben\n",
    "   2. Ausgabe bestimmter Titelgruppen als csv-Datei  \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorarbeiten\n",
    "\n",
    "### Pandas Libraries laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                # für das Arbeiten mit der CSV-Datei\n",
    "import urllib.request                                              # für das Abrufen der URL\n",
    "import requests                                                    # für die Bestandsabfragen \n",
    "pd.options.mode.chained_assignment = None                          # default='warn' abschalten beim Beschreiben der neuen Spalten\n",
    "import time                                                        # für das Schreiben des Datums Logdatei und Excel-Export und Arbeiten mit dem Erscheinungsdatum\n",
    "import datetime                                                    # für das Berechnen des Updates\n",
    "import numpy as np                                                 # für das Bearbeiten von Spalten\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung, ob die Verbindung zum Aleph-Server für Abfragen korrekt funktioniert:\n",
    "\n",
    "    Nur zugelassene IPs können diese Schnittstelle abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Server antwortet korrekt\n"
     ]
    }
   ],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783482648434\"\n",
    "\n",
    "reply = requests.get(test).text\n",
    "a = reply.find(\"Forbidden\")\n",
    "b =  reply.find(\"?xml\")\n",
    "\n",
    "if (a > 50):\n",
    "    print(\"Es gibt ein Problem mit dem Server\")\n",
    "if (b == 1):\n",
    "    print(\"Der Server antwortet korrekt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version = \"1.0\" encoding = \"UTF-8\"?>\n",
      "<find>\n",
      "<set_number>921747</set_number>\n",
      "<no_records>000000001</no_records>\n",
      "<no_entries>000000001</no_entries>\n",
      "<session-id>2Y3372BU7TGHNBB4K4RKQPHNL9DQMMVU9L38TKA8LV5TC7768F</session-id>\n",
      "</find>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783865058041\"\n",
    "\n",
    "reply = requests.get(test).text \n",
    "\n",
    "print(reply) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensätze abholen und einlesen\n",
    "\n",
    "# Erste Schritte manuell durchführen, um enthaltene Prüfroutinen im Blick zu behalten\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1. Datensätze von Schweitzer\n",
    "\n",
    "*einlesen in df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./input/export.csv', <http.client.HTTPMessage at 0x21067428790>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://content.schweitzer-online.de/static/content/export/mpifg/export.csv\"  # Abruf, der von Schweitzer zur Verfügung gestellten Daten\n",
    "checkout_file = \"./input/export.csv\"  \n",
    "urllib.request.urlretrieve(url, checkout_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/export.csv', encoding = 'UTF-8', sep=';' , keep_default_na=False , low_memory=False) # muss encoding angeben und Trennzeichen, NaN (= leere Werte) direkt beim Import entfernen, da sie später Probleme machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen in Datei: 66240\n",
      "Object_IDs:      66240\n"
     ]
    }
   ],
   "source": [
    "#Aufgrund eines zusätzlichen Zeilenumbruchs im Datensatz gab es im Juni 2022 ein Problem, darum kleine Prüfroutine eingebaut ob in der Import-Datei die Anzahl der Datensätze mit der Anzahl der object-Ids übereinstimmt\n",
    "g = df.shape[0]\n",
    "h= df['object_id'].count()\n",
    "\n",
    "print(\"Zeilen in Datei:\", g)\n",
    "print(\"Object_IDs:     \", h)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ab dem nächsten Schritt kann Skript durchlaufen, Prüfroutinen sind erfolgt\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aleph-Konkordanz Aleph-ID / Schweitzer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime('%Y%m%d')  # %H:%M:%S\n",
    "input_file = './input/ids'+now      #input_file wird tagesaktuell aus Aleph gezogen und auf diesem Wege mit der entsprechenden Endung eingelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ids Field  L       Content  LEN\n",
      "0      000129218   020  L    $$a2258402   10\n",
      "1      000129219   020  L    $$a2522807   10\n",
      "2      000129220   020  L    $$a2308946   10\n",
      "3      000129221   020  L    $$a2220214   10\n",
      "4      000129222   020  L    $$a2479903   10\n",
      "...          ...   ... ..           ...  ...\n",
      "29970  000159265   020  L  $$a169951393   12\n",
      "29971  000159266   020  L  $$a163496075   12\n",
      "29972  000159267   020  L  $$a165662822   12\n",
      "29973  000159268   020  L  $$a170244345   12\n",
      "29974  000159269   020  L  $$a171634215   12\n",
      "\n",
      "[29975 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# neu einlesen der Aleph-IDs mittels Code, da sonst Object-Ids abgeschnitten wurden\n",
    "with open(input_file) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "df = pd.DataFrame(data[0:]) # 0 weil ich keinen header habe\n",
    "\n",
    "# use expand to split strings into separate columns\n",
    "df_alephIDs = df[0].str.rsplit(expand=True)\n",
    "# fix column names\n",
    "df_alephIDs.columns = [\"ids\",\"Field\",\"L\",\"Content\"]\n",
    "\n",
    "df_alephIDs['LEN'] = df_alephIDs['Content'].apply(len)\n",
    "print(df_alephIDs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Datenfelder aufbereiten: Schweitzer-ID extrahieren und Aleph-ID ins richtige Format bringen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169327230    1\n",
       "170143037    1\n",
       "166237816    1\n",
       "171411066    1\n",
       "165985917    1\n",
       "            ..\n",
       "163479759    1\n",
       "169687954    1\n",
       "166992790    1\n",
       "158715772    1\n",
       "169349122    1\n",
       "Name: object_id, Length: 29975, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alephIDs[\"object_id\"] = df_alephIDs[\"Content\"].astype(str).str.slice(start=3,stop=16).apply(int)   #Spalte mit object-Ids herausschneiden und wieder zur Zahl definieren\n",
    "\n",
    "df_alephIDs[\"object_id\"].value_counts() #Hier muss immer eins Stehen in der hinteren Spalte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Daten zu einem Frame zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführung erfolgt einmal mit concat - erlaubt die nicht mehr verwendeten Aleph-IDs zu identifizieren \n",
    "# und einmal mit join, dann erhalte ich nur die Titel, die auch im neuen Datensatz sind zur weiteren Verarbeitung!\n",
    "df_oi= df.set_index(\"object_id\")                          #object-ID zum Index für beide Datenframes\n",
    "df_aleph_oi = df_alephIDs.set_index(\"object_id\")\n",
    "df_update_aleph = pd.concat([df_oi, df_aleph_oi], axis=1)\n",
    "\n",
    "df_update_join = df_oi.join(df_aleph_oi)                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Wenn es bei der obigen Zelle zu Fehlermeldung kommt, dann gibt es vermutlich doppelte SChweitzer-IDs in den Daten.\n",
    "\n",
    "Das kann man beim Output der folgenden Zelle oben sehen:\n",
    "\n",
    "    #nach Dubletten vom letzten Mal vorsichtshalber, check nach doppelten object-ids\n",
    "    df_alephIDs[\"object_id\"].value_counts()\n",
    "\n",
    "Wenn da Nummern mit 2 vorhanden sind, dann muss oben der Code zur Dublettenbereinigung gestartet werden. Dies geschieht, in dem die folgende Zelle von Markdown zu Code geändert wird und durchlaufen. \n",
    "\n",
    "## Dabei wird eine weitere Datei erzeugt, die in Aleph gelöscht werden muss!!\n",
    "\n",
    "WICHTIG: Anschließend, den Code wieder in Markdown verwandeln, da er ja nur in dem Sonderfall gebraucht wird. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code zur Bereinigung von Dubletten im in Aleph schon eingespielten Bestand \n",
    "# Einmalig aktivieren (Markdown > Code) und nutzen und dann wieder (Code > Markdown)\n",
    "\n",
    "df_aleph_einzel = df_alephIDs.drop_duplicates(\"object_id\", keep=False)  #Auslesen der Eintraege mit einzelner object_id\n",
    "df_aleph_doppelt = df_alephIDs.groupby(\"object_id\").filter(lambda g: (g.nunique() >1).any()) #Alle Sätze mit doppelten object_ids rausziehen\n",
    "df_aleph_single = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='first') #Die Dublette, die behalten wird\n",
    "df_aleph_dublette = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='last') #Die Dublette, die gelöscht wird\n",
    "# In Aleph zu löschende in eine Datei schreiben aus df_aleph_dublette\n",
    "with open(\"./output/ges02_dubl\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_dublette.index:\n",
    "        fa.write(df_aleph_dublette[\"ids\"][i]+'GES02'+'\\n')\n",
    "        \n",
    "df_aleph_vorhanden = df_aleph_einzel.append(df_aleph_single) #Einzelne und ausgewählt Dubletten zusammenfuehren\n",
    "\n",
    "df_oi= df.set_index(\"object_id\")                          #Dann nochmal den Schritt, an dem es vorhin gehakt hat\n",
    "df_aleph_oi = df_aleph_vorhanden.set_index(\"object_id\")\n",
    "df_update_aleph = pd.concat([df_oi, df_aleph_oi], axis=1)\n",
    "\n",
    "df_update_join = df_oi.join(df_aleph_oi)   \n",
    "\n",
    "# Noch kleine Rechnung zur Ueberprüfung, ob berechnete Dubletten und mit Skript ermittelte Dublettenzahl übereinstimmen\n",
    "g = df_alephIDs.shape[0]\n",
    "h= df_aleph_einzel.shape[0]\n",
    "j = df_aleph_doppelt.shape[0]\n",
    "\n",
    "print(\"Dubletten berechnet: \",  g-h)\n",
    "print(\"Dubletten gezählt:   \", j)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wenn alles gut ist, kann das Skript ab hier wieder mit \"all Cells below\" durchlaufen\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG-Datei für den Prozess, zur Dokumentation des Imports und als Kontrollanzeige hier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der enthaltenen Datensätze: 66240\n",
      "-------------------------\n",
      "Aleph-IDs anfangs 29975\n",
      "Aleph-IDs nach join 29772\n"
     ]
    }
   ],
   "source": [
    "x = df.shape[0]\n",
    "print('Anzahl der enthaltenen Datensätze:', x)\n",
    "#print('vorhandene ISBNs:', df_update[\"isbn_ean\"].shape[0])\n",
    "\n",
    "z = df_alephIDs[\"ids\"].count()\n",
    "y = df_update_join[\"ids\"].count()\n",
    "\n",
    "print('-------------------------')\n",
    "print('Aleph-IDs anfangs', z)\n",
    "print('Aleph-IDs nach join' , y)\n",
    "\n",
    "timestr = time.strftime('%d.%m.%Y - %H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('Logdatei PDA-Import vom ')\n",
    "    log.write(timestr)\n",
    "    log.write('\\n------------------------------------------\\n')\n",
    "    log.write('Gelieferte Datensätze:             ' + str(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1. Identifizierung von Titeln in Aleph-Dubletten, die nicht mehr im Datensatz sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>edition_number</th>\n",
       "      <th>edition_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>...</th>\n",
       "      <th>cover</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>follower</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>watchlist_name</th>\n",
       "      <th>ids</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "      <th>LEN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217164</th>\n",
       "      <td>9.78341e+12</td>\n",
       "      <td>Unternehmensbewertung für Juristen</td>\n",
       "      <td></td>\n",
       "      <td>Dirk Hachmeister;Matthias Schüppen</td>\n",
       "      <td>C.H.BECK</td>\n",
       "      <td>Neue Juristische Wochenschrift (NJW) - Praxis ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-02-03 05:50:21</td>\n",
       "      <td>Himbeere Wirtschaft #181.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218910</th>\n",
       "      <td>9.78341e+12</td>\n",
       "      <td>Grundkurs Steuerrecht</td>\n",
       "      <td></td>\n",
       "      <td>Rainer Wernsmann;Christian Thiemann</td>\n",
       "      <td>C.H.BECK</td>\n",
       "      <td>Grundkurse</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-02-05 06:58:17</td>\n",
       "      <td>brombeere Politik #69.1</td>\n",
       "      <td>000129231</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a218910</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237277</th>\n",
       "      <td>9.78383e+12</td>\n",
       "      <td>Käuferverhalten</td>\n",
       "      <td>Eine marketingorientierte Einführung</td>\n",
       "      <td>Alfred Kuß;Torsten Tomczak;Silke Lennerts</td>\n",
       "      <td>UTB</td>\n",
       "      <td>Grundwissen der Ökonomik Band 1604</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>5., komplett überarbeitete Auflage 2017</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td>9783825216047</td>\n",
       "      <td></td>\n",
       "      <td>2025-03-05 20:40:44</td>\n",
       "      <td>Himbeere Wirtschaft #26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240369</th>\n",
       "      <td>9.78383e+12</td>\n",
       "      <td>WTO-Recht in Fällen</td>\n",
       "      <td></td>\n",
       "      <td>Jan Neumann;Christian Pitschas</td>\n",
       "      <td>Nomos</td>\n",
       "      <td>NomosStudium</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td>9783832904241</td>\n",
       "      <td></td>\n",
       "      <td>2025-03-05 20:41:38</td>\n",
       "      <td>Himbeere Wirtschaft #112.2</td>\n",
       "      <td>000129233</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a240369</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261723</th>\n",
       "      <td>9.78349e+12</td>\n",
       "      <td>Gesundheitsökonomie</td>\n",
       "      <td>Eine Einführung</td>\n",
       "      <td>Kornelia van der Beek;Gregor van der Beek;Wilf...</td>\n",
       "      <td>De Gruyter Oldenbourg</td>\n",
       "      <td>de Gruyter Studium</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2., überarbeitete und erweiterte Auflage</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td>9783486586862</td>\n",
       "      <td></td>\n",
       "      <td>2025-02-03 06:13:52</td>\n",
       "      <td>Himbeere Wirtschaft #119.1</td>\n",
       "      <td>000129303</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a261723</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173032813</th>\n",
       "      <td>9.78384e+12</td>\n",
       "      <td>Visuelle Transformationen des Exils</td>\n",
       "      <td>Die jüdischen Künstlerinnen Grete Stern, Hedy ...</td>\n",
       "      <td>Christina Wieder</td>\n",
       "      <td>transcript</td>\n",
       "      <td>Historische Geschlechterforschung 21</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-11 19:58:50</td>\n",
       "      <td>brombeere Politik #200.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173032814</th>\n",
       "      <td>9.78384e+12</td>\n",
       "      <td>Talking Politics and Society Again</td>\n",
       "      <td>Reengaging with Fellow Citizens</td>\n",
       "      <td>Hans Blokland</td>\n",
       "      <td>transcript</td>\n",
       "      <td>Kultur und soziale Praxis</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-11 19:58:50</td>\n",
       "      <td>brombeere Politik #200.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173052865</th>\n",
       "      <td>9.78234e+12</td>\n",
       "      <td>Europe, as-tu une âme ?</td>\n",
       "      <td>Essai pour une pédagogie de l'Europe</td>\n",
       "      <td>Vincent Carbonel</td>\n",
       "      <td>Editions L'Harmattan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-05 18:40:16</td>\n",
       "      <td>brombeere Politik #200.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173056680</th>\n",
       "      <td>9.78369e+12</td>\n",
       "      <td>Geldwäsche und Terrorismusfinanzierung</td>\n",
       "      <td>Validierung von Frühwarnindikatoren für Kredit...</td>\n",
       "      <td>Christoph Matthias Lüling</td>\n",
       "      <td>Tectum Wissenschaftsverlag</td>\n",
       "      <td>Young Academics: Betriebswirtschaftslehre 12</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-12 06:11:14</td>\n",
       "      <td>Himbeere Wirtschaft #199.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173059497</th>\n",
       "      <td>9.78234e+12</td>\n",
       "      <td>Les enjeux culturels du conflit russo-ukrainien</td>\n",
       "      <td>La redéfinition de l'identité ukrainienne au X...</td>\n",
       "      <td>Julien Plouchart</td>\n",
       "      <td>Editions L'Harmattan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-06 18:28:43</td>\n",
       "      <td>brombeere Politik #200.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66443 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              isbn_ean                                            title  \\\n",
       "object_id                                                                 \n",
       "217164     9.78341e+12               Unternehmensbewertung für Juristen   \n",
       "218910     9.78341e+12                            Grundkurs Steuerrecht   \n",
       "237277     9.78383e+12                                  Käuferverhalten   \n",
       "240369     9.78383e+12                              WTO-Recht in Fällen   \n",
       "261723     9.78349e+12                              Gesundheitsökonomie   \n",
       "...                ...                                              ...   \n",
       "173032813  9.78384e+12              Visuelle Transformationen des Exils   \n",
       "173032814  9.78384e+12               Talking Politics and Society Again   \n",
       "173052865  9.78234e+12                          Europe, as-tu une âme ?   \n",
       "173056680  9.78369e+12           Geldwäsche und Terrorismusfinanzierung   \n",
       "173059497  9.78234e+12  Les enjeux culturels du conflit russo-ukrainien   \n",
       "\n",
       "                                                    subtitle  \\\n",
       "object_id                                                      \n",
       "217164                                                         \n",
       "218910                                                         \n",
       "237277                  Eine marketingorientierte Einführung   \n",
       "240369                                                         \n",
       "261723                                       Eine Einführung   \n",
       "...                                                      ...   \n",
       "173032813  Die jüdischen Künstlerinnen Grete Stern, Hedy ...   \n",
       "173032814                    Reengaging with Fellow Citizens   \n",
       "173052865               Essai pour une pédagogie de l'Europe   \n",
       "173056680  Validierung von Frühwarnindikatoren für Kredit...   \n",
       "173059497  La redéfinition de l'identité ukrainienne au X...   \n",
       "\n",
       "                                                 contributor  \\\n",
       "object_id                                                      \n",
       "217164                    Dirk Hachmeister;Matthias Schüppen   \n",
       "218910                   Rainer Wernsmann;Christian Thiemann   \n",
       "237277             Alfred Kuß;Torsten Tomczak;Silke Lennerts   \n",
       "240369                        Jan Neumann;Christian Pitschas   \n",
       "261723     Kornelia van der Beek;Gregor van der Beek;Wilf...   \n",
       "...                                                      ...   \n",
       "173032813                                   Christina Wieder   \n",
       "173032814                                      Hans Blokland   \n",
       "173052865                                   Vincent Carbonel   \n",
       "173056680                          Christoph Matthias Lüling   \n",
       "173059497                                   Julien Plouchart   \n",
       "\n",
       "                            publisher  \\\n",
       "object_id                               \n",
       "217164                       C.H.BECK   \n",
       "218910                       C.H.BECK   \n",
       "237277                            UTB   \n",
       "240369                          Nomos   \n",
       "261723          De Gruyter Oldenbourg   \n",
       "...                               ...   \n",
       "173032813                  transcript   \n",
       "173032814                  transcript   \n",
       "173052865        Editions L'Harmattan   \n",
       "173056680  Tectum Wissenschaftsverlag   \n",
       "173059497        Editions L'Harmattan   \n",
       "\n",
       "                                                      series thesis  \\\n",
       "object_id                                                             \n",
       "217164     Neue Juristische Wochenschrift (NJW) - Praxis ...          \n",
       "218910                                            Grundkurse          \n",
       "237277                    Grundwissen der Ökonomik Band 1604          \n",
       "240369                                          NomosStudium          \n",
       "261723                                    de Gruyter Studium          \n",
       "...                                                      ...    ...   \n",
       "173032813               Historische Geschlechterforschung 21          \n",
       "173032814                          Kultur und soziale Praxis          \n",
       "173052865                                                             \n",
       "173056680       Young Academics: Betriebswirtschaftslehre 12          \n",
       "173059497                                                             \n",
       "\n",
       "          edition_number                              edition_text media_type  \\\n",
       "object_id                                                                       \n",
       "217164                 1                                            hardcover   \n",
       "218910                 1                                            hardcover   \n",
       "237277                 5   5., komplett überarbeitete Auflage 2017  hardcover   \n",
       "240369                 2                                            hardcover   \n",
       "261723                    2., überarbeitete und erweiterte Auflage  hardcover   \n",
       "...                  ...                                       ...        ...   \n",
       "173032813              1                                            hardcover   \n",
       "173032814              1                                            hardcover   \n",
       "173052865                                                           hardcover   \n",
       "173056680              1                                            hardcover   \n",
       "173059497                                                           hardcover   \n",
       "\n",
       "           ...                                              cover  \\\n",
       "object_id  ...                                                      \n",
       "217164     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "218910     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "237277     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "240369     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "261723     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "...        ...                                                ...   \n",
       "173032813  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173032814  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173052865  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173056680  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173059497  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "\n",
       "             predecessor follower        last_modified  \\\n",
       "object_id                                                \n",
       "217164                             2025-02-03 05:50:21   \n",
       "218910                             2025-02-05 06:58:17   \n",
       "237277     9783825216047           2025-03-05 20:40:44   \n",
       "240369     9783832904241           2025-03-05 20:41:38   \n",
       "261723     9783486586862           2025-02-03 06:13:52   \n",
       "...                  ...      ...                  ...   \n",
       "173032813                          2025-03-11 19:58:50   \n",
       "173032814                          2025-03-11 19:58:50   \n",
       "173052865                          2025-03-05 18:40:16   \n",
       "173056680                          2025-03-12 06:11:14   \n",
       "173059497                          2025-03-06 18:28:43   \n",
       "\n",
       "                       watchlist_name        ids Field     L    Content   LEN  \n",
       "object_id                                                                      \n",
       "217164     Himbeere Wirtschaft #181.2       None  None  None       None  None  \n",
       "218910        brombeere Politik #69.1  000129231   020     L  $$a218910     9  \n",
       "237277        Himbeere Wirtschaft #26       None  None  None       None  None  \n",
       "240369     Himbeere Wirtschaft #112.2  000129233   020     L  $$a240369     9  \n",
       "261723     Himbeere Wirtschaft #119.1  000129303   020     L  $$a261723     9  \n",
       "...                               ...        ...   ...   ...        ...   ...  \n",
       "173032813    brombeere Politik #200.2       None  None  None       None  None  \n",
       "173032814    brombeere Politik #200.2       None  None  None       None  None  \n",
       "173052865    brombeere Politik #200.1       None  None  None       None  None  \n",
       "173056680  Himbeere Wirtschaft #199.1       None  None  None       None  None  \n",
       "173059497    brombeere Politik #200.1       None  None  None       None  None  \n",
       "\n",
       "[66443 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_update_aleph.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_loeschen = df_update_aleph[df_update_aleph['isbn_ean'].isna()]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>edition_number</th>\n",
       "      <th>edition_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>...</th>\n",
       "      <th>cover</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>follower</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>watchlist_name</th>\n",
       "      <th>ids</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "      <th>LEN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2254200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129227</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2254200</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368410</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129249</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2368410</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430847</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129244</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2430847</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481913</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129235</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2481913</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576208</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129267</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2576208</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171787741</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000155532</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171787741</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171796455</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152210</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171796455</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171868889</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152355</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171868889</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171894384</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152319</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171894384</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171894386</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152318</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171894386</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           isbn_ean title subtitle contributor publisher series thesis  \\\n",
       "object_id                                                                \n",
       "2254200         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2368410         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2430847         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2481913         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2576208         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "...             ...   ...      ...         ...       ...    ...    ...   \n",
       "171787741       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171796455       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171868889       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171894384       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171894386       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "\n",
       "          edition_number edition_text media_type  ... cover predecessor  \\\n",
       "object_id                                         ...                     \n",
       "2254200              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2368410              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2430847              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2481913              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2576208              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "...                  ...          ...        ...  ...   ...         ...   \n",
       "171787741            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171796455            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171868889            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171894384            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171894386            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "\n",
       "          follower last_modified watchlist_name        ids Field  L  \\\n",
       "object_id                                                             \n",
       "2254200        NaN           NaN            NaN  000129227   020  L   \n",
       "2368410        NaN           NaN            NaN  000129249   020  L   \n",
       "2430847        NaN           NaN            NaN  000129244   020  L   \n",
       "2481913        NaN           NaN            NaN  000129235   020  L   \n",
       "2576208        NaN           NaN            NaN  000129267   020  L   \n",
       "...            ...           ...            ...        ...   ... ..   \n",
       "171787741      NaN           NaN            NaN  000155532   020  L   \n",
       "171796455      NaN           NaN            NaN  000152210   020  L   \n",
       "171868889      NaN           NaN            NaN  000152355   020  L   \n",
       "171894384      NaN           NaN            NaN  000152319   020  L   \n",
       "171894386      NaN           NaN            NaN  000152318   020  L   \n",
       "\n",
       "                Content   LEN  \n",
       "object_id                      \n",
       "2254200      $$a2254200  10.0  \n",
       "2368410      $$a2368410  10.0  \n",
       "2430847      $$a2430847  10.0  \n",
       "2481913      $$a2481913  10.0  \n",
       "2576208      $$a2576208  10.0  \n",
       "...                 ...   ...  \n",
       "171787741  $$a171787741  12.0  \n",
       "171796455  $$a171796455  12.0  \n",
       "171868889  $$a171868889  12.0  \n",
       "171894384  $$a171894384  12.0  \n",
       "171894386  $$a171894386  12.0  \n",
       "\n",
       "[203 rows x 27 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_loeschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Loeschen-datei!\n",
    "\n",
    "with open(\"./output/ges02_weg\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_loeschen.index:\n",
    "        fa.write(df_aleph_loeschen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Dublettencheck innerhalb gelieferten Buchhandelsdaten \n",
    "\n",
    "Aufgaben im Rahmen des Dublettencheck:\n",
    "1. Dublettenkontrolle anhand von Titel, Untertitel und Autor \n",
    "   - Zunächst Behebung der unsauberen Titel / Untertitel-Trennung für korrekteren Abgleich\n",
    "   - Trennung der Datensätze in Dubletten und \"Einzeltitel\"\n",
    "     - Einzeltitel werden direkt für Bestandsprüfung vorgemerkt\n",
    "     - Dubletten werden auf neueste Version reduziert und diese der Bestandsprüfungsdatei hinzugefügt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Dublettenkontrolle Anhand von Titel, Untertitel und Autor\n",
    "\n",
    "*Entfernen von Untertiteln aus der Titelspalte, Extrahieren von Untertiteln und Abgleich mit Untertitelspalte und Schreiben der vorhandenen Informationen in neue Untertitel-Spalte.   \n",
    "Durch diese Spalte werden ca. 1/3 mehr Dubletten erkannt, als ohne die Bereinigung. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_update_join[df_update_join['isbn_ean'].notna()]  #jetzt alle die zu verarbeiten sind rausziehen\n",
    "df.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu = df[\"title\"].str.split(':', n = 1, expand = True)  #Titel am 1. Doppelpunkt splitten und getrennt in neue Felder schreiben\n",
    "df[\"title_sep\"]= neu[0]\n",
    "df[\"subtitle_sep\"]= neu[1]\n",
    "\n",
    "df[\"subtitle_sep\"] = df[\"subtitle_sep\"].replace(np.nan, '', regex=True) #NaN-Werte stören, darum raus damit ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = np.where(df[\"subtitle\"] == df[\"subtitle_sep\"], '', df[\"subtitle\"])    # Abgleich - wenn in beiden das Gleiche steht, dann ursprüngliches \"Subtitle\"-Feld nehmen\n",
    "df[\"subtitle_comparison\"] = comparison  \n",
    "\n",
    "comparison2 = np.where(df[\"subtitle\"] < df[\"subtitle_sep\"], df[\"subtitle_sep\"], '') # Wenn nur in \"subtitle_sep\" Infos stehen, diese übernehmen, das ist noch nicht ganz sauber, da hier manchmal anderes steht als in \"subtitle\"\n",
    "df[\"subtitle_comparison2\"] = comparison2 \n",
    "\n",
    "df[\"subtitle_all\"] = df[\"subtitle_comparison\"]+df[\"subtitle_comparison2\"]          # Beide Informationen in neuer Subtitle-Spalte zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short_title\"] = df[\"title_sep\"] + ' ' + df[\"subtitle_all\"] + ' / ' + df[\"contributor\"]  # aus den bereinigten Daten einen Kurztitel erzeugen, der dann für den Dublettencheck verwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dubletten = df.groupby(\"short_title\").filter(lambda g: (g.nunique() >1).any()) # schreibt alle mehrfach vorhandenen Titel in ein eigenes Datenframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dubl_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep='first')   # sortiert Dubletten nach Jahr und schreibt den jeweils ersten (= neuesten) Eintrag in neues Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohne_dubletten = df.drop_duplicates(\"short_title\", keep=False)       #durch \"\"keep=False\" werden alle nicht-Dubletten rausgezogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dubl_nicht_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep=False)\n",
    "\n",
    "#Kontrolle, ob es in Dubletten Titel gibt, die schon in Aleph sind\n",
    "df_dubl_nicht_einspielen[\"ids\"].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_einspielen = df_ohne_dubletten.append(df_dubl_einspielen)                    # die ausgewählten Dubletten und alle Nicht-Dubletten werden in ein Datenframe zusammengeführt\n",
    "df_einspielen.reset_index(inplace=True)                                         # für weitere Bearbeitung index-reset nötig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Überblick zu den Daten und Trennen in \"in Aleph vorhanden\" und \"neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zum Einspielen: 53951\n",
      "Davon in Aleph: 27684\n",
      "Neue Titel: 26267\n"
     ]
    }
   ],
   "source": [
    "m = df_einspielen.shape[0]\n",
    "n = df_einspielen[\"ids\"].count()\n",
    "print(\"Zum Einspielen:\", m)\n",
    "print(\"Davon in Aleph:\", n)\n",
    "print(\"Neue Titel:\", m-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27684"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph = df_einspielen.dropna(subset=['ids'])\n",
    "df_in_aleph.reset_index(inplace=True)\n",
    "df_in_aleph.shape[0]\n",
    "\n",
    "# zur Prüfung am GES und Ebx Bestand geprüft werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26267"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ganz_neu = df_einspielen[df_einspielen['ids'].isnull()]\n",
    "\n",
    "df_ganz_neu.reset_index(inplace=True)\n",
    "df_ganz_neu.shape[0]\n",
    "\n",
    "# diese müssen ebenfalls am Ges und Ebx Bestand geprüft werden, durch Trennung der Sets schnellere Bearbeitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bestandsabfragen\n",
    "\n",
    "\n",
    "\n",
    "*URLs für die Abfrage über den X-Server unseres Bibliothekssystems werden erzeugt und über die ISBN eine Abfrage auf Bestand gemacht. Die Abfrage funktioniert nur für zugelassene IPs (darum oben die Prüfung).  \n",
    "Für die Abfrage in unseren Bestand ist die ISBN sehr gut, da in den Titeldaten alle im Buch befindlichen ISBNs - auch die anderer Ausgabeformen - mit übernommen sind. Beim MPG-Ebooks Katalog handelt sich um Daten von Verlagen, die sich in ihrer Qualität und Informationsumfang sehr unterscheiden. Hier wird noch zu prüfen sein, inwieweit ein anderer Abfragemechansimus gewählt werden muss.*  \n",
    "\n",
    "### 1. Ganz neue Titel  \n",
    "\n",
    "= df_ganz_neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ges\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link','http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "#Es funktionierte nicht, dass die URLs an die vorhandenen ISBNs einfach so angefügt werden, darum der Workaround mit einem Platzhalter, der sich dann über replace vom richtigen Link überschreiben ließ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs für Ebooks-Katalog erzeugen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ebx\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n",
    "\n",
    "##### Zunächst für die Daten des MPIfG \n",
    "\n",
    "*Vorgehensweise: Abfrage und Sammeln der Antworten in einer Datei, diese Antworten werden dann in Ausdrücke \"übersetzt\" - \"vorhanden\" und \"neu\" und diese Daten in eine Spalte ins Dataframe zur weiteren Auswertung übertragen.   \n",
    "Schwierigkeit hier war, die Sammlung der Antworten zu den einzelnen Titeln, um sie in das Datenframe einzuspielen. Der störende XML-Header der Antworten wird erst gar nicht in die Datei geschrieben.   *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses', 'w') as fn:  \n",
    "    for url in df_ganz_neu[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses', 'r') as f:\n",
    "    with open('./input/server_responses_transfered', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf = pd.read_fwf('./input/server_responses_transfered', names=[\"Abfrage_ges\"])\n",
    "df_result = df_ganz_neu.join(df_fwf)                    #df_result = pd.concat([df_ganz_neu, df_fwf], axis=1), funktioniert nicht mehr                  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Randnotiz: \n",
    "    Bei 2400 Titels brauchte der Abgleich ca 350 Sekunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 5329 \n",
      "Anzahl der Antworten vom Server:    5329\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_ganz_neu.shape[0]\n",
    "y = df_fwf.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datenabgleich mit dem Bestand des MPG Ebooks-Katalog\n",
    "\n",
    "*Vorgehensweise analog Bestandsabfrage MPI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx', 'w') as fn:  \n",
    "    for url in df_result[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx = pd.read_fwf('./input/server_responses_transfered_ebx', names=[\"Abfrage_ebx\"])\n",
    "df_result_neu = df_result.join(df_fwf_ebx)           #df_result_neu = pd.concat([df_result, df_fwf_ebx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 5329 \n",
      "Anzahl der Antworten vom Server:    5329\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_neu.shape[0]\n",
    "y = df_fwf_ebx.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Titel in Aleph vorhanden\n",
    "\n",
    "= df_in_aleph\n",
    "\n",
    "*Hier reicht Abgleich mit Ebooks, das erworbene Bücher manuell gelöscht werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph[\"url_ges\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link', 'http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "df_in_aleph[\"url_ebx\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses2', 'w') as fn:  \n",
    "    for url in df_in_aleph[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ges = pd.read_fwf('./input/server_responses_transfered2', names=[\"Abfrage_ges\"])\n",
    "df_result2 = df_in_aleph.join(df_fwf_ges)               #df_result2 = pd.concat([df_in_aleph, df_fwf_ges], axis=1)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 26725 \n",
      "Anzahl der Antworten vom Server:    26725\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_in_aleph.shape[0]\n",
    "y = df_fwf_ges.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        neu\n",
       "1        neu\n",
       "2        neu\n",
       "3        neu\n",
       "4        neu\n",
       "        ... \n",
       "26720    neu\n",
       "26721    neu\n",
       "26722    neu\n",
       "26723    neu\n",
       "26724    neu\n",
       "Name: Abfrage_ges, Length: 26725, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result2['Abfrage_ges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx2', 'w') as fn:  \n",
    "    for url in df_result2[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx2 = pd.read_fwf('./input/server_responses_transfered_ebx2', names=[\"Abfrage_ebx\"])\n",
    "df_result_in_aleph = df_result2.join(df_fwf_ebx2)           #df_result_in_aleph = pd.concat([df_result2, df_fwf_ebx2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 26725 \n",
      "Anzahl der Antworten vom Server:    26725\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_in_aleph.shape[0]\n",
    "y = df_fwf_ebx2.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Exportvorbereitungen \n",
    "\n",
    "\n",
    "### 1. Neue Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_nicht_einspielen =  df_result_neu.drop(df_result_neu[(df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu')].index)          \n",
    "# alle Titel rausholen, die in einer der beiden Datenbanken vorhanden waren, diese werden mit den vorhandenen aus Update unten in Excel geschrieben\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Einspielen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_aleph_einspielen = df_result_neu.loc[((df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu'))]   #das sind die komplett neuen Titel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In Aleph vorhandene Titel\n",
    "\n",
    "*Hier ist der Fall: was im Ebooks-Katalog vorhanden ist, muss gelöscht werden*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>object_id</th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sep</th>\n",
       "      <th>subtitle_sep</th>\n",
       "      <th>subtitle_comparison</th>\n",
       "      <th>subtitle_comparison2</th>\n",
       "      <th>subtitle_all</th>\n",
       "      <th>short_title</th>\n",
       "      <th>url_ges</th>\n",
       "      <th>url_ebx</th>\n",
       "      <th>Abfrage_ges</th>\n",
       "      <th>Abfrage_ebx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>685</td>\n",
       "      <td>993</td>\n",
       "      <td>11878921</td>\n",
       "      <td>9780415390798</td>\n",
       "      <td>Karl Polanyi and the Paradoxes of the Double M...</td>\n",
       "      <td></td>\n",
       "      <td>John Vail</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Studies in Social and Political Thought</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Karl Polanyi and the Paradoxes of the Double M...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Karl Polanyi and the Paradoxes of the Double M...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>743</td>\n",
       "      <td>1057</td>\n",
       "      <td>130769734</td>\n",
       "      <td>9780415858762</td>\n",
       "      <td>War as Protection and Punishment</td>\n",
       "      <td>Armed International Intervention at the 'End o...</td>\n",
       "      <td>Teresa Degenhardt</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Frontiers of Criminal Justice</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>War as Protection and Punishment</td>\n",
       "      <td></td>\n",
       "      <td>Armed International Intervention at the 'End o...</td>\n",
       "      <td></td>\n",
       "      <td>Armed International Intervention at the 'End o...</td>\n",
       "      <td>War as Protection and Punishment Armed Interna...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>832</td>\n",
       "      <td>1163</td>\n",
       "      <td>133493308</td>\n",
       "      <td>9781138784925</td>\n",
       "      <td>The Routledge Companion to Environmental Ethics</td>\n",
       "      <td></td>\n",
       "      <td>Benjamin Hale</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Philosophy Companions</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>The Routledge Companion to Environmental Ethics</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Routledge Companion to Environmental Ethic...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>988</td>\n",
       "      <td>1357</td>\n",
       "      <td>137861669</td>\n",
       "      <td>9781138293823</td>\n",
       "      <td>Law, Space, and the Vehicular Environment</td>\n",
       "      <td>Pavement and Asphalt</td>\n",
       "      <td>Sarah Marusek</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Space, Materiality and the Normative</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Law, Space, and the Vehicular Environment</td>\n",
       "      <td></td>\n",
       "      <td>Pavement and Asphalt</td>\n",
       "      <td></td>\n",
       "      <td>Pavement and Asphalt</td>\n",
       "      <td>Law, Space, and the Vehicular Environment Pave...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1217</td>\n",
       "      <td>1643</td>\n",
       "      <td>144553561</td>\n",
       "      <td>9781138491946</td>\n",
       "      <td>The Diaspora's Role in Africa</td>\n",
       "      <td>Transculturalism, Challenges, and Development</td>\n",
       "      <td>Stella-Monica N. Mpande</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Rethinking Development</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>The Diaspora's Role in Africa</td>\n",
       "      <td></td>\n",
       "      <td>Transculturalism, Challenges, and Development</td>\n",
       "      <td></td>\n",
       "      <td>Transculturalism, Challenges, and Development</td>\n",
       "      <td>The Diaspora's Role in Africa Transculturalism...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26512</th>\n",
       "      <td>31793</td>\n",
       "      <td>4782</td>\n",
       "      <td>157121198</td>\n",
       "      <td>9780367681524</td>\n",
       "      <td>Aid and Influence</td>\n",
       "      <td>Patronage, Power and Politics</td>\n",
       "      <td>Stephen Browne</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Aid and Influence</td>\n",
       "      <td></td>\n",
       "      <td>Patronage, Power and Politics</td>\n",
       "      <td></td>\n",
       "      <td>Patronage, Power and Politics</td>\n",
       "      <td>Aid and Influence Patronage, Power and Politic...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26544</th>\n",
       "      <td>31831</td>\n",
       "      <td>1559</td>\n",
       "      <td>143382178</td>\n",
       "      <td>9781138671324</td>\n",
       "      <td>African Americans and the Mississippi River</td>\n",
       "      <td>Race, History, and the Environment</td>\n",
       "      <td>Dorothy Zeisler-Vralsted</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Environmental Humanities</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>African Americans and the Mississippi River</td>\n",
       "      <td></td>\n",
       "      <td>Race, History, and the Environment</td>\n",
       "      <td></td>\n",
       "      <td>Race, History, and the Environment</td>\n",
       "      <td>African Americans and the Mississippi River Ra...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26562</th>\n",
       "      <td>31852</td>\n",
       "      <td>20020</td>\n",
       "      <td>163737954</td>\n",
       "      <td>9780367648596</td>\n",
       "      <td>Advancing Culturally Responsive Research and R...</td>\n",
       "      <td>Qualitative, Quantitative, and Mixed Methods</td>\n",
       "      <td>Penny A. Pasque</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Advancing Culturally Responsive Research and R...</td>\n",
       "      <td></td>\n",
       "      <td>Qualitative, Quantitative, and Mixed Methods</td>\n",
       "      <td></td>\n",
       "      <td>Qualitative, Quantitative, and Mixed Methods</td>\n",
       "      <td>Advancing Culturally Responsive Research and R...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26620</th>\n",
       "      <td>31918</td>\n",
       "      <td>29345</td>\n",
       "      <td>165866839</td>\n",
       "      <td>9780367419905</td>\n",
       "      <td>Abolish Criminology</td>\n",
       "      <td></td>\n",
       "      <td>Viviane Saleh-Hanna</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Studies in Penal Abolition and Trans...</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Abolish Criminology</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Abolish Criminology  / Viviane Saleh-Hanna</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26687</th>\n",
       "      <td>32009</td>\n",
       "      <td>16358</td>\n",
       "      <td>163044079</td>\n",
       "      <td>9780367861445</td>\n",
       "      <td>A Fresh Look at Fraud</td>\n",
       "      <td>Theoretical and Applied Perspectives</td>\n",
       "      <td>Yaniv Hanoch</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>A Fresh Look at Fraud</td>\n",
       "      <td></td>\n",
       "      <td>Theoretical and Applied Perspectives</td>\n",
       "      <td></td>\n",
       "      <td>Theoretical and Applied Perspectives</td>\n",
       "      <td>A Fresh Look at Fraud Theoretical and Applied ...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index  object_id       isbn_ean  \\\n",
       "536        685    993   11878921  9780415390798   \n",
       "586        743   1057  130769734  9780415858762   \n",
       "665        832   1163  133493308  9781138784925   \n",
       "804        988   1357  137861669  9781138293823   \n",
       "1011      1217   1643  144553561  9781138491946   \n",
       "...        ...    ...        ...            ...   \n",
       "26512    31793   4782  157121198  9780367681524   \n",
       "26544    31831   1559  143382178  9781138671324   \n",
       "26562    31852  20020  163737954  9780367648596   \n",
       "26620    31918  29345  165866839  9780367419905   \n",
       "26687    32009  16358  163044079  9780367861445   \n",
       "\n",
       "                                                   title  \\\n",
       "536    Karl Polanyi and the Paradoxes of the Double M...   \n",
       "586                     War as Protection and Punishment   \n",
       "665      The Routledge Companion to Environmental Ethics   \n",
       "804            Law, Space, and the Vehicular Environment   \n",
       "1011                       The Diaspora's Role in Africa   \n",
       "...                                                  ...   \n",
       "26512                                  Aid and Influence   \n",
       "26544        African Americans and the Mississippi River   \n",
       "26562  Advancing Culturally Responsive Research and R...   \n",
       "26620                                Abolish Criminology   \n",
       "26687                              A Fresh Look at Fraud   \n",
       "\n",
       "                                                subtitle  \\\n",
       "536                                                        \n",
       "586    Armed International Intervention at the 'End o...   \n",
       "665                                                        \n",
       "804                                 Pavement and Asphalt   \n",
       "1011       Transculturalism, Challenges, and Development   \n",
       "...                                                  ...   \n",
       "26512                      Patronage, Power and Politics   \n",
       "26544                 Race, History, and the Environment   \n",
       "26562       Qualitative, Quantitative, and Mixed Methods   \n",
       "26620                                                      \n",
       "26687               Theoretical and Applied Perspectives   \n",
       "\n",
       "                    contributor  publisher  \\\n",
       "536                   John Vail  Routledge   \n",
       "586           Teresa Degenhardt  Routledge   \n",
       "665               Benjamin Hale  Routledge   \n",
       "804               Sarah Marusek  Routledge   \n",
       "1011    Stella-Monica N. Mpande  Routledge   \n",
       "...                         ...        ...   \n",
       "26512            Stephen Browne  Routledge   \n",
       "26544  Dorothy Zeisler-Vralsted  Routledge   \n",
       "26562           Penny A. Pasque  Routledge   \n",
       "26620       Viviane Saleh-Hanna  Routledge   \n",
       "26687              Yaniv Hanoch  Routledge   \n",
       "\n",
       "                                                  series thesis  ...  \\\n",
       "536    Routledge Studies in Social and Political Thought         ...   \n",
       "586              Routledge Frontiers of Criminal Justice         ...   \n",
       "665                      Routledge Philosophy Companions         ...   \n",
       "804                 Space, Materiality and the Normative         ...   \n",
       "1011                              Rethinking Development         ...   \n",
       "...                                                  ...    ...  ...   \n",
       "26512                                                            ...   \n",
       "26544                 Routledge Environmental Humanities         ...   \n",
       "26562                                                            ...   \n",
       "26620  Routledge Studies in Penal Abolition and Trans...         ...   \n",
       "26687                                                            ...   \n",
       "\n",
       "                                               title_sep subtitle_sep  \\\n",
       "536    Karl Polanyi and the Paradoxes of the Double M...                \n",
       "586                     War as Protection and Punishment                \n",
       "665      The Routledge Companion to Environmental Ethics                \n",
       "804            Law, Space, and the Vehicular Environment                \n",
       "1011                       The Diaspora's Role in Africa                \n",
       "...                                                  ...          ...   \n",
       "26512                                  Aid and Influence                \n",
       "26544        African Americans and the Mississippi River                \n",
       "26562  Advancing Culturally Responsive Research and R...                \n",
       "26620                                Abolish Criminology                \n",
       "26687                              A Fresh Look at Fraud                \n",
       "\n",
       "                                     subtitle_comparison subtitle_comparison2  \\\n",
       "536                                                                             \n",
       "586    Armed International Intervention at the 'End o...                        \n",
       "665                                                                             \n",
       "804                                 Pavement and Asphalt                        \n",
       "1011       Transculturalism, Challenges, and Development                        \n",
       "...                                                  ...                  ...   \n",
       "26512                      Patronage, Power and Politics                        \n",
       "26544                 Race, History, and the Environment                        \n",
       "26562       Qualitative, Quantitative, and Mixed Methods                        \n",
       "26620                                                                           \n",
       "26687               Theoretical and Applied Perspectives                        \n",
       "\n",
       "                                            subtitle_all  \\\n",
       "536                                                        \n",
       "586    Armed International Intervention at the 'End o...   \n",
       "665                                                        \n",
       "804                                 Pavement and Asphalt   \n",
       "1011       Transculturalism, Challenges, and Development   \n",
       "...                                                  ...   \n",
       "26512                      Patronage, Power and Politics   \n",
       "26544                 Race, History, and the Environment   \n",
       "26562       Qualitative, Quantitative, and Mixed Methods   \n",
       "26620                                                      \n",
       "26687               Theoretical and Applied Perspectives   \n",
       "\n",
       "                                             short_title  \\\n",
       "536    Karl Polanyi and the Paradoxes of the Double M...   \n",
       "586    War as Protection and Punishment Armed Interna...   \n",
       "665    The Routledge Companion to Environmental Ethic...   \n",
       "804    Law, Space, and the Vehicular Environment Pave...   \n",
       "1011   The Diaspora's Role in Africa Transculturalism...   \n",
       "...                                                  ...   \n",
       "26512  Aid and Influence Patronage, Power and Politic...   \n",
       "26544  African Americans and the Mississippi River Ra...   \n",
       "26562  Advancing Culturally Responsive Research and R...   \n",
       "26620         Abolish Criminology  / Viviane Saleh-Hanna   \n",
       "26687  A Fresh Look at Fraud Theoretical and Applied ...   \n",
       "\n",
       "                                                 url_ges  \\\n",
       "536    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "586    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "665    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "804    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "1011   http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "...                                                  ...   \n",
       "26512  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26544  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26562  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26620  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26687  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "\n",
       "                                                 url_ebx  Abfrage_ges  \\\n",
       "536    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "586    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "665    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "804    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "1011   http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "...                                                  ...          ...   \n",
       "26512  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26544  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26562  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26620  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26687  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "\n",
       "      Abfrage_ebx  \n",
       "536           neu  \n",
       "586           neu  \n",
       "665           neu  \n",
       "804           neu  \n",
       "1011          neu  \n",
       "...           ...  \n",
       "26512         neu  \n",
       "26544         neu  \n",
       "26562         neu  \n",
       "26620         neu  \n",
       "26687         neu  \n",
       "\n",
       "[540 rows x 40 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_nicht_einspielen = df_result_in_aleph.drop(df_result_in_aleph[(df_result_in_aleph[\"Abfrage_ebx\"] == 'neu') & (df_result_in_aleph[\"Abfrage_ges\"] == 'neu')].index) \n",
    "df_in_aleph_nicht_einspielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen der Datei zum Löschen der Titel\n",
    "\n",
    "with open(\"./output/ges02_weg\", \"a\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_nicht_einspielen.index:\n",
    "        fa.write(df_in_aleph_nicht_einspielen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Statistik und Kontrolle zusammenführen aller Titel, die nicht eingespielt werden und Ausgabe in einer CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gesamt_nicht_einspielen = df_in_aleph_nicht_einspielen.append(df_neu_nicht_einspielen)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y_%m_%d\")                                              # Zeit erfassen für Dateibenennung\n",
    "\n",
    "df_gesamt_nicht_einspielen[\"object_id\"] = df_gesamt_nicht_einspielen.object_id.astype(str)  # wandelt die spalte von Int64 zu Object um, so dass es in Excel korrekt eingelesen wird\n",
    "df_gesamt_nicht_einspielen[\"isbn_ean\"] = df_gesamt_nicht_einspielen.isbn_ean.astype(str)\n",
    "df_gesamt_nicht_einspielen = df_gesamt_nicht_einspielen.drop(columns=[\"url_ebx\", \"url_ges\", \"cover\", \"title_sep\", \"subtitle_comparison\", \"subtitle_comparison2\", \"subtitle_all\", \"subtitle_sep\"]) # unnötige Spalten entfernen\n",
    "\n",
    "df_neu_nicht_einspielen.to_csv('./output/Vorhandene_Titel_'+date+'.csv')   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Updaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update = df_result_in_aleph.loc[(df_result_in_aleph[\"Abfrage_ebx\"]== 'neu') & (df_result_in_aleph[\"Abfrage_ges\"] == 'neu')] # prüfen, ob es wirklich ein Datenupdate gab, sonst nicht neu einspielen??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Für die Logdatei Ermittlung verschiedener Zahlen und hier zur direkten Ansicht ausgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kleine Statistik:\n",
      "===================================== \n",
      "Gelieferte Datensätze:              38142 \n",
      "-------------------------------------------- \n",
      "Sätze ohne Dubletten                26080 \n",
      "   Dubletten:               12062 \n",
      "   Auswahl zum Einspielen   5974 \n",
      "   in Aleph löschen         0 \n",
      "Zu prüfende Datensätze:             32054 \n",
      "-------------------------------------------- \n",
      "Prüfung Titel in Aleph:             26725 \n",
      "   Davon als Update         26185 \n",
      "   Davon in Aleph löschen   540 \n",
      "Prüfung neue Titel    :             5329 \n",
      "   Davon ganz neu           3098 \n",
      "   Davon bereits Bestand    2231 \n",
      "-------------------------------------------- \n",
      "Titel, die in Aleph verarbeitet werden:  29283 \n",
      "-------------------------------------------- \n",
      "Datensätze GES02 vor Einspielen:    31360 \n",
      "Nicht mehr im Export               - 3 \n",
      "Titel in Aleph löschen, da Bestand - 540 \n",
      "Zu löschen, da Dublette            - 0 \n",
      "Neue Titel für Aleph               + 3098 \n",
      "-------------------------------------------- \n",
      "In Aleph nach Einspielen:           33915\n"
     ]
    }
   ],
   "source": [
    "#Kontrollmechanismus, ob für alle Titel auch Treffer da sind\n",
    "x = df.shape[0]\n",
    "a = df_ohne_dubletten.shape[0]\n",
    "c = df_dubletten.shape[0]\n",
    "b = df_dubl_einspielen.shape[0] #Auswahl der neuen Treffer\n",
    "m = df_in_aleph_nicht_einspielen.shape[0]\n",
    "n = df_ganz_neu.shape[0]\n",
    "h = df_neu_aleph_einspielen.shape[0]\n",
    "j = df_neu_nicht_einspielen.shape[0]\n",
    "o = df_in_aleph_update.shape[0]\n",
    "g = df_in_aleph.shape[0]\n",
    "k = df_alephIDs.shape[0]\n",
    "z = df_neu_aleph_einspielen.shape[0]       #neu ermitteln aus neuen\n",
    "l = df_aleph_loeschen.shape[0]\n",
    "y = df_dubl_nicht_einspielen.shape[0]\n",
    "\n",
    "\n",
    "print('Kleine Statistik:\\n=====================================',\n",
    "    '\\nGelieferte Datensätze:             ', x,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nSätze ohne Dubletten               ', a,\n",
    "    '\\n   Dubletten:              ', c,\n",
    "    '\\n   Auswahl zum Einspielen  ', b,\n",
    "    '\\n   in Aleph löschen        ', y,\n",
    "    '\\nZu prüfende Datensätze:            ', a+b,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nPrüfung Titel in Aleph:            ', g,\n",
    "    '\\n   Davon als Update        ', o,\n",
    "    '\\n   Davon in Aleph löschen  ', m ,    \n",
    "    '\\nPrüfung neue Titel    :            ', n,\n",
    "    '\\n   Davon ganz neu          ', h,\n",
    "    '\\n   Davon bereits Bestand   ', j,      \n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nTitel, die in Aleph verarbeitet werden: ', o+h,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nDatensätze GES02 vor Einspielen:   ', k,\n",
    "    '\\nNicht mehr im Export               -', l,\n",
    "    '\\nTitel in Aleph löschen, da Bestand -', m,\n",
    "    '\\nZu löschen, da Dublette            -', y,\n",
    "    '\\nNeue Titel für Aleph               +', h,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nIn Aleph nach Einspielen:          ', k-l-m-y+h,)\n",
    "\n",
    "#hier entsprechende Einträge für die Log-Datei\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:\n",
    "    log.write(\"\\nGelieferte Datensätze:             \" + str(x))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nSätze ohne Dubletten               \" + str(a))\n",
    "    log.write(\"\\n   Dubletten:              \" + str(c))\n",
    "    log.write(\"\\n   Davon in Aleph löeschen \" + str(y))\n",
    "    log.write(\"\\n   Auswahl zum Einspielen  \" + str(b))\n",
    "    log.write(\"\\nZu prüfende Datensätze:            \" + str(a+b))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nPrüfung Titel in Aleph:            \" + str(g))\n",
    "    log.write(\"\\n   Davon als Update        \" + str(o))\n",
    "    log.write(\"\\n   Davon in Aleph löschen  \" + str(m) + \"    \\n\")   \n",
    "    log.write(\"\\nPrüfung neue Titel    :            \" + str(n))\n",
    "    log.write(\"\\n   Davon ganz neu          \" + str(h))\n",
    "    log.write(\"\\n   Davon bereits Bestand   \" + str(j) + \"    \\n\")     \n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nTitel, die in Aleph verarbeitet werden: \" + str(o+h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nDatensätze GES02 vor Einspielen:   \" + str(k))\n",
    "    log.write(\"\\nNicht mehr im Export               -\" + str(l))\n",
    "    log.write(\"\\nZu löschender dubletter Titel      -\" + str(y))\n",
    "    log.write(\"\\nTitel in Aleph löschen, da Bestand -\" + str(m))\n",
    "    log.write(\"\\nNeue Titel für Aleph               +\" + str(h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nIn Aleph nach Einspielen:          \" + str(k-l-m-y+h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Exportdateien Aufbereiten\n",
    "\n",
    "#### Zielformat für das Einspielen in Aleph:\n",
    "\n",
    "    000000001 LDR   L -----nM2.01200024------h              \n",
    "    000000001 020   L $$a (object_id))\n",
    "    000000001 030   L $$aaz||rrrza||||\n",
    "    000000001 051   L $$am|||||||\n",
    "    000000001 070   L $$aSchweitzer\n",
    "    000000001 077   L $$aMonographie\n",
    "    000000001 078   L $$aSchweitzer\n",
    "    000000001 082   L $$azum Bestellen\n",
    "    000000001 100   L $$a (contributor_1)\n",
    "    000000001 104   L $$a (contributor_2)\n",
    "    000000001 108   L $$a (contributor_3)\n",
    "    000000001 331   L $$a (title_sep)\n",
    "    000000001 335   L $$a (subtitle_all)\n",
    "    000000001 403   L $$a (edition_number / edition_text)  #noch prüfen, was besser zu verwenden ist \n",
    "    000000001 419   L $$b (publisher) $$a (date_combined)\n",
    "    000000001 433   L $$a (pages)\n",
    "    000000001 451   L $$a (series)\n",
    "    000000001 520   L $$a (thesis)\n",
    "    000000001 540   L $$a (isbn_ean)\n",
    "    000000001 656   L $$a (cover)\n",
    "    000000001 750   L $$a (description)\n",
    "    000000001 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch        \n",
    "    \n",
    "Anmerkung zum Feld 655: die URL wird NACH dem Einspielen in Aleph mit der Datensatz-ID angereichert (siehe Juypter-Notebook \"Link-Anreicherung\"), um einen klaren Bestellink für den Kaufvorschlag zu haben\n",
    "\n",
    "*Hierfür werden immer die Feldbenennung und bestimmte Codierungen VOR den Inhalt - in Klammern de Bezeichnung der entsprechenden Spalte - gesetzt, bzw. erfoderliche Felder komplett neu hinzugefügt.   \n",
    "Am Anfang jeder Zeile braucht Aleph eine 9-Stellige eindeutige Zahl pro Titel.*   \n",
    "\n",
    "*Manchmal ließ sich der Inhalt einer Spalte direkt in die Datei schreiben, manchmal musst die Spalte zuvor über apply aufbereitet werden.* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aufbereiten der neuen Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen = df_neu_aleph_einspielen           #zur vereinfachten Wiederverwertung des alten Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"020\"] = df_aleph_einspielen[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_aleph_einspielen[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung der Personendaten\n",
    "\n",
    "*Da bis zu 3 Personen in einer Spalte zu finden sind, werden diese im Discovery nicht getrennt suchbar, darum werden sie gesplittet. Für die Dublettenkontrolle hat sich das als irrelevant erwiesen, darum erfolgt dieser Schritt erst hier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = df_aleph_einspielen[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= person[0]\n",
    "df_aleph_einspielen[\"contributor_2\"]= person[1]\n",
    "df_aleph_einspielen[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= df_aleph_einspielen[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_2\"]= df_aleph_einspielen[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_3\"]= df_aleph_einspielen[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung des Erscheinungsdatum und Erscheinungsjahres\n",
    "\n",
    "*In der Auswahl unserer Titel befinden sich auch im Erscheinen befindliche Titel der kommenden Monate. Diese Information möchten wir gerne im Discovery sichtbar machen. Hierfür bleibt uns nur Aleph-Feld 419c, das dem Erscheinungsjahr vorbehalten ist.   \n",
    "Wunsch ist es: Wenn des Erscheinungsdatum weiter als 10 Tage weg vom heutigen Datum ist, soll das komplette Datum angezeigt werden, ansonsten nur das Erscheinungsjahr.*\n",
    "\n",
    "Zur Umsetzung muss die Spalte \"publication_date\" in ein Datum verwandelt werden und nach den genannten Kriterien unterschiedlich angezeigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"publ_data_repl\"]= df_aleph_einspielen[\"publication_date\"].astype(str).str.replace('00','01') # korrektur komischer Daten\n",
    "#df_aleph_einspielen[\"publ_data_repl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"publ_date_date\"] = pd.to_datetime(df_aleph_einspielen[\"publ_data_repl\"].astype(str), format='%Y%m%d', errors='coerce') #Umwandlung in Datum (Zahl geht nicht)\n",
    "#df_aleph_einspielen[\"publ_date_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2025-12-31\n",
       "1              2022\n",
       "2              2022\n",
       "3        2026-06-01\n",
       "4              2022\n",
       "            ...    \n",
       "26262          2022\n",
       "26263    2025-03-20\n",
       "26264          2025\n",
       "26265          2024\n",
       "26266    2025-04-01\n",
       "Name: date_combined, Length: 26267, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = pd.Timestamp(datetime.today().date())  # Heute \n",
    "\n",
    "def transform_date(date):\n",
    "    if date > today:\n",
    "        return date.strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        return date.year\n",
    "\n",
    "# Apply the function to create the new column\n",
    "df_aleph_einspielen['date_combined'] = df_aleph_einspielen['publ_date_date'].apply(transform_date)\n",
    "#df_aleph_einspielen['date_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9783406582387\n",
       "1        9783406753732\n",
       "2        9783406767890\n",
       "3        9783848728732\n",
       "4        9783170314221\n",
       "             ...      \n",
       "26262    9783030875183\n",
       "26263    9781350242401\n",
       "26264    9781009523103\n",
       "26265    9781538180631\n",
       "26266    9781961856783\n",
       "Name: isbn_ean, Length: 26267, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_einspielen['isbn_ean'] = df_aleph_einspielen['isbn_ean'].astype(np.int64)   #da die Zahl als Object genommen wurde, bekam sie ein .0 angehängt, das ist durch umwandeln in Zahl weg\n",
    "\n",
    "df_aleph_einspielen['isbn_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"419b\"] = df_aleph_einspielen[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_aleph_einspielen[\"419c\"] = df_aleph_einspielen[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_aleph_einspielen[\"419\"] = df_aleph_einspielen[\"419b\"]+df_aleph_einspielen[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"403\"] = df_aleph_einspielen[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_aleph_einspielen[\"433\"] = df_aleph_einspielen[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_aleph_einspielen[\"451\"] = df_aleph_einspielen[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_aleph_einspielen[\"520\"] = df_aleph_einspielen[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_aleph_einspielen[\"540\"] = df_aleph_einspielen[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_aleph_einspielen[\"656\"] = df_aleph_einspielen[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_aleph_einspielen[\"750\"] = df_aleph_einspielen[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Für das Durchzählen der Titel braucht es eine neue Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier entsteht eine neue Spalte mit Zahlen ab 1 durchgehend gezählt, die für den korrekten Import der Daten in Aleph nötig ist\n",
    "x = df_aleph_einspielen.shape[0]   \n",
    "df_aleph_einspielen[\"id\"] = range(1,x+1)                                                       #Notwendig ist die Zählung ab 1, da Aleph sonst nicht korrekt einließt\n",
    "df_aleph_einspielen[\"id\"] = df_aleph_einspielen[\"id\"].apply(lambda x: f\"{x:09d}\")              #Die Zahl muss 9-Stellig aufgefüllt werden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Vorbereitungen abgeschlossen, jetzt das Schreiben der Datei im Aleph-Sequential-Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./output/ges02_neu\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_einspielen.index:\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' LDR   L -----nM2.01200024------h'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"020\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 030   L $$aaz||rrrza||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 051   L $$am|||||||m|||||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 077   L $$aMonographie'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 078   L $$aSchweitzer'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 082   L $$azum Bestellen'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 100   L $$a'+df_aleph_einspielen[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 104   L $$a'+df_aleph_einspielen[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 108   L $$a'+df_aleph_einspielen[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 331   L $$a'+df_aleph_einspielen[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 335   L $$a'+df_aleph_einspielen[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"403\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"419\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"433\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"451\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"520\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"540\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"750\"][i]+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Daten für Update\n",
    "\n",
    "*es wird anhand der Spalte \"last_modified\" geprüft, ob die Titel seit dem letzten Einspielen, bzw. seit 30 Tagen ein Update erfahren haben*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies mache ich, um die Menge zum Updaten zu reduzieren. Es werden nur die rausgezogen, die tatsächlich ein neueres Aktualisierungsdatum haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2025-02-11 17:29:59.012199')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_month_ago = pd.Timestamp(one_month_ago)\n",
    "one_month_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2025-03-09 21:36:50\n",
       "1       2025-02-07 06:11:51\n",
       "2       2025-02-11 05:29:42\n",
       "3       2025-02-18 06:40:09\n",
       "4       2025-02-20 20:40:50\n",
       "                ...        \n",
       "27679   2025-02-23 20:07:03\n",
       "27680   2025-02-28 17:30:14\n",
       "27681   2025-02-26 17:28:55\n",
       "27682   2025-02-12 02:13:37\n",
       "27683   2025-02-27 17:56:55\n",
       "Name: last_modified, Length: 27684, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update['last_modified'] = pd.to_datetime(df_in_aleph_update.last_modified)          #Umwandlung, da Spalteninhalt object ist\n",
    "df_in_aleph_update['last_modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>object_id</th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>...</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "      <th>LEN</th>\n",
       "      <th>title_sep</th>\n",
       "      <th>subtitle_sep</th>\n",
       "      <th>subtitle_comparison</th>\n",
       "      <th>subtitle_comparison2</th>\n",
       "      <th>subtitle_all</th>\n",
       "      <th>short_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2522807</td>\n",
       "      <td>9783848768073</td>\n",
       "      <td>Klimaschutzrecht</td>\n",
       "      <td>Bundes-Klimaschutzgesetz | Landesklimagesetze ...</td>\n",
       "      <td></td>\n",
       "      <td>Nomos</td>\n",
       "      <td>NomosHandkommentar</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2522807</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Klimaschutzrecht</td>\n",
       "      <td></td>\n",
       "      <td>Bundes-Klimaschutzgesetz | Landesklimagesetze ...</td>\n",
       "      <td></td>\n",
       "      <td>Bundes-Klimaschutzgesetz | Landesklimagesetze ...</td>\n",
       "      <td>Klimaschutzrecht Bundes-Klimaschutzgesetz | La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2308946</td>\n",
       "      <td>9783848728497</td>\n",
       "      <td>Rehabilitation und Teilhabe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Nomos</td>\n",
       "      <td>Kompendien der Sozialen Arbeit Band 5</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2308946</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Rehabilitation und Teilhabe</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Rehabilitation und Teilhabe  /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2489418</td>\n",
       "      <td>9783825254476</td>\n",
       "      <td>Biodiversität? Frag doch einfach!</td>\n",
       "      <td>Klare Antworten aus erster Hand</td>\n",
       "      <td>Heike Feldhaar</td>\n",
       "      <td>UTB</td>\n",
       "      <td>Frag doch einfach!</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2489418</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Biodiversität? Frag doch einfach!</td>\n",
       "      <td></td>\n",
       "      <td>Klare Antworten aus erster Hand</td>\n",
       "      <td></td>\n",
       "      <td>Klare Antworten aus erster Hand</td>\n",
       "      <td>Biodiversität? Frag doch einfach! Klare Antwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2220214</td>\n",
       "      <td>9783406701948</td>\n",
       "      <td>Besteuerung privater Kapitalanlagen</td>\n",
       "      <td>Finanzinstrumente, Investmentanteile, Immobili...</td>\n",
       "      <td>Hans-Jürgen A. Feyerabend</td>\n",
       "      <td>C.H.BECK</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2220214</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Besteuerung privater Kapitalanlagen</td>\n",
       "      <td></td>\n",
       "      <td>Finanzinstrumente, Investmentanteile, Immobili...</td>\n",
       "      <td></td>\n",
       "      <td>Finanzinstrumente, Investmentanteile, Immobili...</td>\n",
       "      <td>Besteuerung privater Kapitalanlagen Finanzinst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>218910</td>\n",
       "      <td>9783406645877</td>\n",
       "      <td>Grundkurs Steuerrecht</td>\n",
       "      <td></td>\n",
       "      <td>Rainer Wernsmann;Christian Thiemann</td>\n",
       "      <td>C.H.BECK</td>\n",
       "      <td>Grundkurse</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a218910</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Grundkurs Steuerrecht</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Grundkurs Steuerrecht  / Rainer Wernsmann;Chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>53939</td>\n",
       "      <td>7266</td>\n",
       "      <td>158629965</td>\n",
       "      <td>9783110710045</td>\n",
       "      <td>(Re-)Mobilizing Voters in Britain and the Unit...</td>\n",
       "      <td>Political Strategies from Parties and Grassroo...</td>\n",
       "      <td>Gregory Benedetti</td>\n",
       "      <td>De Gruyter Oldenbourg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a158629965</td>\n",
       "      <td>12.0</td>\n",
       "      <td>(Re-)Mobilizing Voters in Britain and the Unit...</td>\n",
       "      <td></td>\n",
       "      <td>Political Strategies from Parties and Grassroo...</td>\n",
       "      <td></td>\n",
       "      <td>Political Strategies from Parties and Grassroo...</td>\n",
       "      <td>(Re-)Mobilizing Voters in Britain and the Unit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27680</th>\n",
       "      <td>53941</td>\n",
       "      <td>54779</td>\n",
       "      <td>169951393</td>\n",
       "      <td>9781032411460</td>\n",
       "      <td>(Co)Designing Hope</td>\n",
       "      <td>Aqueous Landscapes in Transition</td>\n",
       "      <td>Laura Cipriani</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a169951393</td>\n",
       "      <td>12.0</td>\n",
       "      <td>(Co)Designing Hope</td>\n",
       "      <td></td>\n",
       "      <td>Aqueous Landscapes in Transition</td>\n",
       "      <td></td>\n",
       "      <td>Aqueous Landscapes in Transition</td>\n",
       "      <td>(Co)Designing Hope Aqueous Landscapes in Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27681</th>\n",
       "      <td>53945</td>\n",
       "      <td>51527</td>\n",
       "      <td>169483493</td>\n",
       "      <td>9781032520728</td>\n",
       "      <td>'Just Like Us'?: The Politics of Ministerial P...</td>\n",
       "      <td>The Politics of Ministerial Promotion in UK Go...</td>\n",
       "      <td>Bill Jones</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Studies in British Politics</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a169483493</td>\n",
       "      <td>12.0</td>\n",
       "      <td>'Just Like Us'?</td>\n",
       "      <td>The Politics of Ministerial Promotion in UK G...</td>\n",
       "      <td>The Politics of Ministerial Promotion in UK Go...</td>\n",
       "      <td></td>\n",
       "      <td>The Politics of Ministerial Promotion in UK Go...</td>\n",
       "      <td>'Just Like Us'? The Politics of Ministerial Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27682</th>\n",
       "      <td>53946</td>\n",
       "      <td>20200</td>\n",
       "      <td>163496075</td>\n",
       "      <td>9783030962708</td>\n",
       "      <td>'Going Native?'</td>\n",
       "      <td>Settler Colonialism and Food</td>\n",
       "      <td>Ronald Ranta</td>\n",
       "      <td>Palgrave Macmillan</td>\n",
       "      <td>Food and Identity in a Globalising World</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a163496075</td>\n",
       "      <td>12.0</td>\n",
       "      <td>'Going Native?'</td>\n",
       "      <td></td>\n",
       "      <td>Settler Colonialism and Food</td>\n",
       "      <td></td>\n",
       "      <td>Settler Colonialism and Food</td>\n",
       "      <td>'Going Native?' Settler Colonialism and Food /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27683</th>\n",
       "      <td>53947</td>\n",
       "      <td>32117</td>\n",
       "      <td>165662822</td>\n",
       "      <td>9781350276765</td>\n",
       "      <td>'Economy' in European History</td>\n",
       "      <td>Words, Contexts and Change over Time</td>\n",
       "      <td>Luigi Alonzi</td>\n",
       "      <td>Bloomsbury Academic</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a165662822</td>\n",
       "      <td>12.0</td>\n",
       "      <td>'Economy' in European History</td>\n",
       "      <td></td>\n",
       "      <td>Words, Contexts and Change over Time</td>\n",
       "      <td></td>\n",
       "      <td>Words, Contexts and Change over Time</td>\n",
       "      <td>'Economy' in European History Words, Contexts ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15411 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index  object_id       isbn_ean  \\\n",
       "1            1      1    2522807  9783848768073   \n",
       "2            2      2    2308946  9783848728497   \n",
       "3            3      3    2489418  9783825254476   \n",
       "4            4      4    2220214  9783406701948   \n",
       "5            5      5     218910  9783406645877   \n",
       "...        ...    ...        ...            ...   \n",
       "27679    53939   7266  158629965  9783110710045   \n",
       "27680    53941  54779  169951393  9781032411460   \n",
       "27681    53945  51527  169483493  9781032520728   \n",
       "27682    53946  20200  163496075  9783030962708   \n",
       "27683    53947  32117  165662822  9781350276765   \n",
       "\n",
       "                                                   title  \\\n",
       "1                                       Klimaschutzrecht   \n",
       "2                            Rehabilitation und Teilhabe   \n",
       "3                      Biodiversität? Frag doch einfach!   \n",
       "4                    Besteuerung privater Kapitalanlagen   \n",
       "5                                  Grundkurs Steuerrecht   \n",
       "...                                                  ...   \n",
       "27679  (Re-)Mobilizing Voters in Britain and the Unit...   \n",
       "27680                                 (Co)Designing Hope   \n",
       "27681  'Just Like Us'?: The Politics of Ministerial P...   \n",
       "27682                                    'Going Native?'   \n",
       "27683                      'Economy' in European History   \n",
       "\n",
       "                                                subtitle  \\\n",
       "1      Bundes-Klimaschutzgesetz | Landesklimagesetze ...   \n",
       "2                                                          \n",
       "3                        Klare Antworten aus erster Hand   \n",
       "4      Finanzinstrumente, Investmentanteile, Immobili...   \n",
       "5                                                          \n",
       "...                                                  ...   \n",
       "27679  Political Strategies from Parties and Grassroo...   \n",
       "27680                   Aqueous Landscapes in Transition   \n",
       "27681  The Politics of Ministerial Promotion in UK Go...   \n",
       "27682                       Settler Colonialism and Food   \n",
       "27683               Words, Contexts and Change over Time   \n",
       "\n",
       "                               contributor              publisher  \\\n",
       "1                                                           Nomos   \n",
       "2                                                           Nomos   \n",
       "3                           Heike Feldhaar                    UTB   \n",
       "4                Hans-Jürgen A. Feyerabend               C.H.BECK   \n",
       "5      Rainer Wernsmann;Christian Thiemann               C.H.BECK   \n",
       "...                                    ...                    ...   \n",
       "27679                    Gregory Benedetti  De Gruyter Oldenbourg   \n",
       "27680                       Laura Cipriani              Routledge   \n",
       "27681                           Bill Jones              Routledge   \n",
       "27682                         Ronald Ranta     Palgrave Macmillan   \n",
       "27683                         Luigi Alonzi    Bloomsbury Academic   \n",
       "\n",
       "                                         series thesis  ... Field  L  \\\n",
       "1                            NomosHandkommentar         ...   020  L   \n",
       "2         Kompendien der Sozialen Arbeit Band 5         ...   020  L   \n",
       "3                            Frag doch einfach!         ...   020  L   \n",
       "4                                                       ...   020  L   \n",
       "5                                    Grundkurse         ...   020  L   \n",
       "...                                         ...    ...  ...   ... ..   \n",
       "27679                                                   ...   020  L   \n",
       "27680                                                   ...   020  L   \n",
       "27681     Routledge Studies in British Politics         ...   020  L   \n",
       "27682  Food and Identity in a Globalising World         ...   020  L   \n",
       "27683                                                   ...   020  L   \n",
       "\n",
       "            Content   LEN                                          title_sep  \\\n",
       "1        $$a2522807  10.0                                   Klimaschutzrecht   \n",
       "2        $$a2308946  10.0                        Rehabilitation und Teilhabe   \n",
       "3        $$a2489418  10.0                  Biodiversität? Frag doch einfach!   \n",
       "4        $$a2220214  10.0                Besteuerung privater Kapitalanlagen   \n",
       "5         $$a218910   9.0                              Grundkurs Steuerrecht   \n",
       "...             ...   ...                                                ...   \n",
       "27679  $$a158629965  12.0  (Re-)Mobilizing Voters in Britain and the Unit...   \n",
       "27680  $$a169951393  12.0                                 (Co)Designing Hope   \n",
       "27681  $$a169483493  12.0                                    'Just Like Us'?   \n",
       "27682  $$a163496075  12.0                                    'Going Native?'   \n",
       "27683  $$a165662822  12.0                      'Economy' in European History   \n",
       "\n",
       "                                            subtitle_sep  \\\n",
       "1                                                          \n",
       "2                                                          \n",
       "3                                                          \n",
       "4                                                          \n",
       "5                                                          \n",
       "...                                                  ...   \n",
       "27679                                                      \n",
       "27680                                                      \n",
       "27681   The Politics of Ministerial Promotion in UK G...   \n",
       "27682                                                      \n",
       "27683                                                      \n",
       "\n",
       "                                     subtitle_comparison subtitle_comparison2  \\\n",
       "1      Bundes-Klimaschutzgesetz | Landesklimagesetze ...                        \n",
       "2                                                                               \n",
       "3                        Klare Antworten aus erster Hand                        \n",
       "4      Finanzinstrumente, Investmentanteile, Immobili...                        \n",
       "5                                                                               \n",
       "...                                                  ...                  ...   \n",
       "27679  Political Strategies from Parties and Grassroo...                        \n",
       "27680                   Aqueous Landscapes in Transition                        \n",
       "27681  The Politics of Ministerial Promotion in UK Go...                        \n",
       "27682                       Settler Colonialism and Food                        \n",
       "27683               Words, Contexts and Change over Time                        \n",
       "\n",
       "                                            subtitle_all  \\\n",
       "1      Bundes-Klimaschutzgesetz | Landesklimagesetze ...   \n",
       "2                                                          \n",
       "3                        Klare Antworten aus erster Hand   \n",
       "4      Finanzinstrumente, Investmentanteile, Immobili...   \n",
       "5                                                          \n",
       "...                                                  ...   \n",
       "27679  Political Strategies from Parties and Grassroo...   \n",
       "27680                   Aqueous Landscapes in Transition   \n",
       "27681  The Politics of Ministerial Promotion in UK Go...   \n",
       "27682                       Settler Colonialism and Food   \n",
       "27683               Words, Contexts and Change over Time   \n",
       "\n",
       "                                             short_title  \n",
       "1      Klimaschutzrecht Bundes-Klimaschutzgesetz | La...  \n",
       "2                        Rehabilitation und Teilhabe  /   \n",
       "3      Biodiversität? Frag doch einfach! Klare Antwor...  \n",
       "4      Besteuerung privater Kapitalanlagen Finanzinst...  \n",
       "5      Grundkurs Steuerrecht  / Rainer Wernsmann;Chri...  \n",
       "...                                                  ...  \n",
       "27679  (Re-)Mobilizing Voters in Britain and the Unit...  \n",
       "27680  (Co)Designing Hope Aqueous Landscapes in Trans...  \n",
       "27681  'Just Like Us'? The Politics of Ministerial Pr...  \n",
       "27682  'Going Native?' Settler Colonialism and Food /...  \n",
       "27683  'Economy' in European History Words, Contexts ...  \n",
       "\n",
       "[15411 rows x 36 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true = df_in_aleph_update[df_in_aleph_update['last_modified'].dt.to_period('M') == one_month_ago.to_period('M')]\n",
    "\n",
    "#df_in_aleph_update_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen der Update-Export-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"020\"] = df_in_aleph_update_true[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_in_aleph_update_true[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufbereitung Personendaten\n",
    "\n",
    "person = df_in_aleph_update_true[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= person[0]\n",
    "df_in_aleph_update_true[\"contributor_2\"]= person[1]\n",
    "df_in_aleph_update_true[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= df_in_aleph_update_true[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_2\"]= df_in_aleph_update_true[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_3\"]= df_in_aleph_update_true[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neue Schritte zur Aufbereitung des Erscheinungsdatums, da der Code von oben hier nicht ging - warum auch immer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        2025-05-01\n",
       "2        2025-07-01\n",
       "3        2025-05-12\n",
       "4        2025-08-31\n",
       "5        2025-06-30\n",
       "            ...    \n",
       "27679          2024\n",
       "27680          2024\n",
       "27681          2024\n",
       "27682          2023\n",
       "27683          2023\n",
       "Name: date_combined, Length: 15411, dtype: object"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true[\"publ_data_repl\"]= df_in_aleph_update_true[\"publication_date\"].astype(str).str.replace('00','01')\n",
    "df_in_aleph_update_true[\"publ_date_date\"] = pd.to_datetime(df_in_aleph_update_true[\"publ_data_repl\"].astype(str), format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Apply the function von oben to create the new column\n",
    "df_in_aleph_update_true['date_combined'] = df_in_aleph_update_true['publ_date_date'].apply(transform_date)\n",
    "df_in_aleph_update_true['date_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024          8445\n",
       "2023          6741\n",
       "2025            70\n",
       "2025-06-01      14\n",
       "2025-12-01      13\n",
       "              ... \n",
       "2025-12-02       1\n",
       "2025-05-22       1\n",
       "2025-10-15       1\n",
       "2025-10-19       1\n",
       "2026-04-30       1\n",
       "Name: date_combined, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true['date_combined'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        9783848768073\n",
       "2        9783848728497\n",
       "3        9783825254476\n",
       "4        9783406701948\n",
       "5        9783406645877\n",
       "             ...      \n",
       "27679    9783110710045\n",
       "27680    9781032411460\n",
       "27681    9781032520728\n",
       "27682    9783030962708\n",
       "27683    9781350276765\n",
       "Name: isbn_ean, Length: 15411, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true['isbn_ean'] = df_in_aleph_update_true['isbn_ean'].astype(np.int64)   #da die Zahl als Object genommen wurde, bekam sie ein .0 angehängt, das ist durch umwandeln in Zahl weg\n",
    "\n",
    "df_in_aleph_update_true['isbn_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearbeitung der Felder\n",
    "\n",
    "df_in_aleph_update_true[\"419b\"] = df_in_aleph_update_true[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"419c\"] = df_in_aleph_update_true[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_in_aleph_update_true[\"419\"] = df_in_aleph_update_true[\"419b\"]+df_in_aleph_update_true[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte\n",
    "\n",
    "df_in_aleph_update_true[\"403\"] = df_in_aleph_update_true[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"433\"] = df_in_aleph_update_true[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_in_aleph_update_true[\"451\"] = df_in_aleph_update_true[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"520\"] = df_in_aleph_update_true[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"540\"] = df_in_aleph_update_true[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"656\"] = df_in_aleph_update_true[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_in_aleph_update_true[\"750\"] = df_in_aleph_update_true[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Ausgabedatei, hier kleine Unterschiede zu den neuen Titeln. Vorhandene ids und bestimmte Felder können nicht verändert sein, brauchen also nicht übernommen zu werden.\n",
    "\n",
    "with open(\"./output/ges02_update\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_update_true.index:\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 100   L $$a'+df_in_aleph_update_true[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 104   L $$a'+df_in_aleph_update_true[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 108   L $$a'+df_in_aleph_update_true[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 331   L $$a'+df_in_aleph_update_true[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 335   L $$a'+df_in_aleph_update_true[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"403\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"419\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"433\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"451\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"520\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"540\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"750\"][i]+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschließende Dinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abschließender Eintrag in Log-Datei\n",
    "endtime = time.strftime('%H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('\\n                                     beendet ')\n",
    "    log.write(endtime)\n",
    "    log.write(\"\\n============================================================\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopien bestimmter Daten zur Einsicht bzw. für Prüfzwecke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen.to_csv('./output/Eingespielte_Titel_'+date+'.csv') \n",
    "df_in_aleph_update_true.to_csv('./output/Update_Titel_'+date+'.csv')\n",
    "\n",
    "df_in_aleph_nicht_einspielen.to_csv('./output/Aleph_loeschen_ebx_vorh_'+date+'.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Folgende Schritte müssen mit den Dateien ausgeführt werden: \n",
    "\n",
    "\n",
    "1. Einspielen der Datei pda_ges01 als neue Titel in Aleph, hier dann auch Export der urls und Anreicherung mit der Aleph-ID mittels \"mailto_link_skript.ipynb\"\n",
    "2. Einspielen der DAtei pda_update als \"Änderungen bestehender Datensätze in Aleph\"\n",
    "3. Einspielen und löschen der Titel die in ges02_loeschen_1 und ges02_loeschen_2 vorhanden sind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "4fac973d8b48a5fcf37e7d133428a31fb47ebbd054f5d1feed8c0da486f2af46"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
