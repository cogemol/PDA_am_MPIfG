{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buchhandelsdaten in Vufind als Grundlage für PDA (Patron Driven Aquisition) am MPIfG / Version 2: Updates laden statt Vollimport\n",
    "\n",
    "Einbindung von freundlicherweise von Schweitzer Fachinformation zur Verfügung gestellten Daten.   \n",
    "(Auswahl über passend konfigurierte Neuerscheinungsabfragen in unserem Kundenprofil).   \n",
    "\n",
    "#### Gründe für die Umstellung auf Updates: Durch Einspielen der Daten in Vufind ergibt sich eine zeitliche Diskrepanz, in der Titel bereits in Aleph gelöscht sind, aber in Vufind noch bestellbar. Zudem werden die Aleph-IDs hochgezähl, denn bei jedem Einspielen werden neue Nummern vergeben. Durch die Updates bleiben die Titel weiterhin verfügbar und das Hochzählen der IDs hält sich im Rahmen. Das Skript beschleunigt sich durch Reduzierung der Abfragen am Server.\n",
    "*Kleiner Nachteil: der manuelle Aufwand vergrößert sich. Mehrere Dateien müssen in Aleph eingespielt und verarbeitet werden, aber hält sich nach wie vor im vertretbaren Rahmen.*\n",
    "\n",
    "Das Jupyter Notebook arbeitet mit Python 3.8.1 und wurde mit Visual Studio Code 1.62.3 erstellt \n",
    "\n",
    "\n",
    "#### Arbeitsschritte im Code:\n",
    "\n",
    "> Vorarbeiten:   \n",
    "  - Notwendige Pandas Libraries aufrufen\n",
    "  - Serverprüfung auf funktionierende Verbindung zum Aleph-X-Server    \n",
    "\n",
    "\n",
    "> Daten abholen und einlesen:   \n",
    "  1. Buchhandelsdaten von Schweitzer \n",
    "  2. Aleph-Konkordanz Aleph-ID /Schweitzer ID  (erzeugt tagesaktuell per p-print-03 in Aleph)\n",
    "     - Aufbereiten der Daten: Schweitzer ID extrahieren und Aleph-ID mit Nullen auffüllen\n",
    "  3. Daten zusammenführen in einem df    \n",
    "\n",
    "\n",
    "> Daten aufbereiten:   \n",
    "  1. Buchhandelsdaten prüfen und vorbereiten\n",
    "     1. Identifizierung von Titeln in Aleph, die nicht mehr im Datensatz sind und schreiben in Datei \"ges02_loeschen_1\"\n",
    "     2. Dublettencheck innerhalb der Buchhandelsdaten\n",
    "     3. Trennung der Daten in \"in Aleph vorhanden\" und \"neu\"  \n",
    "  2. Bestandsabfragen:\n",
    "     1. Ganz neue Titel   \n",
    "        - Bestandsabgleich durch Abfragen (GES und EBX) auf dem Aleph-Server   \n",
    "     2. In Aleph vorhanden\n",
    "        - Bestandsabgleich durch Abfrage EBX auf dem Aleph-Server  \n",
    "     3. Exportvorbereitungen:\n",
    "        1. Neue Titel\n",
    "        2. In Aleph vorhandene Titel   \n",
    "            Identifizierung von zu löschenden Titeln \"ges02_loeschen_2\" und der zu aktualisierenden\n",
    "  3. Exportdateien aufbereiten:\n",
    "     1. Aufbereiten neuer Titel\n",
    "     2. Aufbereiten der vorhandenen Titel   \n",
    "\n",
    "\n",
    "> Informationssammlung\n",
    "   1. Log-Datei mit Rahmendaten wird fortlaufend geschrieben\n",
    "   2. Ausgabe bestimmter Titelgruppen als csv-Datei  \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorarbeiten\n",
    "\n",
    "### Pandas Libraries laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                # für das Arbeiten mit der CSV-Datei\n",
    "import urllib.request                                              # für das Abrufen der URL\n",
    "import requests                                                    # für die Bestandsabfragen \n",
    "pd.options.mode.chained_assignment = None                          # default='warn' abschalten beim Beschreiben der neuen Spalten\n",
    "import time                                                        # für das Schreiben des Datums Logdatei und Excel-Export und Arbeiten mit dem Erscheinungsdatum\n",
    "import datetime                                                    # für das Berechnen des Updates\n",
    "import numpy as np                                                 # für das Bearbeiten von Spalten\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung, ob die Verbindung zum Aleph-Server für Abfragen korrekt funktioniert:\n",
    "\n",
    "    Nur zugelassene IPs können diese Schnittstelle abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Server antwortet korrekt\n"
     ]
    }
   ],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783482648434\"\n",
    "\n",
    "reply = requests.get(test).text\n",
    "a = reply.find(\"Forbidden\")\n",
    "b =  reply.find(\"?xml\")\n",
    "\n",
    "if (a > 50):\n",
    "    print(\"Es gibt ein Problem mit dem Server\")\n",
    "if (b == 1):\n",
    "    print(\"Der Server antwortet korrekt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version = \"1.0\" encoding = \"UTF-8\"?>\n",
      "<find>\n",
      "<set_number>921747</set_number>\n",
      "<no_records>000000001</no_records>\n",
      "<no_entries>000000001</no_entries>\n",
      "<session-id>2Y3372BU7TGHNBB4K4RKQPHNL9DQMMVU9L38TKA8LV5TC7768F</session-id>\n",
      "</find>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783865058041\"\n",
    "\n",
    "reply = requests.get(test).text \n",
    "\n",
    "print(reply) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensätze abholen und einlesen\n",
    "\n",
    "# Erste Schritte manuell durchführen, um enthaltene Prüfroutinen im Blick zu behalten\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1. Datensätze von Schweitzer\n",
    "\n",
    "*einlesen in df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./input/export.csv', <http.client.HTTPMessage at 0x21067428790>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://content.schweitzer-online.de/static/content/export/mpifg/export.csv\"  # Abruf, der von Schweitzer zur Verfügung gestellten Daten\n",
    "checkout_file = \"./input/export.csv\"  \n",
    "urllib.request.urlretrieve(url, checkout_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/export.csv', encoding = 'UTF-8', sep=';' , keep_default_na=False , low_memory=False) # muss encoding angeben und Trennzeichen, NaN (= leere Werte) direkt beim Import entfernen, da sie später Probleme machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen in Datei: 66240\n",
      "Object_IDs:      66240\n"
     ]
    }
   ],
   "source": [
    "#Aufgrund eines zusätzlichen Zeilenumbruchs im Datensatz gab es im Juni 2022 ein Problem, darum kleine Prüfroutine eingebaut ob in der Import-Datei die Anzahl der Datensätze mit der Anzahl der object-Ids übereinstimmt\n",
    "g = df.shape[0]\n",
    "h= df['object_id'].count()\n",
    "\n",
    "print(\"Zeilen in Datei:\", g)\n",
    "print(\"Object_IDs:     \", h)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ab dem nächsten Schritt kann Skript durchlaufen, Prüfroutinen sind erfolgt\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aleph-Konkordanz Aleph-ID / Schweitzer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime('%Y%m%d')  # %H:%M:%S\n",
    "input_file = './input/ids'+now      #input_file wird tagesaktuell aus Aleph gezogen und auf diesem Wege mit der entsprechenden Endung eingelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ids Field  L       Content  LEN\n",
      "0      000129218   020  L    $$a2258402   10\n",
      "1      000129219   020  L    $$a2522807   10\n",
      "2      000129220   020  L    $$a2308946   10\n",
      "3      000129221   020  L    $$a2220214   10\n",
      "4      000129222   020  L    $$a2479903   10\n",
      "...          ...   ... ..           ...  ...\n",
      "29970  000159265   020  L  $$a169951393   12\n",
      "29971  000159266   020  L  $$a163496075   12\n",
      "29972  000159267   020  L  $$a165662822   12\n",
      "29973  000159268   020  L  $$a170244345   12\n",
      "29974  000159269   020  L  $$a171634215   12\n",
      "\n",
      "[29975 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# neu einlesen der Aleph-IDs mittels Code, da sonst Object-Ids abgeschnitten wurden\n",
    "with open(input_file) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "df = pd.DataFrame(data[0:]) # 0 weil ich keinen header habe\n",
    "\n",
    "# use expand to split strings into separate columns\n",
    "df_alephIDs = df[0].str.rsplit(expand=True)\n",
    "# fix column names\n",
    "df_alephIDs.columns = [\"ids\",\"Field\",\"L\",\"Content\"]\n",
    "\n",
    "df_alephIDs['LEN'] = df_alephIDs['Content'].apply(len)\n",
    "print(df_alephIDs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Datenfelder aufbereiten: Schweitzer-ID extrahieren und Aleph-ID ins richtige Format bringen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169327230    1\n",
       "170143037    1\n",
       "166237816    1\n",
       "171411066    1\n",
       "165985917    1\n",
       "            ..\n",
       "163479759    1\n",
       "169687954    1\n",
       "166992790    1\n",
       "158715772    1\n",
       "169349122    1\n",
       "Name: object_id, Length: 29975, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alephIDs[\"object_id\"] = df_alephIDs[\"Content\"].astype(str).str.slice(start=3,stop=16).apply(int)   #Spalte mit object-Ids herausschneiden und wieder zur Zahl definieren\n",
    "\n",
    "df_alephIDs[\"object_id\"].value_counts() #Hier muss immer eins Stehen in der hinteren Spalte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Daten zu einem Frame zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführung erfolgt einmal mit concat - erlaubt die nicht mehr verwendeten Aleph-IDs zu identifizieren \n",
    "# und einmal mit join, dann erhalte ich nur die Titel, die auch im neuen Datensatz sind zur weiteren Verarbeitung!\n",
    "df_oi= df.set_index(\"object_id\")                          #object-ID zum Index für beide Datenframes\n",
    "df_aleph_oi = df_alephIDs.set_index(\"object_id\")\n",
    "df_update_aleph = pd.concat([df_oi, df_aleph_oi], axis=1)\n",
    "\n",
    "df_update_join = df_oi.join(df_aleph_oi)                    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### Wenn es bei der obigen Zelle zu Fehlermeldung kommt, dann gibt es vermutlich doppelte SChweitzer-IDs in den Daten.\n",
    "\n",
    "Das kann man beim Output der folgenden Zelle oben sehen:\n",
    "\n",
    "    #nach Dubletten vom letzten Mal vorsichtshalber, check nach doppelten object-ids\n",
    "    df_alephIDs[\"object_id\"].value_counts()\n",
    "\n",
    "Wenn da Nummern mit 2 vorhanden sind, dann muss oben der Code zur Dublettenbereinigung gestartet werden. Dies geschieht, in dem die folgende Zelle von Markdown zu Code geändert wird und durchlaufen. \n",
    "\n",
    "## Dabei wird eine weitere Datei erzeugt, die in Aleph gelöscht werden muss!!\n",
    "\n",
    "WICHTIG: Anschließend, den Code wieder in Markdown verwandeln, da er ja nur in dem Sonderfall gebraucht wird. \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code zur Bereinigung von Dubletten im in Aleph schon eingespielten Bestand \n",
    "# Einmalig aktivieren (Markdown > Code) und nutzen und dann wieder (Code > Markdown)\n",
    "\n",
    "df_aleph_einzel = df_alephIDs.drop_duplicates(\"object_id\", keep=False)  #Auslesen der Eintraege mit einzelner object_id\n",
    "df_aleph_doppelt = df_alephIDs.groupby(\"object_id\").filter(lambda g: (g.nunique() >1).any()) #Alle Sätze mit doppelten object_ids rausziehen\n",
    "df_aleph_single = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='first') #Die Dublette, die behalten wird\n",
    "df_aleph_dublette = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='last') #Die Dublette, die gelöscht wird\n",
    "# In Aleph zu löschende in eine Datei schreiben aus df_aleph_dublette\n",
    "with open(\"./output/ges02_dubl\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_dublette.index:\n",
    "        fa.write(df_aleph_dublette[\"ids\"][i]+'GES02'+'\\n')\n",
    "        \n",
    "df_aleph_vorhanden = df_aleph_einzel.append(df_aleph_single) #Einzelne und ausgewählt Dubletten zusammenfuehren\n",
    "\n",
    "df_oi= df.set_index(\"object_id\")                          #Dann nochmal den Schritt, an dem es vorhin gehakt hat\n",
    "df_aleph_oi = df_aleph_vorhanden.set_index(\"object_id\")\n",
    "df_update_aleph = pd.concat([df_oi, df_aleph_oi], axis=1)\n",
    "\n",
    "df_update_join = df_oi.join(df_aleph_oi)   \n",
    "\n",
    "# Noch kleine Rechnung zur Ueberprüfung, ob berechnete Dubletten und mit Skript ermittelte Dublettenzahl übereinstimmen\n",
    "g = df_alephIDs.shape[0]\n",
    "h= df_aleph_einzel.shape[0]\n",
    "j = df_aleph_doppelt.shape[0]\n",
    "\n",
    "print(\"Dubletten berechnet: \",  g-h)\n",
    "print(\"Dubletten gezählt:   \", j)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wenn alles gut ist, kann das Skript ab hier wieder mit \"all Cells below\" durchlaufen\n",
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG-Datei für den Prozess, zur Dokumentation des Imports und als Kontrollanzeige hier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der enthaltenen Datensätze: 66240\n",
      "-------------------------\n",
      "Aleph-IDs anfangs 29975\n",
      "Aleph-IDs nach join 29772\n"
     ]
    }
   ],
   "source": [
    "x = df.shape[0]\n",
    "print('Anzahl der enthaltenen Datensätze:', x)\n",
    "#print('vorhandene ISBNs:', df_update[\"isbn_ean\"].shape[0])\n",
    "\n",
    "z = df_alephIDs[\"ids\"].count()\n",
    "y = df_update_join[\"ids\"].count()\n",
    "\n",
    "print('-------------------------')\n",
    "print('Aleph-IDs anfangs', z)\n",
    "print('Aleph-IDs nach join' , y)\n",
    "\n",
    "timestr = time.strftime('%d.%m.%Y - %H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('Logdatei PDA-Import vom ')\n",
    "    log.write(timestr)\n",
    "    log.write('\\n------------------------------------------\\n')\n",
    "    log.write('Gelieferte Datensätze:             ' + str(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1. Identifizierung von Titeln in Aleph-Dubletten, die nicht mehr im Datensatz sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>edition_number</th>\n",
       "      <th>edition_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>...</th>\n",
       "      <th>cover</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>follower</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>watchlist_name</th>\n",
       "      <th>ids</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "      <th>LEN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217164</th>\n",
       "      <td>9.78341e+12</td>\n",
       "      <td>Unternehmensbewertung für Juristen</td>\n",
       "      <td></td>\n",
       "      <td>Dirk Hachmeister;Matthias Schüppen</td>\n",
       "      <td>C.H.BECK</td>\n",
       "      <td>Neue Juristische Wochenschrift (NJW) - Praxis ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-02-03 05:50:21</td>\n",
       "      <td>Himbeere Wirtschaft #181.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218910</th>\n",
       "      <td>9.78341e+12</td>\n",
       "      <td>Grundkurs Steuerrecht</td>\n",
       "      <td></td>\n",
       "      <td>Rainer Wernsmann;Christian Thiemann</td>\n",
       "      <td>C.H.BECK</td>\n",
       "      <td>Grundkurse</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-02-05 06:58:17</td>\n",
       "      <td>brombeere Politik #69.1</td>\n",
       "      <td>000129231</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a218910</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237277</th>\n",
       "      <td>9.78383e+12</td>\n",
       "      <td>Käuferverhalten</td>\n",
       "      <td>Eine marketingorientierte Einführung</td>\n",
       "      <td>Alfred Kuß;Torsten Tomczak;Silke Lennerts</td>\n",
       "      <td>UTB</td>\n",
       "      <td>Grundwissen der Ökonomik Band 1604</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>5., komplett überarbeitete Auflage 2017</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td>9783825216047</td>\n",
       "      <td></td>\n",
       "      <td>2025-03-05 20:40:44</td>\n",
       "      <td>Himbeere Wirtschaft #26</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240369</th>\n",
       "      <td>9.78383e+12</td>\n",
       "      <td>WTO-Recht in Fällen</td>\n",
       "      <td></td>\n",
       "      <td>Jan Neumann;Christian Pitschas</td>\n",
       "      <td>Nomos</td>\n",
       "      <td>NomosStudium</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td>9783832904241</td>\n",
       "      <td></td>\n",
       "      <td>2025-03-05 20:41:38</td>\n",
       "      <td>Himbeere Wirtschaft #112.2</td>\n",
       "      <td>000129233</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a240369</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261723</th>\n",
       "      <td>9.78349e+12</td>\n",
       "      <td>Gesundheitsökonomie</td>\n",
       "      <td>Eine Einführung</td>\n",
       "      <td>Kornelia van der Beek;Gregor van der Beek;Wilf...</td>\n",
       "      <td>De Gruyter Oldenbourg</td>\n",
       "      <td>de Gruyter Studium</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2., überarbeitete und erweiterte Auflage</td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td>9783486586862</td>\n",
       "      <td></td>\n",
       "      <td>2025-02-03 06:13:52</td>\n",
       "      <td>Himbeere Wirtschaft #119.1</td>\n",
       "      <td>000129303</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a261723</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173032813</th>\n",
       "      <td>9.78384e+12</td>\n",
       "      <td>Visuelle Transformationen des Exils</td>\n",
       "      <td>Die jüdischen Künstlerinnen Grete Stern, Hedy ...</td>\n",
       "      <td>Christina Wieder</td>\n",
       "      <td>transcript</td>\n",
       "      <td>Historische Geschlechterforschung 21</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-11 19:58:50</td>\n",
       "      <td>brombeere Politik #200.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173032814</th>\n",
       "      <td>9.78384e+12</td>\n",
       "      <td>Talking Politics and Society Again</td>\n",
       "      <td>Reengaging with Fellow Citizens</td>\n",
       "      <td>Hans Blokland</td>\n",
       "      <td>transcript</td>\n",
       "      <td>Kultur und soziale Praxis</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-11 19:58:50</td>\n",
       "      <td>brombeere Politik #200.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173052865</th>\n",
       "      <td>9.78234e+12</td>\n",
       "      <td>Europe, as-tu une âme ?</td>\n",
       "      <td>Essai pour une pédagogie de l'Europe</td>\n",
       "      <td>Vincent Carbonel</td>\n",
       "      <td>Editions L'Harmattan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-05 18:40:16</td>\n",
       "      <td>brombeere Politik #200.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173056680</th>\n",
       "      <td>9.78369e+12</td>\n",
       "      <td>Geldwäsche und Terrorismusfinanzierung</td>\n",
       "      <td>Validierung von Frühwarnindikatoren für Kredit...</td>\n",
       "      <td>Christoph Matthias Lüling</td>\n",
       "      <td>Tectum Wissenschaftsverlag</td>\n",
       "      <td>Young Academics: Betriebswirtschaftslehre 12</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-12 06:11:14</td>\n",
       "      <td>Himbeere Wirtschaft #199.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173059497</th>\n",
       "      <td>9.78234e+12</td>\n",
       "      <td>Les enjeux culturels du conflit russo-ukrainien</td>\n",
       "      <td>La redéfinition de l'identité ukrainienne au X...</td>\n",
       "      <td>Julien Plouchart</td>\n",
       "      <td>Editions L'Harmattan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2025-03-06 18:28:43</td>\n",
       "      <td>brombeere Politik #200.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66443 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              isbn_ean                                            title  \\\n",
       "object_id                                                                 \n",
       "217164     9.78341e+12               Unternehmensbewertung für Juristen   \n",
       "218910     9.78341e+12                            Grundkurs Steuerrecht   \n",
       "237277     9.78383e+12                                  Käuferverhalten   \n",
       "240369     9.78383e+12                              WTO-Recht in Fällen   \n",
       "261723     9.78349e+12                              Gesundheitsökonomie   \n",
       "...                ...                                              ...   \n",
       "173032813  9.78384e+12              Visuelle Transformationen des Exils   \n",
       "173032814  9.78384e+12               Talking Politics and Society Again   \n",
       "173052865  9.78234e+12                          Europe, as-tu une âme ?   \n",
       "173056680  9.78369e+12           Geldwäsche und Terrorismusfinanzierung   \n",
       "173059497  9.78234e+12  Les enjeux culturels du conflit russo-ukrainien   \n",
       "\n",
       "                                                    subtitle  \\\n",
       "object_id                                                      \n",
       "217164                                                         \n",
       "218910                                                         \n",
       "237277                  Eine marketingorientierte Einführung   \n",
       "240369                                                         \n",
       "261723                                       Eine Einführung   \n",
       "...                                                      ...   \n",
       "173032813  Die jüdischen Künstlerinnen Grete Stern, Hedy ...   \n",
       "173032814                    Reengaging with Fellow Citizens   \n",
       "173052865               Essai pour une pédagogie de l'Europe   \n",
       "173056680  Validierung von Frühwarnindikatoren für Kredit...   \n",
       "173059497  La redéfinition de l'identité ukrainienne au X...   \n",
       "\n",
       "                                                 contributor  \\\n",
       "object_id                                                      \n",
       "217164                    Dirk Hachmeister;Matthias Schüppen   \n",
       "218910                   Rainer Wernsmann;Christian Thiemann   \n",
       "237277             Alfred Kuß;Torsten Tomczak;Silke Lennerts   \n",
       "240369                        Jan Neumann;Christian Pitschas   \n",
       "261723     Kornelia van der Beek;Gregor van der Beek;Wilf...   \n",
       "...                                                      ...   \n",
       "173032813                                   Christina Wieder   \n",
       "173032814                                      Hans Blokland   \n",
       "173052865                                   Vincent Carbonel   \n",
       "173056680                          Christoph Matthias Lüling   \n",
       "173059497                                   Julien Plouchart   \n",
       "\n",
       "                            publisher  \\\n",
       "object_id                               \n",
       "217164                       C.H.BECK   \n",
       "218910                       C.H.BECK   \n",
       "237277                            UTB   \n",
       "240369                          Nomos   \n",
       "261723          De Gruyter Oldenbourg   \n",
       "...                               ...   \n",
       "173032813                  transcript   \n",
       "173032814                  transcript   \n",
       "173052865        Editions L'Harmattan   \n",
       "173056680  Tectum Wissenschaftsverlag   \n",
       "173059497        Editions L'Harmattan   \n",
       "\n",
       "                                                      series thesis  \\\n",
       "object_id                                                             \n",
       "217164     Neue Juristische Wochenschrift (NJW) - Praxis ...          \n",
       "218910                                            Grundkurse          \n",
       "237277                    Grundwissen der Ökonomik Band 1604          \n",
       "240369                                          NomosStudium          \n",
       "261723                                    de Gruyter Studium          \n",
       "...                                                      ...    ...   \n",
       "173032813               Historische Geschlechterforschung 21          \n",
       "173032814                          Kultur und soziale Praxis          \n",
       "173052865                                                             \n",
       "173056680       Young Academics: Betriebswirtschaftslehre 12          \n",
       "173059497                                                             \n",
       "\n",
       "          edition_number                              edition_text media_type  \\\n",
       "object_id                                                                       \n",
       "217164                 1                                            hardcover   \n",
       "218910                 1                                            hardcover   \n",
       "237277                 5   5., komplett überarbeitete Auflage 2017  hardcover   \n",
       "240369                 2                                            hardcover   \n",
       "261723                    2., überarbeitete und erweiterte Auflage  hardcover   \n",
       "...                  ...                                       ...        ...   \n",
       "173032813              1                                            hardcover   \n",
       "173032814              1                                            hardcover   \n",
       "173052865                                                           hardcover   \n",
       "173056680              1                                            hardcover   \n",
       "173059497                                                           hardcover   \n",
       "\n",
       "           ...                                              cover  \\\n",
       "object_id  ...                                                      \n",
       "217164     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "218910     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "237277     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "240369     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "261723     ...  https://content.schweitzer-online.de/static/ca...   \n",
       "...        ...                                                ...   \n",
       "173032813  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173032814  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173052865  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173056680  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "173059497  ...  https://content.schweitzer-online.de/static/ca...   \n",
       "\n",
       "             predecessor follower        last_modified  \\\n",
       "object_id                                                \n",
       "217164                             2025-02-03 05:50:21   \n",
       "218910                             2025-02-05 06:58:17   \n",
       "237277     9783825216047           2025-03-05 20:40:44   \n",
       "240369     9783832904241           2025-03-05 20:41:38   \n",
       "261723     9783486586862           2025-02-03 06:13:52   \n",
       "...                  ...      ...                  ...   \n",
       "173032813                          2025-03-11 19:58:50   \n",
       "173032814                          2025-03-11 19:58:50   \n",
       "173052865                          2025-03-05 18:40:16   \n",
       "173056680                          2025-03-12 06:11:14   \n",
       "173059497                          2025-03-06 18:28:43   \n",
       "\n",
       "                       watchlist_name        ids Field     L    Content   LEN  \n",
       "object_id                                                                      \n",
       "217164     Himbeere Wirtschaft #181.2       None  None  None       None  None  \n",
       "218910        brombeere Politik #69.1  000129231   020     L  $$a218910     9  \n",
       "237277        Himbeere Wirtschaft #26       None  None  None       None  None  \n",
       "240369     Himbeere Wirtschaft #112.2  000129233   020     L  $$a240369     9  \n",
       "261723     Himbeere Wirtschaft #119.1  000129303   020     L  $$a261723     9  \n",
       "...                               ...        ...   ...   ...        ...   ...  \n",
       "173032813    brombeere Politik #200.2       None  None  None       None  None  \n",
       "173032814    brombeere Politik #200.2       None  None  None       None  None  \n",
       "173052865    brombeere Politik #200.1       None  None  None       None  None  \n",
       "173056680  Himbeere Wirtschaft #199.1       None  None  None       None  None  \n",
       "173059497    brombeere Politik #200.1       None  None  None       None  None  \n",
       "\n",
       "[66443 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_update_aleph.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_loeschen = df_update_aleph[df_update_aleph['isbn_ean'].isna()]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>edition_number</th>\n",
       "      <th>edition_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>...</th>\n",
       "      <th>cover</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>follower</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>watchlist_name</th>\n",
       "      <th>ids</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "      <th>LEN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2254200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129227</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2254200</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368410</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129249</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2368410</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430847</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129244</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2430847</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481913</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129235</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2481913</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576208</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000129267</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2576208</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171787741</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000155532</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171787741</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171796455</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152210</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171796455</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171868889</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152355</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171868889</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171894384</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152319</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171894384</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171894386</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>000152318</td>\n",
       "      <td>020</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a171894386</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           isbn_ean title subtitle contributor publisher series thesis  \\\n",
       "object_id                                                                \n",
       "2254200         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2368410         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2430847         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2481913         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "2576208         NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "...             ...   ...      ...         ...       ...    ...    ...   \n",
       "171787741       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171796455       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171868889       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171894384       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "171894386       NaN   NaN      NaN         NaN       NaN    NaN    NaN   \n",
       "\n",
       "          edition_number edition_text media_type  ... cover predecessor  \\\n",
       "object_id                                         ...                     \n",
       "2254200              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2368410              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2430847              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2481913              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "2576208              NaN          NaN        NaN  ...   NaN         NaN   \n",
       "...                  ...          ...        ...  ...   ...         ...   \n",
       "171787741            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171796455            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171868889            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171894384            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "171894386            NaN          NaN        NaN  ...   NaN         NaN   \n",
       "\n",
       "          follower last_modified watchlist_name        ids Field  L  \\\n",
       "object_id                                                             \n",
       "2254200        NaN           NaN            NaN  000129227   020  L   \n",
       "2368410        NaN           NaN            NaN  000129249   020  L   \n",
       "2430847        NaN           NaN            NaN  000129244   020  L   \n",
       "2481913        NaN           NaN            NaN  000129235   020  L   \n",
       "2576208        NaN           NaN            NaN  000129267   020  L   \n",
       "...            ...           ...            ...        ...   ... ..   \n",
       "171787741      NaN           NaN            NaN  000155532   020  L   \n",
       "171796455      NaN           NaN            NaN  000152210   020  L   \n",
       "171868889      NaN           NaN            NaN  000152355   020  L   \n",
       "171894384      NaN           NaN            NaN  000152319   020  L   \n",
       "171894386      NaN           NaN            NaN  000152318   020  L   \n",
       "\n",
       "                Content   LEN  \n",
       "object_id                      \n",
       "2254200      $$a2254200  10.0  \n",
       "2368410      $$a2368410  10.0  \n",
       "2430847      $$a2430847  10.0  \n",
       "2481913      $$a2481913  10.0  \n",
       "2576208      $$a2576208  10.0  \n",
       "...                 ...   ...  \n",
       "171787741  $$a171787741  12.0  \n",
       "171796455  $$a171796455  12.0  \n",
       "171868889  $$a171868889  12.0  \n",
       "171894384  $$a171894384  12.0  \n",
       "171894386  $$a171894386  12.0  \n",
       "\n",
       "[203 rows x 27 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_loeschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Loeschen-datei!\n",
    "\n",
    "with open(\"./output/ges02_weg\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_loeschen.index:\n",
    "        fa.write(df_aleph_loeschen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Dublettencheck innerhalb gelieferten Buchhandelsdaten \n",
    "\n",
    "Aufgaben im Rahmen des Dublettencheck:\n",
    "1. Dublettenkontrolle anhand von Titel, Untertitel und Autor \n",
    "   - Zunächst Behebung der unsauberen Titel / Untertitel-Trennung für korrekteren Abgleich\n",
    "   - Trennung der Datensätze in Dubletten und \"Einzeltitel\"\n",
    "     - Einzeltitel werden direkt für Bestandsprüfung vorgemerkt\n",
    "     - Dubletten werden auf neueste Version reduziert und diese der Bestandsprüfungsdatei hinzugefügt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Dublettenkontrolle Anhand von Titel, Untertitel und Autor\n",
    "\n",
    "*Entfernen von Untertiteln aus der Titelspalte, Extrahieren von Untertiteln und Abgleich mit Untertitelspalte und Schreiben der vorhandenen Informationen in neue Untertitel-Spalte.   \n",
    "Durch diese Spalte werden ca. 1/3 mehr Dubletten erkannt, als ohne die Bereinigung. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_update_join[df_update_join['isbn_ean'].notna()]  #jetzt alle die zu verarbeiten sind rausziehen\n",
    "df.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu = df[\"title\"].str.split(':', n = 1, expand = True)  #Titel am 1. Doppelpunkt splitten und getrennt in neue Felder schreiben\n",
    "df[\"title_sep\"]= neu[0]\n",
    "df[\"subtitle_sep\"]= neu[1]\n",
    "\n",
    "df[\"subtitle_sep\"] = df[\"subtitle_sep\"].replace(np.nan, '', regex=True) #NaN-Werte stören, darum raus damit ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = np.where(df[\"subtitle\"] == df[\"subtitle_sep\"], '', df[\"subtitle\"])    # Abgleich - wenn in beiden das Gleiche steht, dann ursprüngliches \"Subtitle\"-Feld nehmen\n",
    "df[\"subtitle_comparison\"] = comparison  \n",
    "\n",
    "comparison2 = np.where(df[\"subtitle\"] < df[\"subtitle_sep\"], df[\"subtitle_sep\"], '') # Wenn nur in \"subtitle_sep\" Infos stehen, diese übernehmen, das ist noch nicht ganz sauber, da hier manchmal anderes steht als in \"subtitle\"\n",
    "df[\"subtitle_comparison2\"] = comparison2 \n",
    "\n",
    "df[\"subtitle_all\"] = df[\"subtitle_comparison\"]+df[\"subtitle_comparison2\"]          # Beide Informationen in neuer Subtitle-Spalte zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short_title\"] = df[\"title_sep\"] + ' ' + df[\"subtitle_all\"] + ' / ' + df[\"contributor\"]  # aus den bereinigten Daten einen Kurztitel erzeugen, der dann für den Dublettencheck verwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dubletten = df.groupby(\"short_title\").filter(lambda g: (g.nunique() >1).any()) # schreibt alle mehrfach vorhandenen Titel in ein eigenes Datenframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dubl_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep='first')   # sortiert Dubletten nach Jahr und schreibt den jeweils ersten (= neuesten) Eintrag in neues Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohne_dubletten = df.drop_duplicates(\"short_title\", keep=False)       #durch \"\"keep=False\" werden alle nicht-Dubletten rausgezogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dubl_nicht_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep=False)\n",
    "\n",
    "#Kontrolle, ob es in Dubletten Titel gibt, die schon in Aleph sind\n",
    "df_dubl_nicht_einspielen[\"ids\"].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_einspielen = df_ohne_dubletten.append(df_dubl_einspielen)                    # die ausgewählten Dubletten und alle Nicht-Dubletten werden in ein Datenframe zusammengeführt\n",
    "df_einspielen.reset_index(inplace=True)                                         # für weitere Bearbeitung index-reset nötig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Überblick zu den Daten und Trennen in \"in Aleph vorhanden\" und \"neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zum Einspielen: 32054\n",
      "Davon in Aleph: 26725\n",
      "Neue Titel: 5329\n"
     ]
    }
   ],
   "source": [
    "m = df_einspielen.shape[0]\n",
    "n = df_einspielen[\"ids\"].count()\n",
    "print(\"Zum Einspielen:\", m)\n",
    "print(\"Davon in Aleph:\", n)\n",
    "print(\"Neue Titel:\", m-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26725"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph = df_einspielen.dropna(subset=['ids'])\n",
    "df_in_aleph.reset_index(inplace=True)\n",
    "df_in_aleph.shape[0]\n",
    "\n",
    "# zur Prüfung am GES und Ebx Bestand geprüft werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5329"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ganz_neu = df_einspielen[df_einspielen['ids'].isnull()]\n",
    "\n",
    "df_ganz_neu.reset_index(inplace=True)\n",
    "df_ganz_neu.shape[0]\n",
    "\n",
    "# diese müssen ebenfalls am Ges und Ebx Bestand geprüft werden, durch Trennung der Sets schnellere Bearbeitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bestandsabfragen\n",
    "\n",
    "\n",
    "\n",
    "*URLs für die Abfrage über den X-Server unseres Bibliothekssystems werden erzeugt und über die ISBN eine Abfrage auf Bestand gemacht. Die Abfrage funktioniert nur für zugelassene IPs (darum oben die Prüfung).  \n",
    "Für die Abfrage in unseren Bestand ist die ISBN sehr gut, da in den Titeldaten alle im Buch befindlichen ISBNs - auch die anderer Ausgabeformen - mit übernommen sind. Beim MPG-Ebooks Katalog handelt sich um Daten von Verlagen, die sich in ihrer Qualität und Informationsumfang sehr unterscheiden. Hier wird noch zu prüfen sein, inwieweit ein anderer Abfragemechansimus gewählt werden muss.*  \n",
    "\n",
    "### 1. Ganz neue Titel  \n",
    "\n",
    "= df_ganz_neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ges\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link','http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "#Es funktionierte nicht, dass die URLs an die vorhandenen ISBNs einfach so angefügt werden, darum der Workaround mit einem Platzhalter, der sich dann über replace vom richtigen Link überschreiben ließ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs für Ebooks-Katalog erzeugen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ebx\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n",
    "\n",
    "##### Zunächst für die Daten des MPIfG \n",
    "\n",
    "*Vorgehensweise: Abfrage und Sammeln der Antworten in einer Datei, diese Antworten werden dann in Ausdrücke \"übersetzt\" - \"vorhanden\" und \"neu\" und diese Daten in eine Spalte ins Dataframe zur weiteren Auswertung übertragen.   \n",
    "Schwierigkeit hier war, die Sammlung der Antworten zu den einzelnen Titeln, um sie in das Datenframe einzuspielen. Der störende XML-Header der Antworten wird erst gar nicht in die Datei geschrieben.   *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses', 'w') as fn:  \n",
    "    for url in df_ganz_neu[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses', 'r') as f:\n",
    "    with open('./input/server_responses_transfered', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf = pd.read_fwf('./input/server_responses_transfered', names=[\"Abfrage_ges\"])\n",
    "df_result = df_ganz_neu.join(df_fwf)                    #df_result = pd.concat([df_ganz_neu, df_fwf], axis=1), funktioniert nicht mehr                  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Randnotiz: \n",
    "    Bei 2400 Titels brauchte der Abgleich ca 350 Sekunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 5329 \n",
      "Anzahl der Antworten vom Server:    5329\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_ganz_neu.shape[0]\n",
    "y = df_fwf.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datenabgleich mit dem Bestand des MPG Ebooks-Katalog\n",
    "\n",
    "*Vorgehensweise analog Bestandsabfrage MPI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx', 'w') as fn:  \n",
    "    for url in df_result[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx = pd.read_fwf('./input/server_responses_transfered_ebx', names=[\"Abfrage_ebx\"])\n",
    "df_result_neu = df_result.join(df_fwf_ebx)           #df_result_neu = pd.concat([df_result, df_fwf_ebx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 5329 \n",
      "Anzahl der Antworten vom Server:    5329\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_neu.shape[0]\n",
    "y = df_fwf_ebx.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Titel in Aleph vorhanden\n",
    "\n",
    "= df_in_aleph\n",
    "\n",
    "*Hier reicht Abgleich mit Ebooks, das erworbene Bücher manuell gelöscht werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph[\"url_ges\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link', 'http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "df_in_aleph[\"url_ebx\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses2', 'w') as fn:  \n",
    "    for url in df_in_aleph[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ges = pd.read_fwf('./input/server_responses_transfered2', names=[\"Abfrage_ges\"])\n",
    "df_result2 = df_in_aleph.join(df_fwf_ges)               #df_result2 = pd.concat([df_in_aleph, df_fwf_ges], axis=1)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 26725 \n",
      "Anzahl der Antworten vom Server:    26725\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_in_aleph.shape[0]\n",
    "y = df_fwf_ges.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        neu\n",
       "1        neu\n",
       "2        neu\n",
       "3        neu\n",
       "4        neu\n",
       "        ... \n",
       "26720    neu\n",
       "26721    neu\n",
       "26722    neu\n",
       "26723    neu\n",
       "26724    neu\n",
       "Name: Abfrage_ges, Length: 26725, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result2['Abfrage_ges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx2', 'w') as fn:  \n",
    "    for url in df_result2[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx2 = pd.read_fwf('./input/server_responses_transfered_ebx2', names=[\"Abfrage_ebx\"])\n",
    "df_result_in_aleph = df_result2.join(df_fwf_ebx2)           #df_result_in_aleph = pd.concat([df_result2, df_fwf_ebx2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 26725 \n",
      "Anzahl der Antworten vom Server:    26725\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_in_aleph.shape[0]\n",
    "y = df_fwf_ebx2.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Exportvorbereitungen \n",
    "\n",
    "\n",
    "### 1. Neue Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_nicht_einspielen =  df_result_neu.drop(df_result_neu[(df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu')].index)          \n",
    "# alle Titel rausholen, die in einer der beiden Datenbanken vorhanden waren, diese werden mit den vorhandenen aus Update unten in Excel geschrieben\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Einspielen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_aleph_einspielen = df_result_neu.loc[((df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu'))]   #das sind die komplett neuen Titel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In Aleph vorhandene Titel\n",
    "\n",
    "*Hier ist der Fall: was im Ebooks-Katalog vorhanden ist, muss gelöscht werden*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>object_id</th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>...</th>\n",
       "      <th>title_sep</th>\n",
       "      <th>subtitle_sep</th>\n",
       "      <th>subtitle_comparison</th>\n",
       "      <th>subtitle_comparison2</th>\n",
       "      <th>subtitle_all</th>\n",
       "      <th>short_title</th>\n",
       "      <th>url_ges</th>\n",
       "      <th>url_ebx</th>\n",
       "      <th>Abfrage_ges</th>\n",
       "      <th>Abfrage_ebx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>685</td>\n",
       "      <td>993</td>\n",
       "      <td>11878921</td>\n",
       "      <td>9780415390798</td>\n",
       "      <td>Karl Polanyi and the Paradoxes of the Double M...</td>\n",
       "      <td></td>\n",
       "      <td>John Vail</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Studies in Social and Political Thought</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Karl Polanyi and the Paradoxes of the Double M...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Karl Polanyi and the Paradoxes of the Double M...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>743</td>\n",
       "      <td>1057</td>\n",
       "      <td>130769734</td>\n",
       "      <td>9780415858762</td>\n",
       "      <td>War as Protection and Punishment</td>\n",
       "      <td>Armed International Intervention at the 'End o...</td>\n",
       "      <td>Teresa Degenhardt</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Frontiers of Criminal Justice</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>War as Protection and Punishment</td>\n",
       "      <td></td>\n",
       "      <td>Armed International Intervention at the 'End o...</td>\n",
       "      <td></td>\n",
       "      <td>Armed International Intervention at the 'End o...</td>\n",
       "      <td>War as Protection and Punishment Armed Interna...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>832</td>\n",
       "      <td>1163</td>\n",
       "      <td>133493308</td>\n",
       "      <td>9781138784925</td>\n",
       "      <td>The Routledge Companion to Environmental Ethics</td>\n",
       "      <td></td>\n",
       "      <td>Benjamin Hale</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Philosophy Companions</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>The Routledge Companion to Environmental Ethics</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>The Routledge Companion to Environmental Ethic...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>988</td>\n",
       "      <td>1357</td>\n",
       "      <td>137861669</td>\n",
       "      <td>9781138293823</td>\n",
       "      <td>Law, Space, and the Vehicular Environment</td>\n",
       "      <td>Pavement and Asphalt</td>\n",
       "      <td>Sarah Marusek</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Space, Materiality and the Normative</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Law, Space, and the Vehicular Environment</td>\n",
       "      <td></td>\n",
       "      <td>Pavement and Asphalt</td>\n",
       "      <td></td>\n",
       "      <td>Pavement and Asphalt</td>\n",
       "      <td>Law, Space, and the Vehicular Environment Pave...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1217</td>\n",
       "      <td>1643</td>\n",
       "      <td>144553561</td>\n",
       "      <td>9781138491946</td>\n",
       "      <td>The Diaspora's Role in Africa</td>\n",
       "      <td>Transculturalism, Challenges, and Development</td>\n",
       "      <td>Stella-Monica N. Mpande</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Rethinking Development</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>The Diaspora's Role in Africa</td>\n",
       "      <td></td>\n",
       "      <td>Transculturalism, Challenges, and Development</td>\n",
       "      <td></td>\n",
       "      <td>Transculturalism, Challenges, and Development</td>\n",
       "      <td>The Diaspora's Role in Africa Transculturalism...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26512</th>\n",
       "      <td>31793</td>\n",
       "      <td>4782</td>\n",
       "      <td>157121198</td>\n",
       "      <td>9780367681524</td>\n",
       "      <td>Aid and Influence</td>\n",
       "      <td>Patronage, Power and Politics</td>\n",
       "      <td>Stephen Browne</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Aid and Influence</td>\n",
       "      <td></td>\n",
       "      <td>Patronage, Power and Politics</td>\n",
       "      <td></td>\n",
       "      <td>Patronage, Power and Politics</td>\n",
       "      <td>Aid and Influence Patronage, Power and Politic...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26544</th>\n",
       "      <td>31831</td>\n",
       "      <td>1559</td>\n",
       "      <td>143382178</td>\n",
       "      <td>9781138671324</td>\n",
       "      <td>African Americans and the Mississippi River</td>\n",
       "      <td>Race, History, and the Environment</td>\n",
       "      <td>Dorothy Zeisler-Vralsted</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Environmental Humanities</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>African Americans and the Mississippi River</td>\n",
       "      <td></td>\n",
       "      <td>Race, History, and the Environment</td>\n",
       "      <td></td>\n",
       "      <td>Race, History, and the Environment</td>\n",
       "      <td>African Americans and the Mississippi River Ra...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26562</th>\n",
       "      <td>31852</td>\n",
       "      <td>20020</td>\n",
       "      <td>163737954</td>\n",
       "      <td>9780367648596</td>\n",
       "      <td>Advancing Culturally Responsive Research and R...</td>\n",
       "      <td>Qualitative, Quantitative, and Mixed Methods</td>\n",
       "      <td>Penny A. Pasque</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Advancing Culturally Responsive Research and R...</td>\n",
       "      <td></td>\n",
       "      <td>Qualitative, Quantitative, and Mixed Methods</td>\n",
       "      <td></td>\n",
       "      <td>Qualitative, Quantitative, and Mixed Methods</td>\n",
       "      <td>Advancing Culturally Responsive Research and R...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26620</th>\n",
       "      <td>31918</td>\n",
       "      <td>29345</td>\n",
       "      <td>165866839</td>\n",
       "      <td>9780367419905</td>\n",
       "      <td>Abolish Criminology</td>\n",
       "      <td></td>\n",
       "      <td>Viviane Saleh-Hanna</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Studies in Penal Abolition and Trans...</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Abolish Criminology</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Abolish Criminology  / Viviane Saleh-Hanna</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26687</th>\n",
       "      <td>32009</td>\n",
       "      <td>16358</td>\n",
       "      <td>163044079</td>\n",
       "      <td>9780367861445</td>\n",
       "      <td>A Fresh Look at Fraud</td>\n",
       "      <td>Theoretical and Applied Perspectives</td>\n",
       "      <td>Yaniv Hanoch</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>A Fresh Look at Fraud</td>\n",
       "      <td></td>\n",
       "      <td>Theoretical and Applied Perspectives</td>\n",
       "      <td></td>\n",
       "      <td>Theoretical and Applied Perspectives</td>\n",
       "      <td>A Fresh Look at Fraud Theoretical and Applied ...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>vor</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index  object_id       isbn_ean  \\\n",
       "536        685    993   11878921  9780415390798   \n",
       "586        743   1057  130769734  9780415858762   \n",
       "665        832   1163  133493308  9781138784925   \n",
       "804        988   1357  137861669  9781138293823   \n",
       "1011      1217   1643  144553561  9781138491946   \n",
       "...        ...    ...        ...            ...   \n",
       "26512    31793   4782  157121198  9780367681524   \n",
       "26544    31831   1559  143382178  9781138671324   \n",
       "26562    31852  20020  163737954  9780367648596   \n",
       "26620    31918  29345  165866839  9780367419905   \n",
       "26687    32009  16358  163044079  9780367861445   \n",
       "\n",
       "                                                   title  \\\n",
       "536    Karl Polanyi and the Paradoxes of the Double M...   \n",
       "586                     War as Protection and Punishment   \n",
       "665      The Routledge Companion to Environmental Ethics   \n",
       "804            Law, Space, and the Vehicular Environment   \n",
       "1011                       The Diaspora's Role in Africa   \n",
       "...                                                  ...   \n",
       "26512                                  Aid and Influence   \n",
       "26544        African Americans and the Mississippi River   \n",
       "26562  Advancing Culturally Responsive Research and R...   \n",
       "26620                                Abolish Criminology   \n",
       "26687                              A Fresh Look at Fraud   \n",
       "\n",
       "                                                subtitle  \\\n",
       "536                                                        \n",
       "586    Armed International Intervention at the 'End o...   \n",
       "665                                                        \n",
       "804                                 Pavement and Asphalt   \n",
       "1011       Transculturalism, Challenges, and Development   \n",
       "...                                                  ...   \n",
       "26512                      Patronage, Power and Politics   \n",
       "26544                 Race, History, and the Environment   \n",
       "26562       Qualitative, Quantitative, and Mixed Methods   \n",
       "26620                                                      \n",
       "26687               Theoretical and Applied Perspectives   \n",
       "\n",
       "                    contributor  publisher  \\\n",
       "536                   John Vail  Routledge   \n",
       "586           Teresa Degenhardt  Routledge   \n",
       "665               Benjamin Hale  Routledge   \n",
       "804               Sarah Marusek  Routledge   \n",
       "1011    Stella-Monica N. Mpande  Routledge   \n",
       "...                         ...        ...   \n",
       "26512            Stephen Browne  Routledge   \n",
       "26544  Dorothy Zeisler-Vralsted  Routledge   \n",
       "26562           Penny A. Pasque  Routledge   \n",
       "26620       Viviane Saleh-Hanna  Routledge   \n",
       "26687              Yaniv Hanoch  Routledge   \n",
       "\n",
       "                                                  series thesis  ...  \\\n",
       "536    Routledge Studies in Social and Political Thought         ...   \n",
       "586              Routledge Frontiers of Criminal Justice         ...   \n",
       "665                      Routledge Philosophy Companions         ...   \n",
       "804                 Space, Materiality and the Normative         ...   \n",
       "1011                              Rethinking Development         ...   \n",
       "...                                                  ...    ...  ...   \n",
       "26512                                                            ...   \n",
       "26544                 Routledge Environmental Humanities         ...   \n",
       "26562                                                            ...   \n",
       "26620  Routledge Studies in Penal Abolition and Trans...         ...   \n",
       "26687                                                            ...   \n",
       "\n",
       "                                               title_sep subtitle_sep  \\\n",
       "536    Karl Polanyi and the Paradoxes of the Double M...                \n",
       "586                     War as Protection and Punishment                \n",
       "665      The Routledge Companion to Environmental Ethics                \n",
       "804            Law, Space, and the Vehicular Environment                \n",
       "1011                       The Diaspora's Role in Africa                \n",
       "...                                                  ...          ...   \n",
       "26512                                  Aid and Influence                \n",
       "26544        African Americans and the Mississippi River                \n",
       "26562  Advancing Culturally Responsive Research and R...                \n",
       "26620                                Abolish Criminology                \n",
       "26687                              A Fresh Look at Fraud                \n",
       "\n",
       "                                     subtitle_comparison subtitle_comparison2  \\\n",
       "536                                                                             \n",
       "586    Armed International Intervention at the 'End o...                        \n",
       "665                                                                             \n",
       "804                                 Pavement and Asphalt                        \n",
       "1011       Transculturalism, Challenges, and Development                        \n",
       "...                                                  ...                  ...   \n",
       "26512                      Patronage, Power and Politics                        \n",
       "26544                 Race, History, and the Environment                        \n",
       "26562       Qualitative, Quantitative, and Mixed Methods                        \n",
       "26620                                                                           \n",
       "26687               Theoretical and Applied Perspectives                        \n",
       "\n",
       "                                            subtitle_all  \\\n",
       "536                                                        \n",
       "586    Armed International Intervention at the 'End o...   \n",
       "665                                                        \n",
       "804                                 Pavement and Asphalt   \n",
       "1011       Transculturalism, Challenges, and Development   \n",
       "...                                                  ...   \n",
       "26512                      Patronage, Power and Politics   \n",
       "26544                 Race, History, and the Environment   \n",
       "26562       Qualitative, Quantitative, and Mixed Methods   \n",
       "26620                                                      \n",
       "26687               Theoretical and Applied Perspectives   \n",
       "\n",
       "                                             short_title  \\\n",
       "536    Karl Polanyi and the Paradoxes of the Double M...   \n",
       "586    War as Protection and Punishment Armed Interna...   \n",
       "665    The Routledge Companion to Environmental Ethic...   \n",
       "804    Law, Space, and the Vehicular Environment Pave...   \n",
       "1011   The Diaspora's Role in Africa Transculturalism...   \n",
       "...                                                  ...   \n",
       "26512  Aid and Influence Patronage, Power and Politic...   \n",
       "26544  African Americans and the Mississippi River Ra...   \n",
       "26562  Advancing Culturally Responsive Research and R...   \n",
       "26620         Abolish Criminology  / Viviane Saleh-Hanna   \n",
       "26687  A Fresh Look at Fraud Theoretical and Applied ...   \n",
       "\n",
       "                                                 url_ges  \\\n",
       "536    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "586    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "665    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "804    http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "1011   http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "...                                                  ...   \n",
       "26512  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26544  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26562  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26620  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26687  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "\n",
       "                                                 url_ebx  Abfrage_ges  \\\n",
       "536    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "586    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "665    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "804    http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "1011   http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "...                                                  ...          ...   \n",
       "26512  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26544  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26562  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26620  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "26687  http://aleph.mpg.de/X?op=find&base=ebx01&reque...          vor   \n",
       "\n",
       "      Abfrage_ebx  \n",
       "536           neu  \n",
       "586           neu  \n",
       "665           neu  \n",
       "804           neu  \n",
       "1011          neu  \n",
       "...           ...  \n",
       "26512         neu  \n",
       "26544         neu  \n",
       "26562         neu  \n",
       "26620         neu  \n",
       "26687         neu  \n",
       "\n",
       "[540 rows x 40 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_nicht_einspielen = df_result_in_aleph.drop(df_result_in_aleph[(df_result_in_aleph[\"Abfrage_ebx\"] == 'neu') & (df_result_in_aleph[\"Abfrage_ges\"] == 'neu')].index) \n",
    "df_in_aleph_nicht_einspielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen der Datei zum Löschen der Titel\n",
    "\n",
    "with open(\"./output/ges02_weg\", \"a\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_nicht_einspielen.index:\n",
    "        fa.write(df_in_aleph_nicht_einspielen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Statistik und Kontrolle zusammenführen aller Titel, die nicht eingespielt werden und Ausgabe in einer CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gesamt_nicht_einspielen = df_in_aleph_nicht_einspielen.append(df_neu_nicht_einspielen)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y_%m_%d\")                                              # Zeit erfassen für Dateibenennung\n",
    "\n",
    "df_gesamt_nicht_einspielen[\"object_id\"] = df_gesamt_nicht_einspielen.object_id.astype(str)  # wandelt die spalte von Int64 zu Object um, so dass es in Excel korrekt eingelesen wird\n",
    "df_gesamt_nicht_einspielen[\"isbn_ean\"] = df_gesamt_nicht_einspielen.isbn_ean.astype(str)\n",
    "df_gesamt_nicht_einspielen = df_gesamt_nicht_einspielen.drop(columns=[\"url_ebx\", \"url_ges\", \"cover\", \"title_sep\", \"subtitle_comparison\", \"subtitle_comparison2\", \"subtitle_all\", \"subtitle_sep\"]) # unnötige Spalten entfernen\n",
    "\n",
    "df_neu_nicht_einspielen.to_csv('./output/Vorhandene_Titel_'+date+'.csv')   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Updaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update = df_result_in_aleph.loc[(df_result_in_aleph[\"Abfrage_ebx\"]== 'neu') & (df_result_in_aleph[\"Abfrage_ges\"] == 'neu')] # prüfen, ob es wirklich ein Datenupdate gab, sonst nicht neu einspielen??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Für die Logdatei Ermittlung verschiedener Zahlen und hier zur direkten Ansicht ausgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kleine Statistik:\n",
      "===================================== \n",
      "Gelieferte Datensätze:              38142 \n",
      "-------------------------------------------- \n",
      "Sätze ohne Dubletten                26080 \n",
      "   Dubletten:               12062 \n",
      "   Auswahl zum Einspielen   5974 \n",
      "   in Aleph löschen         0 \n",
      "Zu prüfende Datensätze:             32054 \n",
      "-------------------------------------------- \n",
      "Prüfung Titel in Aleph:             26725 \n",
      "   Davon als Update         26185 \n",
      "   Davon in Aleph löschen   540 \n",
      "Prüfung neue Titel    :             5329 \n",
      "   Davon ganz neu           3098 \n",
      "   Davon bereits Bestand    2231 \n",
      "-------------------------------------------- \n",
      "Titel, die in Aleph verarbeitet werden:  29283 \n",
      "-------------------------------------------- \n",
      "Datensätze GES02 vor Einspielen:    31360 \n",
      "Nicht mehr im Export               - 3 \n",
      "Titel in Aleph löschen, da Bestand - 540 \n",
      "Zu löschen, da Dublette            - 0 \n",
      "Neue Titel für Aleph               + 3098 \n",
      "-------------------------------------------- \n",
      "In Aleph nach Einspielen:           33915\n"
     ]
    }
   ],
   "source": [
    "#Kontrollmechanismus, ob für alle Titel auch Treffer da sind\n",
    "x = df.shape[0]\n",
    "a = df_ohne_dubletten.shape[0]\n",
    "c = df_dubletten.shape[0]\n",
    "b = df_dubl_einspielen.shape[0] #Auswahl der neuen Treffer\n",
    "m = df_in_aleph_nicht_einspielen.shape[0]\n",
    "n = df_ganz_neu.shape[0]\n",
    "h = df_neu_aleph_einspielen.shape[0]\n",
    "j = df_neu_nicht_einspielen.shape[0]\n",
    "o = df_in_aleph_update.shape[0]\n",
    "g = df_in_aleph.shape[0]\n",
    "k = df_alephIDs.shape[0]\n",
    "z = df_neu_aleph_einspielen.shape[0]       #neu ermitteln aus neuen\n",
    "l = df_aleph_loeschen.shape[0]\n",
    "y = df_dubl_nicht_einspielen.shape[0]\n",
    "\n",
    "\n",
    "print('Kleine Statistik:\\n=====================================',\n",
    "    '\\nGelieferte Datensätze:             ', x,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nSätze ohne Dubletten               ', a,\n",
    "    '\\n   Dubletten:              ', c,\n",
    "    '\\n   Auswahl zum Einspielen  ', b,\n",
    "    '\\n   in Aleph löschen        ', y,\n",
    "    '\\nZu prüfende Datensätze:            ', a+b,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nPrüfung Titel in Aleph:            ', g,\n",
    "    '\\n   Davon als Update        ', o,\n",
    "    '\\n   Davon in Aleph löschen  ', m ,    \n",
    "    '\\nPrüfung neue Titel    :            ', n,\n",
    "    '\\n   Davon ganz neu          ', h,\n",
    "    '\\n   Davon bereits Bestand   ', j,      \n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nTitel, die in Aleph verarbeitet werden: ', o+h,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nDatensätze GES02 vor Einspielen:   ', k,\n",
    "    '\\nNicht mehr im Export               -', l,\n",
    "    '\\nTitel in Aleph löschen, da Bestand -', m,\n",
    "    '\\nZu löschen, da Dublette            -', y,\n",
    "    '\\nNeue Titel für Aleph               +', h,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nIn Aleph nach Einspielen:          ', k-l-m-y+h,)\n",
    "\n",
    "#hier entsprechende Einträge für die Log-Datei\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:\n",
    "    log.write(\"\\nGelieferte Datensätze:             \" + str(x))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nSätze ohne Dubletten               \" + str(a))\n",
    "    log.write(\"\\n   Dubletten:              \" + str(c))\n",
    "    log.write(\"\\n   Davon in Aleph löeschen \" + str(y))\n",
    "    log.write(\"\\n   Auswahl zum Einspielen  \" + str(b))\n",
    "    log.write(\"\\nZu prüfende Datensätze:            \" + str(a+b))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nPrüfung Titel in Aleph:            \" + str(g))\n",
    "    log.write(\"\\n   Davon als Update        \" + str(o))\n",
    "    log.write(\"\\n   Davon in Aleph löschen  \" + str(m) + \"    \\n\")   \n",
    "    log.write(\"\\nPrüfung neue Titel    :            \" + str(n))\n",
    "    log.write(\"\\n   Davon ganz neu          \" + str(h))\n",
    "    log.write(\"\\n   Davon bereits Bestand   \" + str(j) + \"    \\n\")     \n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nTitel, die in Aleph verarbeitet werden: \" + str(o+h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nDatensätze GES02 vor Einspielen:   \" + str(k))\n",
    "    log.write(\"\\nNicht mehr im Export               -\" + str(l))\n",
    "    log.write(\"\\nZu löschender dubletter Titel      -\" + str(y))\n",
    "    log.write(\"\\nTitel in Aleph löschen, da Bestand -\" + str(m))\n",
    "    log.write(\"\\nNeue Titel für Aleph               +\" + str(h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nIn Aleph nach Einspielen:          \" + str(k-l-m-y+h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Exportdateien Aufbereiten\n",
    "\n",
    "#### Zielformat für das Einspielen in Aleph:\n",
    "\n",
    "    000000001 LDR   L -----nM2.01200024------h              \n",
    "    000000001 020   L $$a (object_id))\n",
    "    000000001 030   L $$aaz||rrrza||||\n",
    "    000000001 051   L $$am|||||||\n",
    "    000000001 070   L $$aSchweitzer\n",
    "    000000001 077   L $$aMonographie\n",
    "    000000001 078   L $$aSchweitzer\n",
    "    000000001 082   L $$azum Bestellen\n",
    "    000000001 100   L $$a (contributor_1)\n",
    "    000000001 104   L $$a (contributor_2)\n",
    "    000000001 108   L $$a (contributor_3)\n",
    "    000000001 331   L $$a (title_sep)\n",
    "    000000001 335   L $$a (subtitle_all)\n",
    "    000000001 403   L $$a (edition_number / edition_text)  #noch prüfen, was besser zu verwenden ist \n",
    "    000000001 419   L $$b (publisher) $$a (date_combined)\n",
    "    000000001 433   L $$a (pages)\n",
    "    000000001 451   L $$a (series)\n",
    "    000000001 520   L $$a (thesis)\n",
    "    000000001 540   L $$a (isbn_ean)\n",
    "    000000001 656   L $$a (cover)\n",
    "    000000001 750   L $$a (description)\n",
    "    000000001 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch        \n",
    "    \n",
    "Anmerkung zum Feld 655: die URL wird NACH dem Einspielen in Aleph mit der Datensatz-ID angereichert (siehe Juypter-Notebook \"Link-Anreicherung\"), um einen klaren Bestellink für den Kaufvorschlag zu haben\n",
    "\n",
    "*Hierfür werden immer die Feldbenennung und bestimmte Codierungen VOR den Inhalt - in Klammern de Bezeichnung der entsprechenden Spalte - gesetzt, bzw. erfoderliche Felder komplett neu hinzugefügt.   \n",
    "Am Anfang jeder Zeile braucht Aleph eine 9-Stellige eindeutige Zahl pro Titel.*   \n",
    "\n",
    "*Manchmal ließ sich der Inhalt einer Spalte direkt in die Datei schreiben, manchmal musst die Spalte zuvor über apply aufbereitet werden.* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aufbereiten der neuen Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen = df_neu_aleph_einspielen           #zur vereinfachten Wiederverwertung des alten Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"020\"] = df_aleph_einspielen[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_aleph_einspielen[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung der Personendaten\n",
    "\n",
    "*Da bis zu 3 Personen in einer Spalte zu finden sind, werden diese im Discovery nicht getrennt suchbar, darum werden sie gesplittet. Für die Dublettenkontrolle hat sich das als irrelevant erwiesen, darum erfolgt dieser Schritt erst hier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = df_aleph_einspielen[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= person[0]\n",
    "df_aleph_einspielen[\"contributor_2\"]= person[1]\n",
    "df_aleph_einspielen[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= df_aleph_einspielen[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_2\"]= df_aleph_einspielen[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_3\"]= df_aleph_einspielen[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung des Erscheinungsdatum und Erscheinungsjahres\n",
    "\n",
    "*In der Auswahl unserer Titel befinden sich auch im Erscheinen befindliche Titel der kommenden Monate. Diese Information möchten wir gerne im Discovery sichtbar machen. Hierfür bleibt uns nur Aleph-Feld 419c, das dem Erscheinungsjahr vorbehalten ist.   \n",
    "Wunsch ist es: Wenn des Erscheinungsdatum weiter als 10 Tage weg vom heutigen Datum ist, soll das komplette Datum angezeigt werden, ansonsten nur das Erscheinungsjahr.*\n",
    "\n",
    "Zur Umsetzung muss die Spalte \"publication_date\" in ein Datum verwandelt werden und nach den genannten Kriterien unterschiedlich angezeigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(time.strftime('%Y%m%d'))\n",
    "df_aleph_einspielen[\"coming_soon\"] = np.where(df_aleph_einspielen[\"publication_date\"].astype(int) > today+10, df_aleph_einspielen[\"publication_date\"], np.nan) #zieht die über 10 Tage raus, brauchen nan für Umwandlung in Datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"publication_date_soon\"] = df_aleph_einspielen[\"coming_soon\"].astype(str).str.replace('00','01')\n",
    "#df_aleph_einspielen[\"publication_date_soon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"year\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=0,stop=4)    #einfaches Zerlegen in die Datumsbestandteile und anschließendes Zusammenfügen\n",
    "df_aleph_einspielen[\"month\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=4,stop=6)\n",
    "df_aleph_einspielen[\"day\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=6,stop=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"full_coming_soon\"] = df_aleph_einspielen[\"year\"]+'-'+df_aleph_einspielen[\"month\"]+'-'+df_aleph_einspielen[\"day\"]\n",
    "df_aleph_einspielen[\"coming_soon\"] = df_aleph_einspielen[\"full_coming_soon\"].astype(str).str.replace('nan--','') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3       2023-11-16\n",
       "41                \n",
       "52      2023-11-12\n",
       "92                \n",
       "100     2023-11-16\n",
       "           ...    \n",
       "5321              \n",
       "5325    2023-11-09\n",
       "5326    2023-11-24\n",
       "5327              \n",
       "5328              \n",
       "Name: coming_soon, Length: 3098, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_einspielen[\"coming_soon\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3       2023-11-16\n",
       "41            2022\n",
       "52      2023-11-12\n",
       "92            2023\n",
       "100     2023-11-16\n",
       "           ...    \n",
       "5321          2023\n",
       "5325    2023-11-09\n",
       "5326    2023-11-24\n",
       "5327          2023\n",
       "5328          2023\n",
       "Name: date_combined, Length: 3098, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_einspielen[\"published\"] = np.where(df_aleph_einspielen[\"coming_soon\"] == '', df_aleph_einspielen[\"publication_year\"], '') #Auslesen und Kombinieren der Daten\n",
    "df_aleph_einspielen[\"year_publ\"] = df_aleph_einspielen[\"published\"].astype(str).str.slice(start=0,stop=4)                          #da wieder .0 am Ende war, Jahreszahl ausschneidens\n",
    "df_aleph_einspielen[\"date_combined\"] = df_aleph_einspielen[\"year_publ\"]+df_aleph_einspielen[\"coming_soon\"]\n",
    "df_aleph_einspielen['date_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aleph_einspielen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3       9781119962786\n",
       "41      9781119424345\n",
       "52      9781119526414\n",
       "92      9781119634065\n",
       "100     9781119549307\n",
       "            ...      \n",
       "5321    9781527529632\n",
       "5325    9781526631176\n",
       "5326    9781509558247\n",
       "5327    9781032347004\n",
       "5328    9781032466972\n",
       "Name: isbn_ean, Length: 3098, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_einspielen['isbn_ean'] = df_aleph_einspielen['isbn_ean'].astype(np.int64)   #da die Zahl als Object genommen wurde, bekam sie ein .0 angehängt, das ist durch umwandeln in Zahl weg\n",
    "\n",
    "df_aleph_einspielen['isbn_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"419b\"] = df_aleph_einspielen[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_aleph_einspielen[\"419c\"] = df_aleph_einspielen[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_aleph_einspielen[\"419\"] = df_aleph_einspielen[\"419b\"]+df_aleph_einspielen[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"403\"] = df_aleph_einspielen[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_aleph_einspielen[\"433\"] = df_aleph_einspielen[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_aleph_einspielen[\"451\"] = df_aleph_einspielen[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_aleph_einspielen[\"520\"] = df_aleph_einspielen[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_aleph_einspielen[\"540\"] = df_aleph_einspielen[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_aleph_einspielen[\"656\"] = df_aleph_einspielen[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_aleph_einspielen[\"750\"] = df_aleph_einspielen[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Für das Durchzählen der Titel braucht es eine neue Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier entsteht eine neue Spalte mit Zahlen ab 1 durchgehend gezählt, die für den korrekten Import der Daten in Aleph nötig ist\n",
    "x = df_aleph_einspielen.shape[0]   \n",
    "df_aleph_einspielen[\"id\"] = range(1,x+1)                                                       #Notwendig ist die Zählung ab 1, da Aleph sonst nicht korrekt einließt\n",
    "df_aleph_einspielen[\"id\"] = df_aleph_einspielen[\"id\"].apply(lambda x: f\"{x:09d}\")              #Die Zahl muss 9-Stellig aufgefüllt werden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Vorbereitungen abgeschlossen, jetzt das Schreiben der Datei im Aleph-Sequential-Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./output/ges02_neu\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_einspielen.index:\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' LDR   L -----nM2.01200024------h'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"020\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 030   L $$aaz||rrrza||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 051   L $$am|||||||m|||||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 077   L $$aMonographie'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 078   L $$aSchweitzer'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 082   L $$azum Bestellen'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 100   L $$a'+df_aleph_einspielen[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 104   L $$a'+df_aleph_einspielen[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 108   L $$a'+df_aleph_einspielen[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 331   L $$a'+df_aleph_einspielen[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 335   L $$a'+df_aleph_einspielen[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"403\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"419\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"433\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"451\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"520\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"540\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"750\"][i]+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Daten für Update\n",
    "\n",
    "*es wird anhand der Spalte \"last_modified\" geprüft, ob die Titel seit dem letzten Einspielen ein Update erfahren haben*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Um die Menge zum Updaten zu reduzieren, werden nur die rausgezogen, die tatsächlich ein Aktualisierungsdatum haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-13 15:30:28.472376\n"
     ]
    }
   ],
   "source": [
    "tday = datetime.datetime.now()\n",
    "td = datetime.timedelta(days = 7)\n",
    "u = tday - td\n",
    "df_in_aleph_update['timespan'] = u                                                           # Einfügen einer Spalte mit Datum vor 10 Tagen als Basis für Abfrage zu Update-Notwendigkeit\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2023-10-05 04:10:29\n",
       "1       2023-09-19 19:01:38\n",
       "2       2023-10-12 19:53:59\n",
       "3       2023-07-21 18:38:22\n",
       "4       2023-10-10 13:31:17\n",
       "                ...        \n",
       "26720   2023-09-15 23:56:53\n",
       "26721   2023-09-17 19:09:30\n",
       "26722   2023-10-05 00:36:42\n",
       "26723   2023-10-06 05:58:26\n",
       "26724   2023-10-13 22:24:27\n",
       "Name: last_modified, Length: 26185, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update['last_modified'] = pd.to_datetime(df_in_aleph_update.last_modified)          #Umwandlung, da Spalteninhalt object ist\n",
    "df_in_aleph_update['last_modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       NaT\n",
       "1                       NaT\n",
       "2                       NaT\n",
       "3                       NaT\n",
       "4                       NaT\n",
       "                ...        \n",
       "26720                   NaT\n",
       "26721                   NaT\n",
       "26722                   NaT\n",
       "26723                   NaT\n",
       "26724   2023-10-13 22:24:27\n",
       "Name: true, Length: 26185, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update['true'] = np.where(df_in_aleph_update['last_modified'] > u, df_in_aleph_update['last_modified'], np.datetime64('NaT'))\n",
    "df_in_aleph_update['true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true = df_in_aleph_update[pd.notnull(df_in_aleph_update['true'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>object_id</th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>...</th>\n",
       "      <th>subtitle_comparison</th>\n",
       "      <th>subtitle_comparison2</th>\n",
       "      <th>subtitle_all</th>\n",
       "      <th>short_title</th>\n",
       "      <th>url_ges</th>\n",
       "      <th>url_ebx</th>\n",
       "      <th>Abfrage_ges</th>\n",
       "      <th>Abfrage_ebx</th>\n",
       "      <th>timespan</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>237277</td>\n",
       "      <td>9783825239312</td>\n",
       "      <td>Käuferverhalten</td>\n",
       "      <td>Eine marketingorientierte Einführung</td>\n",
       "      <td>Alfred Kuß;Torsten Tomczak;Silke Lennerts</td>\n",
       "      <td>UTB</td>\n",
       "      <td>Grundwissen der Ökonomik Band 1604</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Eine marketingorientierte Einführung</td>\n",
       "      <td></td>\n",
       "      <td>Eine marketingorientierte Einführung</td>\n",
       "      <td>Käuferverhalten Eine marketingorientierte Einf...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-15 19:14:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>2529703</td>\n",
       "      <td>9783825287887</td>\n",
       "      <td>Der Fragebogen</td>\n",
       "      <td>Von der Forschungsidee zur SPSS-Auswertung</td>\n",
       "      <td>Elisabeth Steiner;Michael Benesch</td>\n",
       "      <td>UTB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Von der Forschungsidee zur SPSS-Auswertung</td>\n",
       "      <td></td>\n",
       "      <td>Von der Forschungsidee zur SPSS-Auswertung</td>\n",
       "      <td>Der Fragebogen Von der Forschungsidee zur SPSS...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-17 18:34:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>2534204</td>\n",
       "      <td>9783593512747</td>\n",
       "      <td>Mission</td>\n",
       "      <td>Auf dem Weg zu einer neuen Wirtschaft</td>\n",
       "      <td>Mariana Mazzucato</td>\n",
       "      <td>Campus</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Auf dem Weg zu einer neuen Wirtschaft</td>\n",
       "      <td></td>\n",
       "      <td>Auf dem Weg zu einer neuen Wirtschaft</td>\n",
       "      <td>Mission Auf dem Weg zu einer neuen Wirtschaft ...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-16 18:46:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>2415518</td>\n",
       "      <td>9783847422730</td>\n",
       "      <td>Umweltpolitik und Ressourcenmanagement in China</td>\n",
       "      <td>Zerstörung - Protest - Aufbruch</td>\n",
       "      <td>Anja Senz</td>\n",
       "      <td>Verlag Barbara Budrich</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Zerstörung - Protest - Aufbruch</td>\n",
       "      <td></td>\n",
       "      <td>Zerstörung - Protest - Aufbruch</td>\n",
       "      <td>Umweltpolitik und Ressourcenmanagement in Chin...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-13 19:07:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>88</td>\n",
       "      <td>105</td>\n",
       "      <td>2539645</td>\n",
       "      <td>9783170377882</td>\n",
       "      <td>Konsumentenverhalten</td>\n",
       "      <td></td>\n",
       "      <td>Dirk-Mario Boltz;Volker Trommsdorff</td>\n",
       "      <td>Kohlhammer</td>\n",
       "      <td>Kohlhammer Die Roten Hefte Heft 31</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Konsumentenverhalten  / Dirk-Mario Boltz;Volke...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-16 18:46:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26711</th>\n",
       "      <td>32037</td>\n",
       "      <td>18583</td>\n",
       "      <td>163543435</td>\n",
       "      <td>9783030967079</td>\n",
       "      <td>A Century of East African Integration</td>\n",
       "      <td></td>\n",
       "      <td>Claire A. Amuhaya;Denis A. Degterev</td>\n",
       "      <td>Palgrave Macmillan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>A Century of East African Integration  / Clair...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-19 18:39:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26717</th>\n",
       "      <td>32045</td>\n",
       "      <td>13800</td>\n",
       "      <td>162501862</td>\n",
       "      <td>9780367700409</td>\n",
       "      <td>40 Classic Crude Oil Trades</td>\n",
       "      <td>Real-Life Examples of Innovative Trading</td>\n",
       "      <td>Owain Johnson</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>Routledge Classic Market Trades</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Real-Life Examples of Innovative Trading</td>\n",
       "      <td></td>\n",
       "      <td>Real-Life Examples of Innovative Trading</td>\n",
       "      <td>40 Classic Crude Oil Trades Real-Life Examples...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-17 17:17:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26718</th>\n",
       "      <td>32046</td>\n",
       "      <td>28460</td>\n",
       "      <td>165662769</td>\n",
       "      <td>9781324064879</td>\n",
       "      <td>21st Century Monetary Policy</td>\n",
       "      <td>The Federal Reserve from the Great Inflation t...</td>\n",
       "      <td>Ben S. Bernanke</td>\n",
       "      <td>WW Norton &amp; Co</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>The Federal Reserve from the Great Inflation t...</td>\n",
       "      <td></td>\n",
       "      <td>The Federal Reserve from the Great Inflation t...</td>\n",
       "      <td>21st Century Monetary Policy The Federal Reser...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-18 17:26:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26719</th>\n",
       "      <td>32047</td>\n",
       "      <td>2341</td>\n",
       "      <td>152096035</td>\n",
       "      <td>9780367426569</td>\n",
       "      <td>20th Century Britain</td>\n",
       "      <td>Economic, Cultural and Social Change</td>\n",
       "      <td>Nicole Robertson</td>\n",
       "      <td>Routledge</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Economic, Cultural and Social Change</td>\n",
       "      <td></td>\n",
       "      <td>Economic, Cultural and Social Change</td>\n",
       "      <td>20th Century Britain Economic, Cultural and So...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-17 17:17:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26724</th>\n",
       "      <td>32053</td>\n",
       "      <td>28466</td>\n",
       "      <td>165662822</td>\n",
       "      <td>9781350276765</td>\n",
       "      <td>'Economy' in European History</td>\n",
       "      <td>Words, Contexts and Change over Time</td>\n",
       "      <td>Luigi Alonzi</td>\n",
       "      <td>Bloomsbury Academic</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>Words, Contexts and Change over Time</td>\n",
       "      <td></td>\n",
       "      <td>Words, Contexts and Change over Time</td>\n",
       "      <td>'Economy' in European History Words, Contexts ...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ges01&amp;reque...</td>\n",
       "      <td>http://aleph.mpg.de/X?op=find&amp;base=ebx01&amp;reque...</td>\n",
       "      <td>neu</td>\n",
       "      <td>neu</td>\n",
       "      <td>2023-10-13 15:30:28.472376</td>\n",
       "      <td>2023-10-13 22:24:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7798 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  index  object_id       isbn_ean  \\\n",
       "26          26     26     237277  9783825239312   \n",
       "27          27     27    2529703  9783825287887   \n",
       "34          35     35    2534204  9783593512747   \n",
       "46          49     52    2415518  9783847422730   \n",
       "77          88    105    2539645  9783170377882   \n",
       "...        ...    ...        ...            ...   \n",
       "26711    32037  18583  163543435  9783030967079   \n",
       "26717    32045  13800  162501862  9780367700409   \n",
       "26718    32046  28460  165662769  9781324064879   \n",
       "26719    32047   2341  152096035  9780367426569   \n",
       "26724    32053  28466  165662822  9781350276765   \n",
       "\n",
       "                                                 title  \\\n",
       "26                                     Käuferverhalten   \n",
       "27                                      Der Fragebogen   \n",
       "34                                             Mission   \n",
       "46     Umweltpolitik und Ressourcenmanagement in China   \n",
       "77                                Konsumentenverhalten   \n",
       "...                                                ...   \n",
       "26711            A Century of East African Integration   \n",
       "26717                      40 Classic Crude Oil Trades   \n",
       "26718                     21st Century Monetary Policy   \n",
       "26719                             20th Century Britain   \n",
       "26724                    'Economy' in European History   \n",
       "\n",
       "                                                subtitle  \\\n",
       "26                  Eine marketingorientierte Einführung   \n",
       "27            Von der Forschungsidee zur SPSS-Auswertung   \n",
       "34                 Auf dem Weg zu einer neuen Wirtschaft   \n",
       "46                       Zerstörung - Protest - Aufbruch   \n",
       "77                                                         \n",
       "...                                                  ...   \n",
       "26711                                                      \n",
       "26717           Real-Life Examples of Innovative Trading   \n",
       "26718  The Federal Reserve from the Great Inflation t...   \n",
       "26719               Economic, Cultural and Social Change   \n",
       "26724               Words, Contexts and Change over Time   \n",
       "\n",
       "                                     contributor               publisher  \\\n",
       "26     Alfred Kuß;Torsten Tomczak;Silke Lennerts                     UTB   \n",
       "27             Elisabeth Steiner;Michael Benesch                     UTB   \n",
       "34                             Mariana Mazzucato                  Campus   \n",
       "46                                     Anja Senz  Verlag Barbara Budrich   \n",
       "77           Dirk-Mario Boltz;Volker Trommsdorff              Kohlhammer   \n",
       "...                                          ...                     ...   \n",
       "26711        Claire A. Amuhaya;Denis A. Degterev      Palgrave Macmillan   \n",
       "26717                              Owain Johnson               Routledge   \n",
       "26718                            Ben S. Bernanke          WW Norton & Co   \n",
       "26719                           Nicole Robertson               Routledge   \n",
       "26724                               Luigi Alonzi     Bloomsbury Academic   \n",
       "\n",
       "                                   series thesis  ...  \\\n",
       "26     Grundwissen der Ökonomik Band 1604         ...   \n",
       "27                                                ...   \n",
       "34                                                ...   \n",
       "46                                                ...   \n",
       "77     Kohlhammer Die Roten Hefte Heft 31         ...   \n",
       "...                                   ...    ...  ...   \n",
       "26711                                             ...   \n",
       "26717     Routledge Classic Market Trades         ...   \n",
       "26718                                             ...   \n",
       "26719                                             ...   \n",
       "26724                                             ...   \n",
       "\n",
       "                                     subtitle_comparison subtitle_comparison2  \\\n",
       "26                  Eine marketingorientierte Einführung                        \n",
       "27            Von der Forschungsidee zur SPSS-Auswertung                        \n",
       "34                 Auf dem Weg zu einer neuen Wirtschaft                        \n",
       "46                       Zerstörung - Protest - Aufbruch                        \n",
       "77                                                                              \n",
       "...                                                  ...                  ...   \n",
       "26711                                                                           \n",
       "26717           Real-Life Examples of Innovative Trading                        \n",
       "26718  The Federal Reserve from the Great Inflation t...                        \n",
       "26719               Economic, Cultural and Social Change                        \n",
       "26724               Words, Contexts and Change over Time                        \n",
       "\n",
       "                                            subtitle_all  \\\n",
       "26                  Eine marketingorientierte Einführung   \n",
       "27            Von der Forschungsidee zur SPSS-Auswertung   \n",
       "34                 Auf dem Weg zu einer neuen Wirtschaft   \n",
       "46                       Zerstörung - Protest - Aufbruch   \n",
       "77                                                         \n",
       "...                                                  ...   \n",
       "26711                                                      \n",
       "26717           Real-Life Examples of Innovative Trading   \n",
       "26718  The Federal Reserve from the Great Inflation t...   \n",
       "26719               Economic, Cultural and Social Change   \n",
       "26724               Words, Contexts and Change over Time   \n",
       "\n",
       "                                             short_title  \\\n",
       "26     Käuferverhalten Eine marketingorientierte Einf...   \n",
       "27     Der Fragebogen Von der Forschungsidee zur SPSS...   \n",
       "34     Mission Auf dem Weg zu einer neuen Wirtschaft ...   \n",
       "46     Umweltpolitik und Ressourcenmanagement in Chin...   \n",
       "77     Konsumentenverhalten  / Dirk-Mario Boltz;Volke...   \n",
       "...                                                  ...   \n",
       "26711  A Century of East African Integration  / Clair...   \n",
       "26717  40 Classic Crude Oil Trades Real-Life Examples...   \n",
       "26718  21st Century Monetary Policy The Federal Reser...   \n",
       "26719  20th Century Britain Economic, Cultural and So...   \n",
       "26724  'Economy' in European History Words, Contexts ...   \n",
       "\n",
       "                                                 url_ges  \\\n",
       "26     http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "27     http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "34     http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "46     http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "77     http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "...                                                  ...   \n",
       "26711  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26717  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26718  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26719  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "26724  http://aleph.mpg.de/X?op=find&base=ges01&reque...   \n",
       "\n",
       "                                                 url_ebx Abfrage_ges  \\\n",
       "26     http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "27     http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "34     http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "46     http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "77     http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "...                                                  ...         ...   \n",
       "26711  http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "26717  http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "26718  http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "26719  http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "26724  http://aleph.mpg.de/X?op=find&base=ebx01&reque...         neu   \n",
       "\n",
       "      Abfrage_ebx                   timespan                true  \n",
       "26            neu 2023-10-13 15:30:28.472376 2023-10-15 19:14:58  \n",
       "27            neu 2023-10-13 15:30:28.472376 2023-10-17 18:34:02  \n",
       "34            neu 2023-10-13 15:30:28.472376 2023-10-16 18:46:49  \n",
       "46            neu 2023-10-13 15:30:28.472376 2023-10-13 19:07:34  \n",
       "77            neu 2023-10-13 15:30:28.472376 2023-10-16 18:46:47  \n",
       "...           ...                        ...                 ...  \n",
       "26711         neu 2023-10-13 15:30:28.472376 2023-10-19 18:39:13  \n",
       "26717         neu 2023-10-13 15:30:28.472376 2023-10-17 17:17:50  \n",
       "26718         neu 2023-10-13 15:30:28.472376 2023-10-18 17:26:39  \n",
       "26719         neu 2023-10-13 15:30:28.472376 2023-10-17 17:17:42  \n",
       "26724         neu 2023-10-13 15:30:28.472376 2023-10-13 22:24:27  \n",
       "\n",
       "[7798 rows x 42 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen der Update-Export-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zwischenschritt, um für alle in Aleph das Update zu starten\n",
    "#df_in_aleph_update_true = df_in_aleph_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"020\"] = df_in_aleph_update_true[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_in_aleph_update_true[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufbereitung Personendaten\n",
    "\n",
    "person = df_in_aleph_update_true[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= person[0]\n",
    "df_in_aleph_update_true[\"contributor_2\"]= person[1]\n",
    "df_in_aleph_update_true[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= df_in_aleph_update_true[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_2\"]= df_in_aleph_update_true[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_3\"]= df_in_aleph_update_true[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neue Schritte zur Aufbereitung des Erscheinungsdatums, da der Code von oben hier nicht ging - warum auch immer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#today = int(time.strftime('%Y%m%d'))\n",
    "#soon = today+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl'] = df_in_aleph_update_true['publication_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26       20240715\n",
       "27       20210329\n",
       "34       20210510\n",
       "46       20240617\n",
       "77       20230100\n",
       "           ...   \n",
       "26711    20230404\n",
       "26717    20220131\n",
       "26718    20230630\n",
       "26719    20221230\n",
       "26724    20230921\n",
       "Name: publ_date_repl, Length: 7798, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true['publ_date_repl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl'] = df_in_aleph_update_true['publ_date_repl'].str.replace('00','01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20230925    290\n",
       "20231012     78\n",
       "20231031     78\n",
       "20231010     68\n",
       "20230531     59\n",
       "           ... \n",
       "20230722      1\n",
       "20240822      1\n",
       "19621119      1\n",
       "20210922      1\n",
       "20290101      1\n",
       "Name: publ_date_repl, Length: 1018, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true['publ_date_repl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26      2024-07-15\n",
       "27      2021-03-29\n",
       "34      2021-05-10\n",
       "46      2024-06-17\n",
       "77      2023-01-01\n",
       "           ...    \n",
       "26711   2023-04-04\n",
       "26717   2022-01-31\n",
       "26718   2023-06-30\n",
       "26719   2022-12-30\n",
       "26724   2023-09-21\n",
       "Name: publ_date_repl_date, Length: 7798, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true['publ_date_repl_date'] = pd.to_datetime(df_in_aleph_update_true['publ_date_repl'], errors='coerce')\n",
    "df_in_aleph_update_true['publ_date_repl_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"coming_soon\"] = np.where(df_in_aleph_update_true[\"publ_date_repl_date\"] > u, df_in_aleph_update_true[\"publ_date_repl_date\"], np.datetime64('NaT'))  #df_in_aleph_update_true['today'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26      2024-07-15\n",
       "27             NaT\n",
       "34             NaT\n",
       "46      2024-06-17\n",
       "77             NaT\n",
       "           ...    \n",
       "26711          NaT\n",
       "26717          NaT\n",
       "26718          NaT\n",
       "26719          NaT\n",
       "26724          NaT\n",
       "Name: coming_soon, Length: 7798, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true[\"coming_soon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_in_aleph_update_true[\"coming_soon\"].replace('NaT','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26       2024-07-15\n",
       "27             2021\n",
       "34             2021\n",
       "46       2024-06-17\n",
       "77             2023\n",
       "            ...    \n",
       "26711          2023\n",
       "26717          2022\n",
       "26718          2023\n",
       "26719          2022\n",
       "26724          2023\n",
       "Name: date_combined, Length: 7798, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true[\"date_combined\"] = np.where(df_in_aleph_update_true[\"coming_soon\"].astype(str) == 'NaT', df_in_aleph_update_true[\"publication_year\"], df_in_aleph_update_true[\"coming_soon\"].astype(str))\n",
    "df_in_aleph_update_true[\"date_combined\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26       9783825239312\n",
       "27       9783825287887\n",
       "34       9783593512747\n",
       "46       9783847422730\n",
       "77       9783170377882\n",
       "             ...      \n",
       "26711    9783030967079\n",
       "26717    9780367700409\n",
       "26718    9781324064879\n",
       "26719    9780367426569\n",
       "26724    9781350276765\n",
       "Name: isbn_ean, Length: 7798, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update_true['isbn_ean'] = df_in_aleph_update_true['isbn_ean'].astype(np.int64)   #da die Zahl als Object genommen wurde, bekam sie ein .0 angehängt, das ist durch umwandeln in Zahl weg\n",
    "\n",
    "df_in_aleph_update_true['isbn_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearbeitung der Felder\n",
    "\n",
    "df_in_aleph_update_true[\"419b\"] = df_in_aleph_update_true[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"419c\"] = df_in_aleph_update_true[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_in_aleph_update_true[\"419\"] = df_in_aleph_update_true[\"419b\"]+df_in_aleph_update_true[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte\n",
    "\n",
    "df_in_aleph_update_true[\"403\"] = df_in_aleph_update_true[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"433\"] = df_in_aleph_update_true[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_in_aleph_update_true[\"451\"] = df_in_aleph_update_true[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"520\"] = df_in_aleph_update_true[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"540\"] = df_in_aleph_update_true[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"656\"] = df_in_aleph_update_true[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_in_aleph_update_true[\"750\"] = df_in_aleph_update_true[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Ausgabedatei, hier kleine Unterschiede zu den neuen Titeln. Vorhandene ids und bestimmte Felder können nicht verändert sein, brauchen also nicht übernommen zu werden.\n",
    "\n",
    "with open(\"./output/ges02_update\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_update_true.index:\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 100   L $$a'+df_in_aleph_update_true[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 104   L $$a'+df_in_aleph_update_true[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 108   L $$a'+df_in_aleph_update_true[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 331   L $$a'+df_in_aleph_update_true[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 335   L $$a'+df_in_aleph_update_true[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"403\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"419\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"433\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"451\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"520\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"540\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"750\"][i]+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschließende Dinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abschließender Eintrag in Log-Datei\n",
    "endtime = time.strftime('%H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('\\n                                     beendet ')\n",
    "    log.write(endtime)\n",
    "    log.write(\"\\n============================================================\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopien bestimmter Daten zur Einsicht bzw. für Prüfzwecke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen.to_csv('./output/Eingespielte_Titel_'+date+'.csv') \n",
    "df_in_aleph_update_true.to_csv('./output/Update_Titel_'+date+'.csv')\n",
    "\n",
    "df_in_aleph_nicht_einspielen.to_csv('./output/Aleph_loeschen_ebx_vorh_'+date+'.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Folgende Schritte müssen mit den Dateien ausgeführt werden: \n",
    "\n",
    "\n",
    "1. Einspielen der Datei pda_ges01 als neue Titel in Aleph, hier dann auch Export der urls und Anreicherung mit der Aleph-ID mittels \"mailto_link_skript.ipynb\"\n",
    "2. Einspielen der DAtei pda_update als \"Änderungen bestehender Datensätze in Aleph\"\n",
    "3. Einspielen und löschen der Titel die in ges02_loeschen_1 und ges02_loeschen_2 vorhanden sind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "4fac973d8b48a5fcf37e7d133428a31fb47ebbd054f5d1feed8c0da486f2af46"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
