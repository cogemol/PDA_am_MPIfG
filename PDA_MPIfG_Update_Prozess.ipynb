{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buchhandelsdaten in Vufind als Grundlage für PDA (Patron Driven Aquisition) am MPIfG / Version 2: Updates laden statt Vollimport\n",
    "\n",
    "Einbindung von freundlicherweise von Schweitzer Fachinformation zur Verfügung gestellten Daten.   \n",
    "(Auswahl über passend konfigurierte Neuerscheinungsabfragen in unserem Kundenprofil).   \n",
    "\n",
    "#### Gründe für die Umstellung auf Updates: Durch Einspielen der Daten in Vufind ergibt sich eine zeitliche Diskrepanz, in der Titel bereits in Aleph gelöscht sind, aber in Vufind noch bestellbar. Zudem werden die Aleph-IDs hochgezähl, denn bei jedem Einspielen werden neue Nummern vergeben. Durch die Updates bleiben die Titel weiterhin verfügbar und das Hochzählen der IDs hält sich im Rahmen. Das Skript beschleunigt sich durch Reduzierung der Abfragen am Server.\n",
    "*Kleiner Nachteil: der manuelle Aufwand vergrößert sich. Mehrere Dateien müssen in Aleph eingespielt und verarbeitet werden, aber hält sich nach wie vor im vertretbaren Rahmen.*\n",
    "\n",
    "Das Jupyter Notebook arbeitet mit Python 3.8.1 und wurde mit Visual Studio Code 1.62.3 erstellt \n",
    "\n",
    "\n",
    "#### Arbeitsschritte im Code:\n",
    "\n",
    "> Vorarbeiten:   \n",
    "  - Notwendige Pandas Libraries aufrufen\n",
    "  - Serverprüfung auf funktionierende Verbindung zum Aleph-X-Server    \n",
    "\n",
    "\n",
    "> Daten abholen und einlesen:   \n",
    "  1. Buchhandelsdaten von Schweitzer \n",
    "  2. Aleph-Konkordanz Aleph-ID /Schweitzer ID  (erzeugt tagesaktuell per p-print-03 in Aleph)\n",
    "     - Aufbereiten der Daten: Schweitzer ID extrahieren und Aleph-ID mit Nullen auffüllen\n",
    "  3. Daten zusammenführen in einem df    \n",
    "\n",
    "\n",
    "> Daten aufbereiten:   \n",
    "  1. Buchhandelsdaten prüfen und vorbereiten\n",
    "     1. Identifizierung von Titeln in Aleph, die nicht mehr im Datensatz sind und schreiben in Datei \"ges02_loeschen_1\"\n",
    "     2. Dublettencheck innerhalb der Buchhandelsdaten\n",
    "     3. Trennung der Daten in \"in Aleph vorhanden\" und \"neu\"  \n",
    "  2. Bestandsabfragen:\n",
    "     1. Ganz neue Titel   \n",
    "        - Bestandsabgleich durch Abfragen (GES und EBX) auf dem Aleph-Server   \n",
    "     2. In Aleph vorhanden\n",
    "        - Bestandsabgleich durch Abfrage EBX auf dem Aleph-Server  \n",
    "     3. Exportvorbereitungen:\n",
    "        1. Neue Titel\n",
    "        2. In Aleph vorhandene Titel   \n",
    "            Identifizierung von zu löschenden Titeln \"ges02_loeschen_2\" und der zu aktualisierenden\n",
    "  3. Exportdateien aufbereiten:\n",
    "     1. Aufbereiten neuer Titel\n",
    "     2. Aufbereiten der vorhandenen Titel   \n",
    "\n",
    "\n",
    "> Informationssammlung\n",
    "   1. Log-Datei mit Rahmendaten wird fortlaufend geschrieben\n",
    "   2. Ausgabe bestimmter Titelgruppen als csv-Datei  \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorarbeiten\n",
    "\n",
    "### Pandas Libraries laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                # für das Arbeiten mit der CSV-Datei\n",
    "import urllib.request                                              # für das Abrufen der URL\n",
    "import requests                                                    #für die Bestandsabfragen \n",
    "pd.options.mode.chained_assignment = None                          # default='warn' abschalten beim Beschreiben der neuen Spalten\n",
    "import time                                                        # für das Schreiben des Datums Logdatei und Excel-Export und Arbeiten mit dem Erscheinungsdatum\n",
    "import datetime                                                    # für das Berechnen des Updates\n",
    "import numpy as np                                                 # für das Bearbeiten von Spalten\n",
    "import pymarc                                                      # für das Einlesen der vorhandenen Titel im Marc-Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung, ob die Verbindung zum Aleph-Server für Abfragen korrekt funktioniert:\n",
    "\n",
    "    Nur zugelassene IPs können diese Schnittstelle abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783482648434\"\n",
    "\n",
    "reply = requests.get(test).text\n",
    "a = reply.find(\"Forbidden\")\n",
    "b =  reply.find(\"?xml\")\n",
    "\n",
    "if (a > 50):\n",
    "    print(\"Es gibt ein Problem mit dem Server\")\n",
    "if (b == 1):\n",
    "    print(\"Der Server antwortet korrekt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783865058041\"\n",
    "\n",
    "reply = requests.get(test).text \n",
    "\n",
    "print(reply) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensätze abholen und einlesen\n",
    "\n",
    "### 1. Datensätze von Schweitzer\n",
    "\n",
    "*einlesen in df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://content.schweitzer-online.de/static/content/export/mpifg/export.csv\"  # Abruf, der von Schweitzer zur Verfügung gestellten Daten\n",
    "checkout_file = \"./input/export.csv\"  \n",
    "urllib.request.urlretrieve(url, checkout_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/export.csv', encoding = 'UTF-8', sep=';' , keep_default_na=False) # muss encoding angeben und Trennzeichen, NaN (= leere Werte) direkt beim Import entfernen, da sie später Probleme machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Testbereich wegen Dubletten in Aleph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pymarc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_file = './input/export_vzg_daily_ges02.mrc'\n",
    "df_ges02vufind = pd.read_fwf(input_file, encoding = 'UTF-8', sep='\\n', header=None, keep_default_na=False) \n",
    "#df_ges02test.columns=[\"ID\",\"Field\",\"L\",\"Content\",\"Feld4\", \"Feld5\"]\n",
    "df_ges02vufind.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_ges02vufind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_file = './input/export_vzg_daily_ges02.mrc'\n",
    "\n",
    "reader = pymarc.MARCReader(open(input_file, 'rb'), force_utf8=\"True\") \n",
    "\n",
    "for record in reader:\n",
    "    print (record)\n",
    "    quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_ges02vufind_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebereich\n",
    "input_file2 = './input/ges02test'\n",
    "df_ges02test = pd.read_fwf(input_file2, encoding = 'UTF-8', sep='\\s\\s', header=None, keep_default_na=False) \n",
    "df_ges02test.columns=[\"ID\",\"Field\",\"L\",\"Content\",\"Feld4\", \"Feld5\"]\n",
    "df_ges02test.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_titel = df_ges02test.loc[df_ges02test['Field'] == 331]\n",
    "\n",
    "#loc[((df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df_ges02test.sort_values(\"Field\")\n",
    "df_titel[\"Content\"].count_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_dub_titel1 = df_titel.sort_values(by=[\"Content\", \"ID\"]).drop_duplicates(subset=[\"Content\"], keep='first')\n",
    "\n",
    "df_dub_titel1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_titel[\"Content\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_dub_titel2 = df_titel.sort_values(by=[\"Content\", \"ID\"], ascending = False).drop_duplicates(subset=[\"Content\"], keep=False)\n",
    "\n",
    "df_dub_titel2\n",
    "\n",
    "\n",
    "\n",
    "#sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='last')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testbereich Dubletten in Aleph-Ende\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aleph-Konkordanz Aleph-ID / Schweitzer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime('%Y%m%d')  # %H:%M:%S\n",
    "input_file = './input/ids'+now      #input_file wird tagesaktuell aus Aleph gezogen und auf diesem Wege mit der entsprechenden Endung eingelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_alephIDs = pd.read_fwf(input_file, encoding = 'UTF-8', sep='\\s\\s', header=None, keep_default_na=False) \n",
    "df_alephIDs.columns=[\"ID\",\"Field\",\"L\",\"Content\"]\n",
    "df_alephIDs\n",
    "\n",
    "#Spalte 0 = Aleph-IDs müsste vor Export mit Nullen aufgefüllt werden \n",
    "#Spalte 3 = object_id aus df, hier muss zum Abgleich $$a entfernt werden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Datenfelder aufbereiten: Schweitzer-ID extrahieren und Aleph-ID ins richtige Format bringen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alephIDs[\"object_id\"] = df_alephIDs[\"Content\"].astype(str).str.slice(start=3,stop=13).apply(int)   #Spalte mit object-Ids herausschneiden und wieder zur Zahl definieren\n",
    "\n",
    "df_alephIDs[\"object_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aleph-ID-Spalte mit Nullen Auffüllen\n",
    "df_alephIDs[\"ids\"] = df_alephIDs['ID'].apply(lambda x: f\"{x:09d}\")\n",
    "df_alephIDs[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nach Dubletten vom letzten Mal vorsichtshalber, check nach doppelten object-ids\n",
    "df_alephIDs[\"object_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Aleph gibt es aber Dubletten -nachforschen. Sie haben unterschiedliche object-ids!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Daten zu einem Frame zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführung erfolgt einmal mit concat - erlaubt die nicht mehr verwendeten Aleph-IDs zu identifizieren \n",
    "# und einmal mit join, dann erhalte ich nur die Titel, die auch im neuen Datensatz sind zur weiteren Verarbeitung!\n",
    "df_oi= df.set_index(\"object_id\")                          #object-ID zum Index für beide Datenframes\n",
    "df_aleph_oi = df_alephIDs.set_index(\"object_id\")\n",
    "df_update_aleph = pd.concat([df_oi, df_aleph_oi], axis=1)\n",
    "\n",
    "df_update_join = df_oi.join(df_aleph_oi)                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG-Datei für den Prozess, zur Dokumentation des Imports und als Kontrollanzeige hier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.shape[0]\n",
    "print('Anzahl der enthaltenen Datensätze:', x)\n",
    "#print('vorhandene ISBNs:', df_update[\"isbn_ean\"].shape[0])\n",
    "\n",
    "z = df_alephIDs[\"ids\"].count()\n",
    "y = df_update_join[\"ids\"].count()\n",
    "\n",
    "print('-------------------------')\n",
    "print('Aleph-IDs anfangs', z)\n",
    "print('Aleph-IDs nach join' , y)\n",
    "\n",
    "timestr = time.strftime('%d.%m.%Y - %H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('Logdatei PDA-Import vom ')\n",
    "    log.write(timestr)\n",
    "    log.write('\\n------------------------------------------\\n')\n",
    "    log.write('Gelieferte Datensätze:             ' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemlösung der Aleph-Dubletten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_alephIDs[\"object_id\"].value_counts\n",
    "\n",
    "df_aleph_einzel = df_alephIDs.drop_duplicates(\"object_id\", keep=False)  #Auslesen der Eintraege mit einzelner object_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_aleph_doppelt = df_alephIDs.groupby(\"object_id\").filter(lambda g: (g.nunique() >1).any()) #Doppelte object_ids rausziehen\n",
    "df_aleph_single = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='first')  #von Dubletten 1. behalten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_aleph_vorhanden = df_aleph_einzel.append(df_aleph_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1. Identifizierung von Titeln in Aleph-Dubletten, die nicht mehr im Datensatz sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update_aleph.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_loeschen = df_update_aleph[df_update_aleph['isbn_ean'].isna()]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_loeschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Loeschen-datei!\n",
    "\n",
    "with open(\"./output/ges02_loeschen_1\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_loeschen.index:\n",
    "        fa.write(df_aleph_loeschen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Dublettencheck innerhalb gelieferten Buchhandelsdaten \n",
    "\n",
    "Aufgaben im Rahmen des Dublettencheck:\n",
    "1. Dublettenkontrolle anhand von Titel, Untertitel und Autor \n",
    "   - Zunächst Behebung der unsauberen Titel / Untertitel-Trennung für korrekteren Abgleich\n",
    "   - Trennung der Datensätze in Dubletten und \"Einzeltitel\"\n",
    "     - Einzeltitel werden direkt für Bestandsprüfung vorgemerkt\n",
    "     - Dubletten werden auf neueste Version reduziert und diese der Bestandsprüfungsdatei hinzugefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Dublettenkontrolle Anhand von Titel, Untertitel und Autor\n",
    "\n",
    "*Entfernen von Untertiteln aus der Titelspalte, Extrahieren von Untertiteln und Abgleich mit Untertitelspalte und Schreiben der vorhandenen Informationen in neue Untertitel-Spalte.   \n",
    "Durch diese Spalte werden ca. 1/3 mehr Dubletten erkannt, als ohne die Bereinigung. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_update_join[df_update_join['isbn_ean'].notna()]  #jetzt alle die zu verarbeiten sind rausziehen\n",
    "df.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu = df[\"title\"].str.split(':', n = 1, expand = True)  #Titel am 1. Doppelpunkt splitten und getrennt in neue Felder schreiben\n",
    "df[\"title_sep\"]= neu[0]\n",
    "df[\"subtitle_sep\"]= neu[1]\n",
    "\n",
    "df[\"subtitle_sep\"] = df[\"subtitle_sep\"].replace(np.nan, '', regex=True) #NaN-Werte stören, darum raus damit ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = np.where(df[\"subtitle\"] == df[\"subtitle_sep\"], '', df[\"subtitle\"])    # Abgleich - wenn in beiden das Gleiche steht, dann ursprüngliches \"Subtitle\"-Feld nehmen\n",
    "df[\"subtitle_comparison\"] = comparison  \n",
    "\n",
    "comparison2 = np.where(df[\"subtitle\"] < df[\"subtitle_sep\"], df[\"subtitle_sep\"], '') # Wenn nur in \"subtitle_sep\" Infos stehen, diese übernehmen, das ist noch nicht ganz sauber, da hier manchmal anderes steht als in \"subtitle\"\n",
    "df[\"subtitle_comparison2\"] = comparison2 \n",
    "\n",
    "df[\"subtitle_all\"] = df[\"subtitle_comparison\"]+df[\"subtitle_comparison2\"]          # Beide Informationen in neuer Subtitle-Spalte zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short_title\"] = df[\"title_sep\"] + ' ' + df[\"subtitle_all\"] + ' / ' + df[\"contributor\"]  # aus den bereinigten Daten einen Kurztitel erzeugen, der dann für den Dublettencheck verwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dubletten = df.groupby(\"short_title\").filter(lambda g: (g.nunique() >1).any()) # schreibt alle mehrfach vorhandenen Titel in ein eigenes Datenframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dubl_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep='first')   # sortiert Dubletten nach Jahr und schreibt den jeweils ersten (= neuesten) Eintrag in neues Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohne_dubletten = df.drop_duplicates(\"short_title\", keep=False)       #durch \"\"keep=False\" werden alle nicht-Dubletten rausgezogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dubl_nicht_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep=False)\n",
    "\n",
    "#Kontrolle, ob es in Dubletten Titel gibt, die schon in Aleph sind\n",
    "df_dubl_nicht_einspielen[\"ids\"].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_einspielen = df_ohne_dubletten.append(df_dubl_einspielen)                    # die ausgewählten Dubletten und alle Nicht-Dubletten werden in ein Datenframe zusammengeführt\n",
    "df_einspielen.reset_index(inplace=True)                                         # für weitere Bearbeitung index-reset nötig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Überblick zu den Daten und Trennen in \"in Aleph vorhanden\" und \"neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df_einspielen.shape[0]\n",
    "n = df_einspielen[\"ids\"].count()\n",
    "print(\"Zum Einspielen:\", m)\n",
    "print(\"Davon in Aleph:\", n)\n",
    "print(\"Neue Titel:\", m-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph = df_einspielen.dropna(subset=['ids'])\n",
    "df_in_aleph.reset_index(inplace=True)\n",
    "df_in_aleph.shape[0]\n",
    "\n",
    "# zur Prüfung am GES und Ebx Bestand geprüft werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu = df_einspielen[df_einspielen['ids'].isnull()]\n",
    "\n",
    "df_ganz_neu.reset_index(inplace=True)\n",
    "df_ganz_neu.shape[0]\n",
    "\n",
    "# diese müssen ebenfalls am Ges und Ebx Bestand geprüft werden, durch Trennung der Sets schnellere Bearbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bestandsabfragen\n",
    "\n",
    "\n",
    "\n",
    "*URLs für die Abfrage über den X-Server unseres Bibliothekssystems werden erzeugt und über die ISBN eine Abfrage auf Bestand gemacht. Die Abfrage funktioniert nur für zugelassene IPs (darum oben die Prüfung).  \n",
    "Für die Abfrage in unseren Bestand ist die ISBN sehr gut, da in den Titeldaten alle im Buch befindlichen ISBNs - auch die anderer Ausgabeformen - mit übernommen sind. Beim MPG-Ebooks Katalog handelt sich um Daten von Verlagen, die sich in ihrer Qualität und Informationsumfang sehr unterscheiden. Hier wird noch zu prüfen sein, inwieweit ein anderer Abfragemechansimus gewählt werden muss.*  \n",
    "\n",
    "### 1. Ganz neue Titel  \n",
    "\n",
    "= df_ganz_neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ges\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link','http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "#Es funktionierte nicht, dass die URLs an die vorhandenen ISBNs einfach so angefügt werden, darum der Workaround mit einem Platzhalter, der sich dann über replace vom richtigen Link überschreiben ließ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs für Ebooks-Katalog erzeugen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ebx\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n",
    "\n",
    "##### Zunächst für die Daten des MPIfG \n",
    "\n",
    "*Vorgehensweise: Abfrage und Sammeln der Antworten in einer Datei, diese Antworten werden dann in Ausdrücke \"übersetzt\" - \"vorhanden\" und \"neu\" und diese Daten in eine Spalte ins Dataframe zur weiteren Auswertung übertragen.   \n",
    "Schwierigkeit hier war, die Sammlung der Antworten zu den einzelnen Titeln, um sie in das Datenframe einzuspielen. Der störende XML-Header der Antworten wird erst gar nicht in die Datei geschrieben.   *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses', 'w') as fn:  \n",
    "    for url in df_ganz_neu[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses', 'r') as f:\n",
    "    with open('./input/server_responses_transfered', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf = pd.read_fwf('./input/server_responses_transfered', names=[\"Abfrage_ges\"])\n",
    "df_result = df_ganz_neu.join(df_fwf)                    #df_result = pd.concat([df_ganz_neu, df_fwf], axis=1), funktioniert nicht mehr                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Randnotiz: \n",
    "    Bei 2400 Titels brauchte der Abgleich ca 350 Sekunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_ganz_neu.shape[0]\n",
    "y = df_fwf.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datenabgleich mit dem Bestand des MPG Ebooks-Katalog\n",
    "\n",
    "*Vorgehensweise analog Bestandsabfrage MPI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx', 'w') as fn:  \n",
    "    for url in df_result[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx = pd.read_fwf('./input/server_responses_transfered_ebx', names=[\"Abfrage_ebx\"])\n",
    "df_result_neu = df_result.join(df_fwf_ebx)           #df_result_neu = pd.concat([df_result, df_fwf_ebx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_neu.shape[0]\n",
    "y = df_fwf_ebx.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Titel in Aleph vorhanden\n",
    "\n",
    "= df_in_aleph\n",
    "\n",
    "*Hier reicht Abgleich mit Ebooks, das erworbene Bücher manuell gelöscht werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph[\"url_ges\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link', 'http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "df_in_aleph[\"url_ebx\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses2', 'w') as fn:  \n",
    "    for url in df_in_aleph[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ges = pd.read_fwf('./input/server_responses_transfered2', names=[\"Abfrage_ges\"])\n",
    "df_result2 = df_in_aleph.join(df_fwf_ges)               #df_result2 = pd.concat([df_in_aleph, df_fwf_ges], axis=1)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_in_aleph.shape[0]\n",
    "y = df_fwf_ges.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2['Abfrage_ges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx2', 'w') as fn:  \n",
    "    for url in df_result2[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx2 = pd.read_fwf('./input/server_responses_transfered_ebx2', names=[\"Abfrage_ebx\"])\n",
    "df_result_in_aleph = df_result2.join(df_fwf_ebx2)           #df_result_in_aleph = pd.concat([df_result2, df_fwf_ebx2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_in_aleph.shape[0]\n",
    "y = df_fwf_ebx2.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Exportvorbereitungen \n",
    "\n",
    "\n",
    "### 1. Neue Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_nicht_einspielen =  df_result_neu.drop(df_result_neu[(df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu')].index)          \n",
    "# alle Titel rausholen, die in einer der beiden Datenbanken vorhanden waren, diese werden mit den vorhandenen aus Update unten in Excel geschrieben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Einspielen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_aleph_einspielen = df_result_neu.loc[((df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu'))]   #das sind die komplett neuen Titel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In Aleph vorhandene Titel\n",
    "\n",
    "*Hier ist der Fall: was im Ebooks-Katalog vorhanden ist, muss gelöscht werden*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_nicht_einspielen = df_result_in_aleph.drop(df_result_in_aleph[(df_result_in_aleph[\"Abfrage_ebx\"] == 'neu') & (df_result_in_aleph[\"Abfrage_ges\"] == 'neu')].index) \n",
    "df_in_aleph_nicht_einspielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen der Datei zum Löschen der Titel\n",
    "\n",
    "with open(\"./output/ges02_loeschen_2\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_nicht_einspielen.index:\n",
    "        fa.write(df_in_aleph_nicht_einspielen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Statistik und Kontrolle zusammenführen aller Titel, die nicht eingespielt werden und Ausgabe in einer CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gesamt_nicht_einspielen = df_in_aleph_nicht_einspielen.append(df_neu_nicht_einspielen)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y_%m_%d\")                                              # Zeit erfassen für Dateibenennung\n",
    "\n",
    "df_gesamt_nicht_einspielen[\"object_id\"] = df_gesamt_nicht_einspielen.object_id.astype(str)  # wandelt die spalte von Int64 zu Object um, so dass es in Excel korrekt eingelesen wird\n",
    "df_gesamt_nicht_einspielen[\"isbn_ean\"] = df_gesamt_nicht_einspielen.isbn_ean.astype(str)\n",
    "df_gesamt_nicht_einspielen = df_gesamt_nicht_einspielen.drop(columns=[\"url_ebx\", \"url_ges\", \"cover\", \"title_sep\", \"subtitle_comparison\", \"subtitle_comparison2\", \"subtitle_all\", \"subtitle_sep\"]) # unnötige Spalten entfernen\n",
    "\n",
    "df_neu_nicht_einspielen.to_csv('./output/Vorhandene_Titel_'+date+'.csv')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Updaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update = df_result_in_aleph.loc[(df_result_in_aleph[\"Abfrage_ebx\"]== 'neu') & (df_result_in_aleph[\"Abfrage_ges\"] == 'neu')] # prüfen, ob es wirklich ein Datenupdate gab, sonst nicht neu einspielen??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Für die Logdatei Ermittlung verschiedener Zahlen und hier zur direkten Ansicht ausgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kontrollmechanismus, ob für alle Titel auch Treffer da sind\n",
    "x = df.shape[0]\n",
    "a = df_ohne_dubletten.shape[0]\n",
    "c = df_dubletten.shape[0]\n",
    "b = df_dubl_einspielen.shape[0] #Auswahl der neuen Treffer\n",
    "m = df_in_aleph_nicht_einspielen.shape[0]\n",
    "n = df_ganz_neu.shape[0]\n",
    "h = df_neu_aleph_einspielen.shape[0]\n",
    "j = df_neu_nicht_einspielen.shape[0]\n",
    "o = df_in_aleph_update.shape[0]\n",
    "g = df_in_aleph.shape[0]\n",
    "k = df_alephIDs.shape[0]\n",
    "z = df_neu_aleph_einspielen.shape[0]       #neu ermitteln aus neuen\n",
    "l = df_aleph_loeschen.shape[0]\n",
    "y = df_dubl_nicht_einspielen.shape[0]\n",
    "\n",
    "\n",
    "print('Kleine Statistik:\\n=====================================',\n",
    "    '\\nGelieferte Datensätze:             ', x,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nSätze ohne Dubletten               ', a,\n",
    "    '\\n   Dubletten:              ', c,\n",
    "    '\\n   Auswahl zum Einspielen  ', b,\n",
    "    '\\n   in Aleph löschen        ', y,\n",
    "    '\\nZu prüfende Datensätze:            ', a+b,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nPrüfung Titel in Aleph:            ', g,\n",
    "    '\\n   Davon als Update        ', o,\n",
    "    '\\n   Davon in Aleph löschen  ', m ,    \n",
    "    '\\nPrüfung neue Titel    :            ', n,\n",
    "    '\\n   Davon ganz neu          ', h,\n",
    "    '\\n   Davon bereits Bestand   ', j,      \n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nTitel, die in Aleph verarbeitet werden: ', o+h,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nDatensätze GES02 vor Einspielen:   ', k,\n",
    "    '\\nNicht mehr im Export               -', l,\n",
    "    '\\nTitel in Aleph löschen, da Bestand -', m,\n",
    "    '\\nZu löschen, da Dublette            -', y,\n",
    "    '\\nNeue Titel für Aleph               +', h,\n",
    "    '\\n--------------------------------------------', \n",
    "    '\\nIn Aleph nach Einspielen:          ', k-l-m-y+h,)\n",
    "\n",
    "#hier entsprechende Einträge für die Log-Datei\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:\n",
    "    log.write(\"\\nGelieferte Datensätze:             \" + str(x))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nSätze ohne Dubletten               \" + str(a))\n",
    "    log.write(\"\\n   Dubletten:              \" + str(c))\n",
    "    log.write(\"\\n   Davon in Aleph löeschen \" + str(y))\n",
    "    log.write(\"\\n   Auswahl zum Einspielen  \" + str(b))\n",
    "    log.write(\"\\nZu prüfende Datensätze:            \" + str(a+b))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nPrüfung Titel in Aleph:            \" + str(g))\n",
    "    log.write(\"\\n   Davon als Update        \" + str(o))\n",
    "    log.write(\"\\n   Davon in Aleph löschen  \" + str(m) + \"    \\n\")   \n",
    "    log.write(\"\\nPrüfung neue Titel    :            \" + str(n))\n",
    "    log.write(\"\\n   Davon ganz neu          \" + str(h))\n",
    "    log.write(\"\\n   Davon bereits Bestand   \" + str(j) + \"    \\n\")     \n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nTitel, die in Aleph verarbeitet werden: \" + str(o+h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nDatensätze GES02 vor Einspielen:   \" + str(k))\n",
    "    log.write(\"\\nNicht mehr im Export               -\" + str(l))\n",
    "    log.write(\"\\nZu löschender dubletter Titel      -\" + str(y))\n",
    "    log.write(\"\\nTitel in Aleph löschen, da Bestand -\" + str(m))\n",
    "    log.write(\"\\nNeue Titel für Aleph               +\" + str(h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nIn Aleph nach Einspielen:          \" + str(k-l-m-y+h))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Exportdateien Aufbereiten\n",
    "\n",
    "#### Zielformat für das Einspielen in Aleph:\n",
    "\n",
    "    000000001 LDR   L -----nM2.01200024------h              \n",
    "    000000001 020   L $$a (object_id))\n",
    "    000000001 030   L $$aaz||rrrza||||\n",
    "    000000001 051   L $$am|||||||\n",
    "    000000001 070   L $$aSchweitzer\n",
    "    000000001 077   L $$aMonographie\n",
    "    000000001 078   L $$aSchweitzer\n",
    "    000000001 082   L $$azum Bestellen\n",
    "    000000001 100   L $$a (contributor_1)\n",
    "    000000001 104   L $$a (contributor_2)\n",
    "    000000001 108   L $$a (contributor_3)\n",
    "    000000001 331   L $$a (title_sep)\n",
    "    000000001 335   L $$a (subtitle_all)\n",
    "    000000001 403   L $$a (edition_number / edition_text)  #noch prüfen, was besser zu verwenden ist \n",
    "    000000001 419   L $$b (publisher) $$a (date_combined)\n",
    "    000000001 433   L $$a (pages)\n",
    "    000000001 451   L $$a (series)\n",
    "    000000001 520   L $$a (thesis)\n",
    "    000000001 540   L $$a (isbn_ean)\n",
    "    000000001 656   L $$a (cover)\n",
    "    000000001 750   L $$a (description)\n",
    "    000000001 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch        \n",
    "    \n",
    "Anmerkung zum Feld 655: die URL wird NACH dem Einspielen in Aleph mit der Datensatz-ID angereichert (siehe Juypter-Notebook \"Link-Anreicherung\"), um einen klaren Bestellink für den Kaufvorschlag zu haben\n",
    "\n",
    "*Hierfür werden immer die Feldbenennung und bestimmte Codierungen VOR den Inhalt - in Klammern de Bezeichnung der entsprechenden Spalte - gesetzt, bzw. erfoderliche Felder komplett neu hinzugefügt.   \n",
    "Am Anfang jeder Zeile braucht Aleph eine 9-Stellige eindeutige Zahl pro Titel.*   \n",
    "\n",
    "*Manchmal ließ sich der Inhalt einer Spalte direkt in die Datei schreiben, manchmal musst die Spalte zuvor über apply aufbereitet werden.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aufbereiten der neuen Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen = df_neu_aleph_einspielen           #zur vereinfachten Wiederverwertung des alten Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"020\"] = df_aleph_einspielen[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_aleph_einspielen[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung der Personendaten\n",
    "\n",
    "*Da bis zu 3 Personen in einer Spalte zu finden sind, werden diese im Discovery nicht getrennt suchbar, darum werden sie gesplittet. Für die Dublettenkontrolle hat sich das als irrelevant erwiesen, darum erfolgt dieser Schritt erst hier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = df_aleph_einspielen[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= person[0]\n",
    "df_aleph_einspielen[\"contributor_2\"]= person[1]\n",
    "df_aleph_einspielen[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= df_aleph_einspielen[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_2\"]= df_aleph_einspielen[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_3\"]= df_aleph_einspielen[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung des Erscheinungsdatum und Erscheinungsjahres\n",
    "\n",
    "*In der Auswahl unserer Titel befinden sich auch im Erscheinen befindliche Titel der kommenden Monate. Diese Information möchten wir gerne im Discovery sichtbar machen. Hierfür bleibt uns nur Aleph-Feld 419c, das dem Erscheinungsjahr vorbehalten ist.   \n",
    "Wunsch ist es: Wenn des Erscheinungsdatum weiter als 10 Tage weg vom heutigen Datum ist, soll das komplette Datum angezeigt werden, ansonsten nur das Erscheinungsjahr.*\n",
    "\n",
    "Zur Umsetzung muss die Spalte \"publication_date\" in ein Datum verwandelt werden und nach den genannten Kriterien unterschiedlich angezeigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(time.strftime('%Y%m%d'))\n",
    "df_aleph_einspielen[\"coming_soon\"] = np.where(df_aleph_einspielen[\"publication_date\"].astype(int) > today+10, df_aleph_einspielen[\"publication_date\"], np.nan) #zieht die über 10 Tage raus, brauchen nan für Umwandlung in Datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"publication_date_soon\"] = df_aleph_einspielen[\"coming_soon\"].astype(str).str.replace('00','01')\n",
    "#df_aleph_einspielen[\"publication_date_soon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"year\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=0,stop=4)    #einfaches Zerlegen in die Datumsbestandteile und anschließendes Zusammenfügen\n",
    "df_aleph_einspielen[\"month\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=4,stop=6)\n",
    "df_aleph_einspielen[\"day\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=6,stop=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"full_coming_soon\"] = df_aleph_einspielen[\"year\"]+'-'+df_aleph_einspielen[\"month\"]+'-'+df_aleph_einspielen[\"day\"]\n",
    "df_aleph_einspielen[\"coming_soon\"] = df_aleph_einspielen[\"full_coming_soon\"].astype(str).str.replace('nan--','') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"coming_soon\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"published\"] = np.where(df_aleph_einspielen[\"coming_soon\"] == '', df_aleph_einspielen[\"publication_year\"], '') #Auslesen und Kombinieren der Daten\n",
    "df_aleph_einspielen[\"year_publ\"] = df_aleph_einspielen[\"published\"].astype(str).str.slice(start=0,stop=4)                          #da wieder .0 am Ende war, Jahreszahl ausschneidens\n",
    "df_aleph_einspielen[\"date_combined\"] = df_aleph_einspielen[\"year_publ\"]+df_aleph_einspielen[\"coming_soon\"]\n",
    "df_aleph_einspielen['date_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_aleph_einspielen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen['isbn_ean'] = df_aleph_einspielen['isbn_ean'].astype(np.int64)   #da die Zahl als Object genommen wurde, bekam sie ein .0 angehängt, das ist durch umwandeln in Zahl weg\n",
    "\n",
    "df_aleph_einspielen['isbn_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"419b\"] = df_aleph_einspielen[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_aleph_einspielen[\"419c\"] = df_aleph_einspielen[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_aleph_einspielen[\"419\"] = df_aleph_einspielen[\"419b\"]+df_aleph_einspielen[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"403\"] = df_aleph_einspielen[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_aleph_einspielen[\"433\"] = df_aleph_einspielen[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_aleph_einspielen[\"451\"] = df_aleph_einspielen[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_aleph_einspielen[\"520\"] = df_aleph_einspielen[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_aleph_einspielen[\"540\"] = df_aleph_einspielen[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_aleph_einspielen[\"656\"] = df_aleph_einspielen[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_aleph_einspielen[\"750\"] = df_aleph_einspielen[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Für das Durchzählen der Titel braucht es eine neue Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier entsteht eine neue Spalte mit Zahlen ab 1 durchgehend gezählt, die für den korrekten Import der Daten in Aleph nötig ist\n",
    "x = df_aleph_einspielen.shape[0]   \n",
    "df_aleph_einspielen[\"id\"] = range(1,x+1)                                                       #Notwendig ist die Zählung ab 1, da Aleph sonst nicht korrekt einließt\n",
    "df_aleph_einspielen[\"id\"] = df_aleph_einspielen[\"id\"].apply(lambda x: f\"{x:09d}\")              #Die Zahl muss 9-Stellig aufgefüllt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Vorbereitungen abgeschlossen, jetzt das Schreiben der Datei im Aleph-Sequential-Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./output/ges02_neu\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_einspielen.index:\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' LDR   L -----nM2.01200024------h'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"020\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 030   L $$aaz||rrrza||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 051   L $$am|||||||m|||||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 077   L $$aMonographie'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 078   L $$aSchweitzer'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 082   L $$azum Bestellen'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 100   L $$a'+df_aleph_einspielen[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 104   L $$a'+df_aleph_einspielen[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 108   L $$a'+df_aleph_einspielen[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 331   L $$a'+df_aleph_einspielen[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 335   L $$a'+df_aleph_einspielen[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"403\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"419\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"433\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"451\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"520\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"540\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"750\"][i]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Daten für Update\n",
    "\n",
    "*es wird anhand der Spalte \"last_modified\" geprüft, ob die Titel seit dem letzten Einspielen ein Update erfahren haben*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Um die Menge zum Updaten zu reduzieren, werden nur die rausgezogen, die tatsächlich ein Aktualisierungsdatum haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tday = datetime.datetime.now()\n",
    "td = datetime.timedelta(days = 7)\n",
    "u = tday - td\n",
    "df_in_aleph_update['timespan'] = u                                                           # Einfügen einer Spalte mit Datum vor 10 Tagen als Basis für Abfrage zu Update-Notwendigkeit\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update['last_modified'] = pd.to_datetime(df_in_aleph_update.last_modified)          #Umwandlung, da Spalteninhalt object ist\n",
    "df_in_aleph_update['last_modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update['true'] = np.where(df_in_aleph_update['last_modified'] > u, df_in_aleph_update['last_modified'], np.datetime64('NaT'))\n",
    "df_in_aleph_update['true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true = df_in_aleph_update[pd.notnull(df_in_aleph_update['true'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen der Update-Export-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zwischenschritt, um für alle in Aleph das Update zu starten\n",
    "#df_in_aleph_update_true = df_in_aleph_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"020\"] = df_in_aleph_update_true[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_in_aleph_update_true[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufbereitung Personendaten\n",
    "\n",
    "person = df_in_aleph_update_true[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= person[0]\n",
    "df_in_aleph_update_true[\"contributor_2\"]= person[1]\n",
    "df_in_aleph_update_true[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= df_in_aleph_update_true[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_2\"]= df_in_aleph_update_true[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_3\"]= df_in_aleph_update_true[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neue Schritte zur Aufbereitung des Erscheinungsdatums, da der Code von oben hier nicht ging - warum auch immer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#today = int(time.strftime('%Y%m%d'))\n",
    "#soon = today+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl'] = df_in_aleph_update_true['publication_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl'] = df_in_aleph_update_true['publ_date_repl'].str.replace('00','01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['publ_date_repl_date'] = pd.to_datetime(df_in_aleph_update_true['publ_date_repl'], errors='coerce')\n",
    "df_in_aleph_update_true['publ_date_repl_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"coming_soon\"] = np.where(df_in_aleph_update_true[\"publ_date_repl_date\"] > u, df_in_aleph_update_true[\"publ_date_repl_date\"], np.datetime64('NaT'))  #df_in_aleph_update_true['today'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"coming_soon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_in_aleph_update_true[\"coming_soon\"].replace('NaT','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"date_combined\"] = np.where(df_in_aleph_update_true[\"coming_soon\"].astype(str) == 'NaT', df_in_aleph_update_true[\"publication_year\"], df_in_aleph_update_true[\"coming_soon\"].astype(str))\n",
    "df_in_aleph_update_true[\"date_combined\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true['isbn_ean'] = df_in_aleph_update_true['isbn_ean'].astype(np.int64)   #da die Zahl als Object genommen wurde, bekam sie ein .0 angehängt, das ist durch umwandeln in Zahl weg\n",
    "\n",
    "df_in_aleph_update_true['isbn_ean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearbeitung der Felder\n",
    "\n",
    "df_in_aleph_update_true[\"419b\"] = df_in_aleph_update_true[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"419c\"] = df_in_aleph_update_true[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_in_aleph_update_true[\"419\"] = df_in_aleph_update_true[\"419b\"]+df_in_aleph_update_true[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte\n",
    "\n",
    "df_in_aleph_update_true[\"403\"] = df_in_aleph_update_true[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"433\"] = df_in_aleph_update_true[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_in_aleph_update_true[\"451\"] = df_in_aleph_update_true[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"520\"] = df_in_aleph_update_true[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"540\"] = df_in_aleph_update_true[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"656\"] = df_in_aleph_update_true[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_in_aleph_update_true[\"750\"] = df_in_aleph_update_true[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Ausgabedatei, hier kleine Unterschiede zu den neuen Titeln. Vorhandene ids und bestimmte Felder können nicht verändert sein, brauchen also nicht übernommen zu werden.\n",
    "\n",
    "with open(\"./output/ges02_update\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_update_true.index:\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 100   L $$a'+df_in_aleph_update_true[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 104   L $$a'+df_in_aleph_update_true[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 108   L $$a'+df_in_aleph_update_true[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 331   L $$a'+df_in_aleph_update_true[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 335   L $$a'+df_in_aleph_update_true[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"403\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"419\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"433\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"451\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"520\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"540\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"750\"][i]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschließende Dinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abschließender Eintrag in Log-Datei\n",
    "endtime = time.strftime('%H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('\\n                                     beendet ')\n",
    "    log.write(endtime)\n",
    "    log.write(\"\\n============================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopien bestimmter Daten zur Einsicht bzw. für Prüfzwecke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen.to_csv('./output/Eingespielte_Titel_'+date+'.csv') \n",
    "df_in_aleph_update_true.to_csv('./output/Update_Titel_'+date+'.csv')\n",
    "\n",
    "df_in_aleph_nicht_einspielen.to_csv('./output/Aleph_loeschen_ebx_vorh_'+date+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Folgende Schritte müssen mit den Dateien ausgeführt werden: \n",
    "\n",
    "\n",
    "1. Einspielen der Datei pda_ges01 als neue Titel in Aleph, hier dann auch Export der urls und Anreicherung mit der Aleph-ID mittels \"mailto_link_skript.ipynb\"\n",
    "2. Einspielen der DAtei pda_update als \"Änderungen bestehender Datensätze in Aleph\"\n",
    "3. Einspielen und löschen der Titel die in ges02_loeschen_1 und ges02_loeschen_2 vorhanden sind"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1999307f9afbc7e27238c8b437c01929975b7965fdd2ec967692fa451253dbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "4fac973d8b48a5fcf37e7d133428a31fb47ebbd054f5d1feed8c0da486f2af46"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
