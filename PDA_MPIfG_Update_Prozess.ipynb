{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buchhandelsdaten in Vufind als Grundlage für PDA (Patron Driven Aquisition) am MPIfG / Version 2: Updates laden statt Vollimport\n",
    "\n",
    "Einbindung von freundlicherweise von Schweitzer Fachinformation zur Verfügung gestellten Daten.   \n",
    "(Auswahl über passend konfigurierte Neuerscheinungsabfragen in unserem Kundenprofil).   \n",
    "\n",
    "#### Gründe für die Umstellung auf Updates: Durch Einspielen der Daten in Vufind ergibt sich eine zeitliche Diskrepanz, in der Titel bereits in Aleph gelöscht sind, aber in Vufind noch bestellbar. Zudem werden die Aleph-IDs hochgezähl, denn bei jedem Einspielen werden neue Nummern vergeben. Durch die Updates bleiben die Titel weiterhin verfügbar und das Hochzählen der IDs hält sich im Rahmen. Das Skript beschleunigt sich durch Reduzierung der Abfragen am Server.\n",
    "*Kleiner Nachteil: der manuelle Aufwand vergrößert sich. Mehrere Dateien müssen in Aleph eingespielt und verarbeitet werden, aber hält sich nach wie vor im vertretbaren Rahmen.*\n",
    "\n",
    "Das Jupyter Notebook arbeitet mit Python 3.9.1 und wurde mit Visual Studio Code 1.62.3 erstellt \n",
    "\n",
    "\n",
    "#### Arbeitsschritte im Code:\n",
    "\n",
    "> Vorarbeiten:   \n",
    "  - Notwendige Pandas Libraries aufrufen\n",
    "  - Serverprüfung auf funktionierende Verbindung zum Aleph-X-Server    \n",
    "\n",
    "\n",
    "> Daten abholen und einlesen:   \n",
    "  1. Buchhandelsdaten von Schweitzer \n",
    "  2. Aleph-Konkordanz Aleph-ID /Schweitzer ID  (erzeugt tagesaktuell per p-print-03 in Aleph)\n",
    "     - Aufbereiten der Daten: Schweitzer ID extrahieren und Aleph-ID mit Nullen auffüllen\n",
    "  3. Daten zusammenführen in einem df    \n",
    "\n",
    "\n",
    "> Daten aufbereiten:   \n",
    "  1. Buchhandelsdaten prüfen und vorbereiten\n",
    "     1. Identifizierung von Titeln in Aleph, die nicht mehr im Datensatz sind und schreiben in Datei \"ges02_loeschen_1\"\n",
    "     2. Dublettencheck innerhalb der Buchhandelsdaten\n",
    "     3. Trennung der Daten in \"in Aleph vorhanden\" und \"neu\"  \n",
    "  2. Bestandsabfragen:\n",
    "     1. Ganz neue Titel   \n",
    "        - Bestandsabgleich durch Abfragen (GES und EBX) auf dem Aleph-Server   \n",
    "     2. In Aleph vorhanden\n",
    "        - Bestandsabgleich durch Abfrage EBX auf dem Aleph-Server  \n",
    "     3. Exportvorbereitungen:\n",
    "        1. Neue Titel\n",
    "        2. In Aleph vorhandene Titel   \n",
    "            Identifizierung von zu löschenden Titeln \"ges02_loeschen_2\" und der zu aktualisierenden\n",
    "  3. Exportdateien aufbereiten:\n",
    "     1. Aufbereiten neuer Titel\n",
    "     2. Aufbereiten der vorhandenen Titel   \n",
    "\n",
    "\n",
    "> Informationssammlung\n",
    "   1. Log-Datei mit Rahmendaten wird fortlaufend geschrieben\n",
    "   2. Ausgabe bestimmter Titelgruppen als csv-Datei  \n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorarbeiten\n",
    "\n",
    "### Pandas Libraries laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                                # für das Arbeiten mit der CSV-Datei\n",
    "import urllib.request                                              # für das Abrufen der URL\n",
    "import requests                                                    #für die Bestandsabfragen \n",
    "pd.options.mode.chained_assignment = None                          # default='warn' abschalten beim Beschreiben der neuen Spalten\n",
    "import time                                                        # für das Schreiben des Datums Logdatei und Excel-Export und Arbeiten mit dem Erscheinungsdatum\n",
    "import datetime                                                    # für das Berechnen des Updates\n",
    "import numpy as np                                                 # für das Bearbeiten von Spalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prüfung, ob die Verbindung zum Aleph-Server für Abfragen korrekt funktioniert:\n",
    "\n",
    "    Nur zugelassene IPs können diese Schnittstelle abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Server antwortet korrekt\n"
     ]
    }
   ],
   "source": [
    "test= \"http://aleph.mpg.de/X?op=find&base=ges01&request=IBS=9783482648434\"\n",
    "\n",
    "reply = requests.get(test).text\n",
    "a = reply.find(\"Forbidden\")\n",
    "b =  reply.find(\"?xml\")\n",
    "\n",
    "if (a > 50):\n",
    "    print(\"Es gibt ein Problem mit dem Server\")\n",
    "if (b == 1):\n",
    "    print(\"Der Server antwortet korrekt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datensätze abholen und einlesen\n",
    "\n",
    "### 1. Datensätze von Schweitzer\n",
    "\n",
    "*einlesen in df*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./input/export.csv', <http.client.HTTPMessage at 0x1befed343d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://content.schweitzer-online.de/static/content/export/mpifg/export.csv\"  # Abruf, der von Schweitzer zur Verfügung gestellten Daten\n",
    "checkout_file = \"./input/export.csv\"  \n",
    "urllib.request.urlretrieve(url, checkout_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./input/export.csv', encoding = 'UTF-8', sep=';' , keep_default_na=False) # muss encoding angeben und Trennzeichen, NaN (= leere Werte) direkt beim Import entfernen, da sie später Probleme machen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>edition_number</th>\n",
       "      <th>edition_text</th>\n",
       "      <th>...</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>delivery_signal</th>\n",
       "      <th>delivery_text</th>\n",
       "      <th>price</th>\n",
       "      <th>description</th>\n",
       "      <th>cover</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>follower</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160009340</td>\n",
       "      <td>9783593448084</td>\n",
       "      <td>Kalkulierte Mobilität</td>\n",
       "      <td>Ökonomische und biographische Perspektiven auf...</td>\n",
       "      <td>Judith Schmidt</td>\n",
       "      <td>Campus</td>\n",
       "      <td>Arbeit und Alltag 20</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>20210721</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_status_econtent:available</td>\n",
       "      <td>3699</td>\n",
       "      <td>Saisonarbeit ist eine bedeutende Variable der ...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-18 18:24:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159663468</td>\n",
       "      <td>9783161564154</td>\n",
       "      <td>Die Tradition staatlicher Interventionen in de...</td>\n",
       "      <td></td>\n",
       "      <td>Valesca Maria Molinari</td>\n",
       "      <td>Mohr Siebeck</td>\n",
       "      <td>Rechtsordnung und Wirtschaftsgeschichte</td>\n",
       "      <td>Dissertationsschrift</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>20211100</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_status_econtent:available</td>\n",
       "      <td>5900</td>\n",
       "      <td>Die Entwicklungen auf dem Mietwohnungsmarkt de...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-12 18:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158945668</td>\n",
       "      <td>9780231550734</td>\n",
       "      <td>Experiencing Design</td>\n",
       "      <td>The Innovator's Journey</td>\n",
       "      <td>Jeanne Liedtka;Karen Hold;Jessica Eldridge</td>\n",
       "      <td>Columbia University Press</td>\n",
       "      <td>Columbia Business School Publishing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>20210908</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_status_econtent:available</td>\n",
       "      <td>2799</td>\n",
       "      <td>Individuals become design thinkers by experien...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-24 17:20:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161161334</td>\n",
       "      <td>9783161608469</td>\n",
       "      <td>Im Olymp der Ökonomen</td>\n",
       "      <td>Zur öffentlichen Resonanz wirtschaftspolitisch...</td>\n",
       "      <td>Lino  Wehrheim</td>\n",
       "      <td>Mohr Siebeck</td>\n",
       "      <td>Die Einheit der Gesellschaftswissenschaften im...</td>\n",
       "      <td>Dissertationsschrift</td>\n",
       "      <td>1</td>\n",
       "      <td>1. Auflage 2021</td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>20210916</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_status_econtent:available</td>\n",
       "      <td>9900</td>\n",
       "      <td>In Zeiten von Fake News und alternativen Wahrh...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-28 20:56:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161502056</td>\n",
       "      <td>9783496030478</td>\n",
       "      <td>Die Südseeblase in der englischen Kunst des 18...</td>\n",
       "      <td>Bilder einer Finanzkrise</td>\n",
       "      <td>Kathleen Löwe</td>\n",
       "      <td>Reimer, Dietrich</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>20210709</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_status_econtent:available</td>\n",
       "      <td>4900</td>\n",
       "      <td>Als es 1720 in England zu einem Börsenkrach, d...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-19 18:27:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>161277585</td>\n",
       "      <td>9781800713031</td>\n",
       "      <td>The Creative Industries and International Busi...</td>\n",
       "      <td></td>\n",
       "      <td>Nnamdi O. Madichie;Robert Ebo Hinson</td>\n",
       "      <td>Emerald Publishing Limited</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>20220121</td>\n",
       "      <td>yellow</td>\n",
       "      <td>delivery_status:10</td>\n",
       "      <td>8287</td>\n",
       "      <td>The international business environment has und...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-19 18:09:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7046</th>\n",
       "      <td>161277677</td>\n",
       "      <td>9781801171809</td>\n",
       "      <td>Unravelling Travelling</td>\n",
       "      <td>Uncovering Tourist Emotions through Autoethnog...</td>\n",
       "      <td>Sue Beeton</td>\n",
       "      <td>Emerald Publishing Limited</td>\n",
       "      <td>The Tourist Experience</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>20220120</td>\n",
       "      <td>yellow</td>\n",
       "      <td>delivery_status:10</td>\n",
       "      <td>8924</td>\n",
       "      <td>The Tourist Experience is complex, intrinsical...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-19 18:10:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7047</th>\n",
       "      <td>162078229</td>\n",
       "      <td>9780745340173</td>\n",
       "      <td>Work and the Carceral State</td>\n",
       "      <td></td>\n",
       "      <td>Jon Burnett</td>\n",
       "      <td>Pluto Press</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2022</td>\n",
       "      <td>20220120</td>\n",
       "      <td>yellow</td>\n",
       "      <td>delivery_status:10</td>\n",
       "      <td>9484</td>\n",
       "      <td>'Revolutionises our understanding of the carce...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-29 17:22:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>162745003</td>\n",
       "      <td>9781119745976</td>\n",
       "      <td>The Strategyzer Box Set</td>\n",
       "      <td></td>\n",
       "      <td>Alexander Osterwalder</td>\n",
       "      <td>Wiley</td>\n",
       "      <td>The Strategyzer series</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2020</td>\n",
       "      <td>20201203</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_period:available_15to20days</td>\n",
       "      <td>17078</td>\n",
       "      <td>The Authoritative Library for Business Leaders...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-29 17:35:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7049</th>\n",
       "      <td>163261204</td>\n",
       "      <td>9782343236698</td>\n",
       "      <td>Travail, valeur et prix</td>\n",
       "      <td>Reprise et clôture d'un débat centenaire (1885...</td>\n",
       "      <td>Adolfo Rodriguez-Herrera</td>\n",
       "      <td>Editions L'Harmattan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2021</td>\n",
       "      <td>20211123</td>\n",
       "      <td>green</td>\n",
       "      <td>delivery_period:available_3to4weeks</td>\n",
       "      <td>3600</td>\n",
       "      <td>La transformation des valeurs en prix est, sel...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-28 18:15:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7050 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      object_id       isbn_ean  \\\n",
       "0     160009340  9783593448084   \n",
       "1     159663468  9783161564154   \n",
       "2     158945668  9780231550734   \n",
       "3     161161334  9783161608469   \n",
       "4     161502056  9783496030478   \n",
       "...         ...            ...   \n",
       "7045  161277585  9781800713031   \n",
       "7046  161277677  9781801171809   \n",
       "7047  162078229  9780745340173   \n",
       "7048  162745003  9781119745976   \n",
       "7049  163261204  9782343236698   \n",
       "\n",
       "                                                  title  \\\n",
       "0                                 Kalkulierte Mobilität   \n",
       "1     Die Tradition staatlicher Interventionen in de...   \n",
       "2                                   Experiencing Design   \n",
       "3                                 Im Olymp der Ökonomen   \n",
       "4     Die Südseeblase in der englischen Kunst des 18...   \n",
       "...                                                 ...   \n",
       "7045  The Creative Industries and International Busi...   \n",
       "7046                             Unravelling Travelling   \n",
       "7047                        Work and the Carceral State   \n",
       "7048                            The Strategyzer Box Set   \n",
       "7049                            Travail, valeur et prix   \n",
       "\n",
       "                                               subtitle  \\\n",
       "0     Ökonomische und biographische Perspektiven auf...   \n",
       "1                                                         \n",
       "2                               The Innovator's Journey   \n",
       "3     Zur öffentlichen Resonanz wirtschaftspolitisch...   \n",
       "4                              Bilder einer Finanzkrise   \n",
       "...                                                 ...   \n",
       "7045                                                      \n",
       "7046  Uncovering Tourist Emotions through Autoethnog...   \n",
       "7047                                                      \n",
       "7048                                                      \n",
       "7049  Reprise et clôture d'un débat centenaire (1885...   \n",
       "\n",
       "                                     contributor                   publisher  \\\n",
       "0                                 Judith Schmidt                      Campus   \n",
       "1                         Valesca Maria Molinari                Mohr Siebeck   \n",
       "2     Jeanne Liedtka;Karen Hold;Jessica Eldridge   Columbia University Press   \n",
       "3                                 Lino  Wehrheim                Mohr Siebeck   \n",
       "4                                  Kathleen Löwe            Reimer, Dietrich   \n",
       "...                                          ...                         ...   \n",
       "7045        Nnamdi O. Madichie;Robert Ebo Hinson  Emerald Publishing Limited   \n",
       "7046                                  Sue Beeton  Emerald Publishing Limited   \n",
       "7047                                 Jon Burnett                 Pluto Press   \n",
       "7048                       Alexander Osterwalder                       Wiley   \n",
       "7049                    Adolfo Rodriguez-Herrera        Editions L'Harmattan   \n",
       "\n",
       "                                                 series                thesis  \\\n",
       "0                                  Arbeit und Alltag 20                         \n",
       "1               Rechtsordnung und Wirtschaftsgeschichte  Dissertationsschrift   \n",
       "2                   Columbia Business School Publishing                         \n",
       "3     Die Einheit der Gesellschaftswissenschaften im...  Dissertationsschrift   \n",
       "4                                                                               \n",
       "...                                                 ...                   ...   \n",
       "7045                                                                            \n",
       "7046                             The Tourist Experience                         \n",
       "7047                                                                            \n",
       "7048                             The Strategyzer series                         \n",
       "7049                                                                            \n",
       "\n",
       "     edition_number     edition_text  ... publication_year publication_date  \\\n",
       "0                 1                   ...             2021         20210721   \n",
       "1                 1                   ...             2021         20211100   \n",
       "2                 1                   ...             2021         20210908   \n",
       "3                 1  1. Auflage 2021  ...             2021         20210916   \n",
       "4                 1                   ...             2021         20210709   \n",
       "...             ...              ...  ...              ...              ...   \n",
       "7045                                  ...             2022         20220121   \n",
       "7046                                  ...             2022         20220120   \n",
       "7047                                  ...             2022         20220120   \n",
       "7048                                  ...             2020         20201203   \n",
       "7049                                  ...             2021         20211123   \n",
       "\n",
       "     delivery_signal                         delivery_text  price  \\\n",
       "0              green    delivery_status_econtent:available   3699   \n",
       "1              green    delivery_status_econtent:available   5900   \n",
       "2              green    delivery_status_econtent:available   2799   \n",
       "3              green    delivery_status_econtent:available   9900   \n",
       "4              green    delivery_status_econtent:available   4900   \n",
       "...              ...                                   ...    ...   \n",
       "7045          yellow                    delivery_status:10   8287   \n",
       "7046          yellow                    delivery_status:10   8924   \n",
       "7047          yellow                    delivery_status:10   9484   \n",
       "7048           green  delivery_period:available_15to20days  17078   \n",
       "7049           green   delivery_period:available_3to4weeks   3600   \n",
       "\n",
       "                                            description  \\\n",
       "0     Saisonarbeit ist eine bedeutende Variable der ...   \n",
       "1     Die Entwicklungen auf dem Mietwohnungsmarkt de...   \n",
       "2     Individuals become design thinkers by experien...   \n",
       "3     In Zeiten von Fake News und alternativen Wahrh...   \n",
       "4     Als es 1720 in England zu einem Börsenkrach, d...   \n",
       "...                                                 ...   \n",
       "7045  The international business environment has und...   \n",
       "7046  The Tourist Experience is complex, intrinsical...   \n",
       "7047  'Revolutionises our understanding of the carce...   \n",
       "7048  The Authoritative Library for Business Leaders...   \n",
       "7049  La transformation des valeurs en prix est, sel...   \n",
       "\n",
       "                                                  cover predecessor follower  \\\n",
       "0     https://content.schweitzer-online.de/static/ca...                        \n",
       "1     https://content.schweitzer-online.de/static/ca...                        \n",
       "2     https://content.schweitzer-online.de/static/ca...                        \n",
       "3     https://content.schweitzer-online.de/static/ca...                        \n",
       "4     https://content.schweitzer-online.de/static/ca...                        \n",
       "...                                                 ...         ...      ...   \n",
       "7045  https://content.schweitzer-online.de/static/ca...                        \n",
       "7046  https://content.schweitzer-online.de/static/ca...                        \n",
       "7047                                                                           \n",
       "7048  https://content.schweitzer-online.de/static/ca...                        \n",
       "7049  https://content.schweitzer-online.de/static/ca...                        \n",
       "\n",
       "            last_modified  \n",
       "0     2021-11-18 18:24:57  \n",
       "1     2021-11-12 18:15:00  \n",
       "2     2021-11-24 17:20:02  \n",
       "3     2021-11-28 20:56:06  \n",
       "4     2021-11-19 18:27:38  \n",
       "...                   ...  \n",
       "7045  2021-11-19 18:09:58  \n",
       "7046  2021-11-19 18:10:25  \n",
       "7047  2021-11-29 17:22:03  \n",
       "7048  2021-11-29 17:35:23  \n",
       "7049  2021-11-28 18:15:25  \n",
       "\n",
       "[7050 rows x 22 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aleph-Konkordanz Aleph-ID / Schweitzer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime('%Y%m%d')  # %H:%M:%S\n",
    "input_file = './input/ids'+now      #input_file wird tagesaktuell aus Aleph gezogen und auf diesem Wege mit der entsprechenden Endung eingelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62432</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a160009340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62433</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a158945668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62434</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a161502056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62435</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a159450684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62436</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a156899389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>68627</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a161219987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>68628</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a160093944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6197</th>\n",
       "      <td>68629</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a2543785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>68630</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a159457857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>68631</td>\n",
       "      <td>20</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a161718808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Field  L       Content\n",
       "0     62432     20  L  $$a160009340\n",
       "1     62433     20  L  $$a158945668\n",
       "2     62434     20  L  $$a161502056\n",
       "3     62435     20  L  $$a159450684\n",
       "4     62436     20  L  $$a156899389\n",
       "...     ...    ... ..           ...\n",
       "6195  68627     20  L  $$a161219987\n",
       "6196  68628     20  L  $$a160093944\n",
       "6197  68629     20  L    $$a2543785\n",
       "6198  68630     20  L  $$a159457857\n",
       "6199  68631     20  L  $$a161718808\n",
       "\n",
       "[6200 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_alephIDs = pd.read_fwf(input_file, encoding = 'UTF-8', sep='\\s\\s', header=None, keep_default_na=False) \n",
    "df_alephIDs.columns=[\"ID\",\"Field\",\"L\",\"Content\"]\n",
    "df_alephIDs\n",
    "\n",
    "#Spalte 0 = Aleph-IDs müsste vor Export mit Nullen aufgefüllt werden \n",
    "#Spalte 3 = object_id aus df, hier muss zum Abgleich $$a entfernt werden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Datenfelder aufbereiten: Schweitzer-ID extrahieren und Aleph-ID ins richtige Format bringen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       160009340\n",
       "1       158945668\n",
       "2       161502056\n",
       "3       159450684\n",
       "4       156899389\n",
       "          ...    \n",
       "6195    161219987\n",
       "6196    160093944\n",
       "6197      2543785\n",
       "6198    159457857\n",
       "6199    161718808\n",
       "Name: object_id, Length: 6200, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_alephIDs[\"object_id\"] = df_alephIDs[\"Content\"].astype(str).str.slice(start=3,stop=13).apply(int)   #Spalte mit object-Ids herausschneiden und wieder zur Zahl definieren\n",
    "\n",
    "df_alephIDs[\"object_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       000062432\n",
       "1       000062433\n",
       "2       000062434\n",
       "3       000062435\n",
       "4       000062436\n",
       "          ...    \n",
       "6195    000068627\n",
       "6196    000068628\n",
       "6197    000068629\n",
       "6198    000068630\n",
       "6199    000068631\n",
       "Name: ids, Length: 6200, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aleph-ID-Spalte mit Nullen Auffüllen\n",
    "df_alephIDs[\"ids\"] = df_alephIDs['ID'].apply(lambda x: f\"{x:09d}\")\n",
    "df_alephIDs[\"ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Daten zu einem Frame zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oi= df.set_index(\"object_id\")                       #object-ID zum Index für beide Datenframes\n",
    "df_aleph_oi = df_alephIDs.set_index(\"object_id\")\n",
    "df_update = df_oi.join(df_aleph_oi)                    #dann können sie zusammengeführt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn_ean</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contributor</th>\n",
       "      <th>publisher</th>\n",
       "      <th>series</th>\n",
       "      <th>thesis</th>\n",
       "      <th>edition_number</th>\n",
       "      <th>edition_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>...</th>\n",
       "      <th>description</th>\n",
       "      <th>cover</th>\n",
       "      <th>predecessor</th>\n",
       "      <th>follower</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>ID</th>\n",
       "      <th>Field</th>\n",
       "      <th>L</th>\n",
       "      <th>Content</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160009340</th>\n",
       "      <td>9783593448084</td>\n",
       "      <td>Kalkulierte Mobilität</td>\n",
       "      <td>Ökonomische und biographische Perspektiven auf...</td>\n",
       "      <td>Judith Schmidt</td>\n",
       "      <td>Campus</td>\n",
       "      <td>Arbeit und Alltag 20</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>ebook</td>\n",
       "      <td>...</td>\n",
       "      <td>Saisonarbeit ist eine bedeutende Variable der ...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-18 18:24:57</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a160009340</td>\n",
       "      <td>000062432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159663468</th>\n",
       "      <td>9783161564154</td>\n",
       "      <td>Die Tradition staatlicher Interventionen in de...</td>\n",
       "      <td></td>\n",
       "      <td>Valesca Maria Molinari</td>\n",
       "      <td>Mohr Siebeck</td>\n",
       "      <td>Rechtsordnung und Wirtschaftsgeschichte</td>\n",
       "      <td>Dissertationsschrift</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>ebook</td>\n",
       "      <td>...</td>\n",
       "      <td>Die Entwicklungen auf dem Mietwohnungsmarkt de...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-12 18:15:00</td>\n",
       "      <td>68009.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a159663468</td>\n",
       "      <td>000068009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158945668</th>\n",
       "      <td>9780231550734</td>\n",
       "      <td>Experiencing Design</td>\n",
       "      <td>The Innovator's Journey</td>\n",
       "      <td>Jeanne Liedtka;Karen Hold;Jessica Eldridge</td>\n",
       "      <td>Columbia University Press</td>\n",
       "      <td>Columbia Business School Publishing</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>ebook</td>\n",
       "      <td>...</td>\n",
       "      <td>Individuals become design thinkers by experien...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-24 17:20:02</td>\n",
       "      <td>62433.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a158945668</td>\n",
       "      <td>000062433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161161334</th>\n",
       "      <td>9783161608469</td>\n",
       "      <td>Im Olymp der Ökonomen</td>\n",
       "      <td>Zur öffentlichen Resonanz wirtschaftspolitisch...</td>\n",
       "      <td>Lino  Wehrheim</td>\n",
       "      <td>Mohr Siebeck</td>\n",
       "      <td>Die Einheit der Gesellschaftswissenschaften im...</td>\n",
       "      <td>Dissertationsschrift</td>\n",
       "      <td>1</td>\n",
       "      <td>1. Auflage 2021</td>\n",
       "      <td>ebook</td>\n",
       "      <td>...</td>\n",
       "      <td>In Zeiten von Fake News und alternativen Wahrh...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-28 20:56:06</td>\n",
       "      <td>67902.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a161161334</td>\n",
       "      <td>000067902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161502056</th>\n",
       "      <td>9783496030478</td>\n",
       "      <td>Die Südseeblase in der englischen Kunst des 18...</td>\n",
       "      <td>Bilder einer Finanzkrise</td>\n",
       "      <td>Kathleen Löwe</td>\n",
       "      <td>Reimer, Dietrich</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>ebook</td>\n",
       "      <td>...</td>\n",
       "      <td>Als es 1720 in England zu einem Börsenkrach, d...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-19 18:27:38</td>\n",
       "      <td>62434.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>L</td>\n",
       "      <td>$$a161502056</td>\n",
       "      <td>000062434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161277585</th>\n",
       "      <td>9781800713031</td>\n",
       "      <td>The Creative Industries and International Busi...</td>\n",
       "      <td></td>\n",
       "      <td>Nnamdi O. Madichie;Robert Ebo Hinson</td>\n",
       "      <td>Emerald Publishing Limited</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>The international business environment has und...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-19 18:09:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161277677</th>\n",
       "      <td>9781801171809</td>\n",
       "      <td>Unravelling Travelling</td>\n",
       "      <td>Uncovering Tourist Emotions through Autoethnog...</td>\n",
       "      <td>Sue Beeton</td>\n",
       "      <td>Emerald Publishing Limited</td>\n",
       "      <td>The Tourist Experience</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>The Tourist Experience is complex, intrinsical...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-19 18:10:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162078229</th>\n",
       "      <td>9780745340173</td>\n",
       "      <td>Work and the Carceral State</td>\n",
       "      <td></td>\n",
       "      <td>Jon Burnett</td>\n",
       "      <td>Pluto Press</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>'Revolutionises our understanding of the carce...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-29 17:22:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162745003</th>\n",
       "      <td>9781119745976</td>\n",
       "      <td>The Strategyzer Box Set</td>\n",
       "      <td></td>\n",
       "      <td>Alexander Osterwalder</td>\n",
       "      <td>Wiley</td>\n",
       "      <td>The Strategyzer series</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>The Authoritative Library for Business Leaders...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-29 17:35:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163261204</th>\n",
       "      <td>9782343236698</td>\n",
       "      <td>Travail, valeur et prix</td>\n",
       "      <td>Reprise et clôture d'un débat centenaire (1885...</td>\n",
       "      <td>Adolfo Rodriguez-Herrera</td>\n",
       "      <td>Editions L'Harmattan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>hardcover</td>\n",
       "      <td>...</td>\n",
       "      <td>La transformation des valeurs en prix est, sel...</td>\n",
       "      <td>https://content.schweitzer-online.de/static/ca...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2021-11-28 18:15:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7050 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                isbn_ean                                              title  \\\n",
       "object_id                                                                     \n",
       "160009340  9783593448084                              Kalkulierte Mobilität   \n",
       "159663468  9783161564154  Die Tradition staatlicher Interventionen in de...   \n",
       "158945668  9780231550734                                Experiencing Design   \n",
       "161161334  9783161608469                              Im Olymp der Ökonomen   \n",
       "161502056  9783496030478  Die Südseeblase in der englischen Kunst des 18...   \n",
       "...                  ...                                                ...   \n",
       "161277585  9781800713031  The Creative Industries and International Busi...   \n",
       "161277677  9781801171809                             Unravelling Travelling   \n",
       "162078229  9780745340173                        Work and the Carceral State   \n",
       "162745003  9781119745976                            The Strategyzer Box Set   \n",
       "163261204  9782343236698                            Travail, valeur et prix   \n",
       "\n",
       "                                                    subtitle  \\\n",
       "object_id                                                      \n",
       "160009340  Ökonomische und biographische Perspektiven auf...   \n",
       "159663468                                                      \n",
       "158945668                            The Innovator's Journey   \n",
       "161161334  Zur öffentlichen Resonanz wirtschaftspolitisch...   \n",
       "161502056                           Bilder einer Finanzkrise   \n",
       "...                                                      ...   \n",
       "161277585                                                      \n",
       "161277677  Uncovering Tourist Emotions through Autoethnog...   \n",
       "162078229                                                      \n",
       "162745003                                                      \n",
       "163261204  Reprise et clôture d'un débat centenaire (1885...   \n",
       "\n",
       "                                          contributor  \\\n",
       "object_id                                               \n",
       "160009340                              Judith Schmidt   \n",
       "159663468                      Valesca Maria Molinari   \n",
       "158945668  Jeanne Liedtka;Karen Hold;Jessica Eldridge   \n",
       "161161334                              Lino  Wehrheim   \n",
       "161502056                               Kathleen Löwe   \n",
       "...                                               ...   \n",
       "161277585        Nnamdi O. Madichie;Robert Ebo Hinson   \n",
       "161277677                                  Sue Beeton   \n",
       "162078229                                 Jon Burnett   \n",
       "162745003                       Alexander Osterwalder   \n",
       "163261204                    Adolfo Rodriguez-Herrera   \n",
       "\n",
       "                            publisher  \\\n",
       "object_id                               \n",
       "160009340                      Campus   \n",
       "159663468                Mohr Siebeck   \n",
       "158945668   Columbia University Press   \n",
       "161161334                Mohr Siebeck   \n",
       "161502056            Reimer, Dietrich   \n",
       "...                               ...   \n",
       "161277585  Emerald Publishing Limited   \n",
       "161277677  Emerald Publishing Limited   \n",
       "162078229                 Pluto Press   \n",
       "162745003                       Wiley   \n",
       "163261204        Editions L'Harmattan   \n",
       "\n",
       "                                                      series  \\\n",
       "object_id                                                      \n",
       "160009340                               Arbeit und Alltag 20   \n",
       "159663468            Rechtsordnung und Wirtschaftsgeschichte   \n",
       "158945668                Columbia Business School Publishing   \n",
       "161161334  Die Einheit der Gesellschaftswissenschaften im...   \n",
       "161502056                                                      \n",
       "...                                                      ...   \n",
       "161277585                                                      \n",
       "161277677                             The Tourist Experience   \n",
       "162078229                                                      \n",
       "162745003                             The Strategyzer series   \n",
       "163261204                                                      \n",
       "\n",
       "                         thesis edition_number     edition_text media_type  \\\n",
       "object_id                                                                    \n",
       "160009340                                    1                       ebook   \n",
       "159663468  Dissertationsschrift              1                       ebook   \n",
       "158945668                                    1                       ebook   \n",
       "161161334  Dissertationsschrift              1  1. Auflage 2021      ebook   \n",
       "161502056                                    1                       ebook   \n",
       "...                         ...            ...              ...        ...   \n",
       "161277585                                                        hardcover   \n",
       "161277677                                                        hardcover   \n",
       "162078229                                                        hardcover   \n",
       "162745003                                                        hardcover   \n",
       "163261204                                                        hardcover   \n",
       "\n",
       "           ...                                        description  \\\n",
       "object_id  ...                                                      \n",
       "160009340  ...  Saisonarbeit ist eine bedeutende Variable der ...   \n",
       "159663468  ...  Die Entwicklungen auf dem Mietwohnungsmarkt de...   \n",
       "158945668  ...  Individuals become design thinkers by experien...   \n",
       "161161334  ...  In Zeiten von Fake News und alternativen Wahrh...   \n",
       "161502056  ...  Als es 1720 in England zu einem Börsenkrach, d...   \n",
       "...        ...                                                ...   \n",
       "161277585  ...  The international business environment has und...   \n",
       "161277677  ...  The Tourist Experience is complex, intrinsical...   \n",
       "162078229  ...  'Revolutionises our understanding of the carce...   \n",
       "162745003  ...  The Authoritative Library for Business Leaders...   \n",
       "163261204  ...  La transformation des valeurs en prix est, sel...   \n",
       "\n",
       "                                                       cover predecessor  \\\n",
       "object_id                                                                  \n",
       "160009340  https://content.schweitzer-online.de/static/ca...               \n",
       "159663468  https://content.schweitzer-online.de/static/ca...               \n",
       "158945668  https://content.schweitzer-online.de/static/ca...               \n",
       "161161334  https://content.schweitzer-online.de/static/ca...               \n",
       "161502056  https://content.schweitzer-online.de/static/ca...               \n",
       "...                                                      ...         ...   \n",
       "161277585  https://content.schweitzer-online.de/static/ca...               \n",
       "161277677  https://content.schweitzer-online.de/static/ca...               \n",
       "162078229                                                                  \n",
       "162745003  https://content.schweitzer-online.de/static/ca...               \n",
       "163261204  https://content.schweitzer-online.de/static/ca...               \n",
       "\n",
       "          follower        last_modified       ID Field    L       Content  \\\n",
       "object_id                                                                   \n",
       "160009340           2021-11-18 18:24:57  62432.0  20.0    L  $$a160009340   \n",
       "159663468           2021-11-12 18:15:00  68009.0  20.0    L  $$a159663468   \n",
       "158945668           2021-11-24 17:20:02  62433.0  20.0    L  $$a158945668   \n",
       "161161334           2021-11-28 20:56:06  67902.0  20.0    L  $$a161161334   \n",
       "161502056           2021-11-19 18:27:38  62434.0  20.0    L  $$a161502056   \n",
       "...            ...                  ...      ...   ...  ...           ...   \n",
       "161277585           2021-11-19 18:09:58      NaN   NaN  NaN           NaN   \n",
       "161277677           2021-11-19 18:10:25      NaN   NaN  NaN           NaN   \n",
       "162078229           2021-11-29 17:22:03      NaN   NaN  NaN           NaN   \n",
       "162745003           2021-11-29 17:35:23      NaN   NaN  NaN           NaN   \n",
       "163261204           2021-11-28 18:15:25      NaN   NaN  NaN           NaN   \n",
       "\n",
       "                 ids  \n",
       "object_id             \n",
       "160009340  000062432  \n",
       "159663468  000068009  \n",
       "158945668  000062433  \n",
       "161161334  000067902  \n",
       "161502056  000062434  \n",
       "...              ...  \n",
       "161277585        NaN  \n",
       "161277677        NaN  \n",
       "162078229        NaN  \n",
       "162745003        NaN  \n",
       "163261204        NaN  \n",
       "\n",
       "[7050 rows x 26 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG-Datei für den Prozess, zur Dokumentation des Imports und als Kontrollanzeige hier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der enthaltenen Datensätze: 7050\n",
      "-------------------------\n",
      "Aleph-IDs anfangs 5974\n",
      "Aleph-IDs nach join 5970\n"
     ]
    }
   ],
   "source": [
    "x = df_update.shape[0]\n",
    "print('Anzahl der enthaltenen Datensätze:', x)\n",
    "#print('vorhandene ISBNs:', df_update[\"isbn_ean\"].shape[0])\n",
    "\n",
    "#z = df_alephIDs[\"ids\"].count()\n",
    "z= df_aleph_vorhanden[\"ids\"].count()\n",
    "y = df_update[\"ids\"].count()\n",
    "\n",
    "print('-------------------------')\n",
    "print('Aleph-IDs anfangs', z)\n",
    "print('Aleph-IDs nach join' , y)\n",
    "\n",
    "timestr = time.strftime('%d.%m.%Y - %H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('Logdatei PDA-Import vom ')\n",
    "    log.write(timestr)\n",
    "    log.write('\\n------------------------------------------\\n')\n",
    "    log.write('Gelieferte Datensätze:             ' + str(x))\n",
    "    log.write('Datensätze in Aleph:               ' + str(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 4 IDs verschwinden, sind dann nicht mehr vorhanden, die muss ich noch identifizieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemlösung der Aleph-Dubletten\n",
    "## nur in Aleph aber nicht mehr im Import, diese müssen identifiziert werden und ggf. gelöscht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alephIDs[\"object_id\"].value_counts\n",
    "\n",
    "df_aleph_einzel = df_alephIDs.drop_duplicates(\"object_id\", keep=False)  #Auslesen der Eintraege mit einzelner object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_doppelt = df_alephIDs.groupby(\"object_id\").filter(lambda g: (g.nunique() >1).any()) #Doppelte object_ids rausziehen\n",
    "df_aleph_single = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='first')  #von Dubletten 1. behalten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_vorhanden = df_aleph_einzel.append(df_aleph_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oi= df.set_index(\"object_id\")                       #object-ID zum Index für beide Datenframes\n",
    "df_aleph_oi = df_aleph_vorhanden.set_index(\"object_id\")\n",
    "df_update = df_oi.join(df_aleph_oi)                    #dann können sie zusammengeführt werden \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nächsten Schritt noch machen, um Daten zu löschen!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten\n",
    "\n",
    "<hr>\n",
    "\n",
    "### 1. Identifizierung von Titeln in Aleph, die nicht mehr im Datensatz sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# andere Kriterien!!! Alle, bei denen Titel leer ist z.b. oder geht das nicht, weil die gar nicht gemappt werden können bei Join???\n",
    "\n",
    "df_aleph_loeschen = df_aleph_doppelt.sort_values(by=[\"object_id\", \"ids\"], ascending =False).drop_duplicates(subset=[\"object_id\"], keep='last') #Datei der zu löschenden, Aleph-IDs noch rausschreiben und mit GES02 anreichern, dann kann es Eingabedatei zum Löschen sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/ges02_loeschen_1\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_loeschen.index:\n",
    "        fa.write(df_aleph_loeschen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_update.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Dublettencheck innerhalb der Buchhandelsdaten \n",
    "\n",
    "Aufgaben im Rahmen des Dublettencheck:\n",
    "1. Dublettenkontrolle anhand von Titel, Untertitel und Autor \n",
    "   - Zunächst Behebung der unsauberen Titel / Untertitel-Trennung für korrekteren Abgleich\n",
    "   - Trennung der Datensätze in Dubletten und \"Einzeltitel\"\n",
    "     - Einzeltitel werden direkt für Bestandsprüfung vorgemerkt\n",
    "     - Dubletten werden auf neueste Version reduziert und diese der Bestandsprüfungsdatei hinzugefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Dublettenkontrolle Anhand von Titel, Untertitel und Autor\n",
    "\n",
    "*Entfernen von Untertiteln aus der Titelspalte, Extrahieren von Untertiteln und Abgleich mit Untertitelspalte und Schreiben der vorhandenen Informationen in neue Untertitel-Spalte.   \n",
    "Durch diese Spalte werden ca. 1/3 mehr Dubletten erkannt, als ohne die Bereinigung. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update-Datei wieder umbenennung zu df, um Skript nicht komplett umschreiben zu  müssen\n",
    "df = df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu = df[\"title\"].str.split(':', n = 1, expand = True)  #Titel am 1. Doppelpunkt splitten und getrennt in neue Felder schreiben\n",
    "df[\"title_sep\"]= neu[0]\n",
    "df[\"subtitle_sep\"]= neu[1]\n",
    "\n",
    "df[\"subtitle_sep\"] = df[\"subtitle_sep\"].replace(np.nan, '', regex=True) #NaN-Werte stören, darum raus damit ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = np.where(df[\"subtitle\"] == df[\"subtitle_sep\"], '', df[\"subtitle\"])    # Abgleich - wenn in beiden das Gleiche steht, dann ursprüngliches \"Subtitle\"-Feld nehmen\n",
    "df[\"subtitle_comparison\"] = comparison  \n",
    "\n",
    "comparison2 = np.where(df[\"subtitle\"] < df[\"subtitle_sep\"], df[\"subtitle_sep\"], '') # Wenn nur in \"subtitle_sep\" Infos stehen, diese übernehmen, das ist noch nicht ganz sauber, da hier manchmal anderes steht als in \"subtitle\"\n",
    "df[\"subtitle_comparison2\"] = comparison2 \n",
    "\n",
    "df[\"subtitle_all\"] = df[\"subtitle_comparison\"]+df[\"subtitle_comparison2\"]          # Beide Informationen in neuer Subtitle-Spalte zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"short_title\"] = df[\"title_sep\"] + ' ' + df[\"subtitle_all\"] + ' / ' + df[\"contributor\"]  # aus den bereinigten Daten einen Kurztitel erzeugen, der dann für den Dublettencheck verwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dubletten = df.groupby(\"short_title\").filter(lambda g: (g.nunique() >1).any()) # schreibt alle mehrfach vorhandenen Titel in ein eigenes Datenframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dubl_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep='first')   # sortiert Dubletten nach Jahr und schreibt den jeweils ersten (= neuesten) Eintrag in neues Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohne_dubletten = df.drop_duplicates(\"short_title\", keep=False)       #durch \"\"keep=False\" werden alle nicht-Dubletten rausgezogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dubl_nicht_einspielen = df_dubletten.sort_values(by=[\"short_title\", \"publication_year\"], ascending =False).drop_duplicates(subset=[\"short_title\"], keep=False)\n",
    "\n",
    "#Kontrolle, ob es in Dubletten Titel gibt, die schon in Aleph sind\n",
    "df_dubl_nicht_einspielen[\"ids\"].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_einspielen = df_ohne_dubletten.append(df_dubl_einspielen)                    # die ausgewählten Dubletten und alle Nicht-Dubletten werden in ein Datenframe zusammengeführt\n",
    "df_einspielen.reset_index(inplace=True)                                         # für weitere Bearbeitung index-reset nötig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Überblick zu den Daten und Trennen in \"in Aleph vorhanden\" und \"neu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zum Einspielen: 6217\n",
      "Davon in Aleph: 5740\n",
      "Neue Titel: 477\n"
     ]
    }
   ],
   "source": [
    "m = df_einspielen.shape[0]\n",
    "n = df_einspielen[\"ids\"].count()\n",
    "print(\"Zum Einspielen:\", m)\n",
    "print(\"Davon in Aleph:\", n)\n",
    "print(\"Neue Titel:\", m-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph = df_einspielen.dropna(subset=['ids'])\n",
    "df_in_aleph.reset_index(inplace=True)\n",
    "df_in_aleph.shape[0]\n",
    "\n",
    "# diese müssen nur am Ebx-Bestand geprüft werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ganz_neu = df_einspielen[df_einspielen['ids'].isnull()]\n",
    "\n",
    "df_ganz_neu.reset_index(inplace=True)\n",
    "df_ganz_neu.shape[0]\n",
    "\n",
    "# diese müssen am Ges und Ebx Bestand geprüft werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bestandsabfragen\n",
    "\n",
    "\n",
    "\n",
    "*URLs für die Abfrage über den X-Server unseres Bibliothekssystems werden erzeugt und über die ISBN eine Abfrage auf Bestand gemacht. Die Abfrage funktioniert nur für zugelassene IPs (darum oben die Prüfung).  \n",
    "Für die Abfrage in unseren Bestand ist die ISBN sehr gut, da in den Titeldaten alle im Buch befindlichen ISBNs - auch die anderer Ausgabeformen - mit übernommen sind. Beim MPG-Ebooks Katalog handelt sich um Daten von Verlagen, die sich in ihrer Qualität und Informationsumfang sehr unterscheiden. Hier wird noch zu prüfen sein, inwieweit ein anderer Abfragemechansimus gewählt werden muss.*  \n",
    "\n",
    "### 1. Ganz neue Titel  \n",
    "\n",
    "= df_ganz_neu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ges\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ges_link{x}\".replace('ges_link','http://aleph.mpg.de/X?op=find&base=ges01&request=IBS='))\n",
    "\n",
    "#Es funktionierte nicht, dass die URLs an die vorhandenen ISBNs einfach so angefügt werden, darum der Workaround mit einem Platzhalter, der sich dann über replace vom richtigen Link überschreiben ließ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### URLs für Ebooks-Katalog erzeugen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ganz_neu[\"url_ebx\"] = df_ganz_neu[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n",
    "\n",
    "##### Zunächst für die Daten des MPIfG \n",
    "\n",
    "*Vorgehensweise: Abfrage und Sammeln der Antworten in einer Datei, diese Antworten werden dann in Ausdrücke \"übersetzt\" - \"vorhanden\" und \"neu\" und diese Daten in eine Spalte ins Dataframe zur weiteren Auswertung übertragen.   \n",
    "Schwierigkeit hier war, die Sammlung der Antworten zu den einzelnen Titeln, um sie in das Datenframe einzuspielen. Der störende XML-Header der Antworten wird erst gar nicht in die Datei geschrieben.   *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses', 'w') as fn:  \n",
    "    for url in df_ganz_neu[\"url_ges\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses', 'r') as f:\n",
    "    with open('./input/server_responses_transfered', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf = pd.read_fwf('./input/server_responses_transfered', names=[\"Abfrage_ges\"])\n",
    "df_result = pd.concat([df_ganz_neu, df_fwf], axis=1)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Randnotiz: \n",
    "    Bei 2400 Titels brauchte der Abgleich ca 350 Sekunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 477 \n",
      "Anzahl der Antworten vom Server:    477\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind, wird mittelfristig rausfallen\n",
    "x = df_ganz_neu.shape[0]\n",
    "y = df_fwf.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datenabgleich mit dem Bestand des MPG Ebooks-Katalog\n",
    "\n",
    "*Vorgehensweise analog Bestandsabfrage MPI.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx', 'w') as fn:  \n",
    "    for url in df_result[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx = pd.read_fwf('./input/server_responses_transfered_ebx', names=[\"Abfrage_ebx\"])\n",
    "df_result_neu = pd.concat([df_result, df_fwf_ebx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 477 \n",
      "Anzahl der Antworten vom Server:    477\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_neu.shape[0]\n",
    "y = df_fwf_ebx.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Titel in Aleph vorhanden\n",
    "\n",
    "= df_in_aleph\n",
    "\n",
    "*Hier reicht Abgleich mit Ebooks, das erworbene Bücher manuell gelöscht werden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph[\"url_ebx\"] = df_in_aleph[\"isbn_ean\"].apply(lambda x: f\"ebx_link{x}\".replace('ebx_link', 'http://aleph.mpg.de/X?op=find&base=ebx01&request=IBN='))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abfragen beim Server\n",
    "\n",
    "##### bei diesen Daten nur bei den Ebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./input/server_responses_ebx2', 'w') as fn:  \n",
    "    for url in df_in_aleph[\"url_ebx\"]:\n",
    "        reply = requests.get(url).text\n",
    "        a = reply.replace('<?xml version = \"1.0\" encoding = \"UTF-8\"?>', '') \n",
    "        fn.write(a)\n",
    "\n",
    "with open('./input/server_responses_ebx2', 'r') as f:\n",
    "    with open('./input/server_responses_transfered_ebx2', 'w') as fr:\n",
    "        for line in f:\n",
    "            if 'empty' in line:\n",
    "                fr.write('neu\\n')\n",
    "            elif 'no_records' in line:\n",
    "                fr.write('vorhanden\\n')\n",
    "\n",
    "df_fwf_ebx2 = pd.read_fwf('./input/server_responses_transfered_ebx2', names=[\"Abfrage_ebx\"])\n",
    "df_result_in_aleph = pd.concat([df_in_aleph, df_fwf_ebx2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der eingelesenen Datensätze: 5740 \n",
      "Anzahl der Antworten vom Server:    5740\n"
     ]
    }
   ],
   "source": [
    "#Kontrollabfrage, ob für alle Titel auch Treffer da sind\n",
    "x = df_result_in_aleph.shape[0]\n",
    "y = df_fwf_ebx2.shape[0]\n",
    "print('Anzahl der eingelesenen Datensätze:', x, '\\nAnzahl der Antworten vom Server:   ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Exportvorbereitungen \n",
    "\n",
    "\n",
    "### 1. Neue Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neu_nicht_einspielen = df_result_neu.drop(df_result_neu[(df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu')].index) \n",
    "# alle Titel rausholen, die in einer der beiden Datenbanken vorhanden waren, diese werden mit den vorhandenen aus Update unten in Excel geschrieben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Einspielen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_neu_einspielen = df_result_neu.loc[((df_result_neu[\"Abfrage_ebx\"]== 'neu') & (df_result_neu[\"Abfrage_ges\"] == 'neu'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In Aleph vorhandene Titel\n",
    "\n",
    "*Hier ist der Fall: was im Ebooks-Katalog vorhanden ist, muss gelöscht werden*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_nicht_einspielen = df_result_in_aleph.drop(df_result_in_aleph[(df_result_in_aleph[\"Abfrage_ebx\"]== 'neu')].index) #alles, was schon im Ebook-Katalog ist, diese Titel müssen gelöscht werden\n",
    "df_in_aleph_nicht_einspielen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen der Datei zum Löschen der Titel\n",
    "\n",
    "with open(\"./output/ges02_loeschen_2\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_nicht_einspielen.index:\n",
    "        fa.write(df_in_aleph_nicht_einspielen[\"ids\"][i]+'GES02'+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Statistik und Kontrolle zusammenführen aller Titel, die nicht eingespielt werden und Ausgabe in einer CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gesamt_nicht_einspielen = df_in_aleph_nicht_einspielen.append(df_neu_nicht_einspielen)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlsxwriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-3282cb520507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_gesamt_nicht_einspielen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_gesamt_nicht_einspielen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"url_ebx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"url_ges\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cover\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"title_sep\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subtitle_comparison\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subtitle_comparison2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subtitle_all\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subtitle_sep\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# unnötige Spalten entfernen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf_gesamt_nicht_einspielen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./output/Vorhandene_Titel_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'xlsxwriter'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#lokal funktioniert das schreiben zu Excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#df_neu_nicht_einspielen.to_csv('./output/Vorhandene_Titel_'+date+'.csv')   #Variante für die Ausführung in JuypterLab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2184\u001b[0m             \u001b[0minf_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minf_rep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2185\u001b[0m         )\n\u001b[1;32m-> 2186\u001b[1;33m         formatter.write(\n\u001b[0m\u001b[0;32m   2187\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2188\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[1;31m# abstract class 'ExcelWriter' with abstract attributes 'engine',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[1;31m# 'save', 'supported_extensions' and 'write_cells'  [abstract]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m             writer = ExcelWriter(  # type: ignore[abstract]\n\u001b[0m\u001b[0;32m    816\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, **engine_kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     ):\n\u001b[0;32m    176\u001b[0m         \u001b[1;31m# Use the xlsxwriter module as the Excel writer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mxlsxwriter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorkbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xlsxwriter'"
     ]
    }
   ],
   "source": [
    "date = time.strftime(\"%Y_%m_%d\")                                              # Zeit erfassen für Dateibenennung\n",
    "\n",
    "df_gesamt_nicht_einspielen[\"object_id\"] = df_gesamt_nicht_einspielen.object_id.astype(str)  # wandelt die spalte von Int64 zu Object um, so dass es in Excel korrekt eingelesen wird\n",
    "df_gesamt_nicht_einspielen[\"isbn_ean\"] = df_gesamt_nicht_einspielen.isbn_ean.astype(str)\n",
    "df_gesamt_nicht_einspielen = df_gesamt_nicht_einspielen.drop(columns=[\"url_ebx\", \"url_ges\", \"cover\", \"title_sep\", \"subtitle_comparison\", \"subtitle_comparison2\", \"subtitle_all\", \"subtitle_sep\"]) # unnötige Spalten entfernen\n",
    "\n",
    "df_neu_nicht_einspielen.to_csv('./output/Vorhandene_Titel_'+date+'.csv')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Extraktion der Titel zum Updaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update = df_result_in_aleph.loc[(df_result_in_aleph[\"Abfrage_ebx\"]== 'neu')] # prüfen, ob es wirklich ein Datenupdate gab, sonst nicht neu einspielen??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### Für die Logdatei Ermittlung verschiedener Zahlen und hier zur direkten Ansicht ausgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kleine Statistik:\n",
      "===================================== \n",
      "Gelieferte Datensätze:            7050 \n",
      "------------------------------------- \n",
      "Sätze ohne Dubletten:             5428 \n",
      "    Dubletten:        1622 \n",
      "    Davon zum Einspielen:         789 \n",
      "Am Bestand abzugleichen:          6217 \n",
      "---------------------------------------------------------------- \n",
      "Titel, die schon in Aleph sind:                            5740 \n",
      "    Davon zum updaten:                                     5735 \n",
      "    Davon aus Aleph zu löschen:                            5 \n",
      "Titel, die komplett neu sind:                              477 \n",
      "    Davon zum Einspielen:                                  267 \n",
      "---------------------------------------------------------------- \n",
      "Vorhandene Titel, die nicht in Aleph exportiert werden:    215 \n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Kontrollmechanismus, ob für alle Titel auch Treffer da sind\n",
    "x = df_update.shape[0]  #Ursprungszahl, eigentlich oben schon berechnet ...\n",
    "a = df_ohne_dubletten.shape[0]\n",
    "c = df_dubletten.shape[0]\n",
    "b = df_dubl_einspielen.shape[0] #Auswahl der neuen Treffer\n",
    "#von oben:\n",
    "m = df_einspielen.shape[0]\n",
    "n = df_einspielen[\"ids\"].count()\n",
    "o = df_in_aleph_update.shape[0]\n",
    "\n",
    "y = df_gesamt_nicht_einspielen.shape[0] #vorhandene von ganz neuen\n",
    "z = df_aleph_neu_einspielen.shape[0]       #neu ermitteln aus neuen\n",
    "d = a + b\n",
    "print('Kleine Statistik:\\n=====================================',\n",
    "    '\\nGelieferte Datensätze:           ', x, \n",
    "    '\\n-------------------------------------',\n",
    "    '\\nSätze ohne Dubletten:            ', a,\n",
    "    '\\n    Dubletten:       ', c, \n",
    "    '\\n    Davon zum Einspielen:        ', b,\n",
    "    '\\nAm Bestand abzugleichen:         ', m,\n",
    "    '\\n----------------------------------------------------------------',\n",
    "    '\\nTitel, die schon in Aleph sind:                           ', n,\n",
    "    '\\n    Davon zum updaten:                                    ', o,\n",
    "    '\\n    Davon aus Aleph zu löschen:                           ', n-o,\n",
    "    '\\nTitel, die komplett neu sind:                             ', m-n,\n",
    "    '\\n    Davon zum Einspielen:                                 ', z,\n",
    "    '\\n----------------------------------------------------------------',    \n",
    "    '\\nVorhandene Titel, die nicht in Aleph exportiert werden:   ', y,\n",
    "    '\\n----------------------------------------------------------------',)\n",
    "\n",
    "#hier entsprechende Einträge für die Log-Datei\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:\n",
    "    log.write(\"\\nSätze ohne Dubletten               \" + str(a))\n",
    "    log.write(\"\\n   Dubletten:              \" + str(c))\n",
    "    log.write(\"\\n   Auswahl zum Einspielen:         \" + str(b))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nDavon bereits in Aleph vorhanden:  \" + str(n))\n",
    "    log.write(\"\\nDavon komplett neu in Aleph:       \" + str(m-n))   \n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nTitel, die eingespielt werden:     \" + str(o+z))\n",
    "    log.write(\"\\nDavon als Update:                  \" + str(o))\n",
    "    log.write(\"\\nDavon ganz neu:                    \" + str(z))\n",
    "    log.write(\"\\n--------------------------------------------\\n\")\n",
    "    log.write(\"\\nTitel in Aleph zu löschen:         \" + str(n-o)) \n",
    "    log.write(\"\\nNicht eingespielt werden:          \" + str(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Exportdateien Aufbereiten\n",
    "\n",
    "#### Zielformat für das Einspielen in Aleph:\n",
    "\n",
    "    000000001 LDR   L -----nM2.01200024------h              \n",
    "    000000001 020   L $$a (object_id))\n",
    "    000000001 030   L $$aaz||rrrza||||\n",
    "    000000001 051   L $$am|||||||\n",
    "    000000001 070   L $$aSchweitzer\n",
    "    000000001 077   L $$aMonographie\n",
    "    000000001 078   L $$aSchweitzer\n",
    "    000000001 082   L $$azum Bestellen\n",
    "    000000001 100   L $$a (contributor_1)\n",
    "    000000001 104   L $$a (contributor_2)\n",
    "    000000001 108   L $$a (contributor_3)\n",
    "    000000001 331   L $$a (title_sep)\n",
    "    000000001 335   L $$a (subtitle_all)\n",
    "    000000001 403   L $$a (edition_number / edition_text)  #noch prüfen, was besser zu verwenden ist \n",
    "    000000001 419   L $$b (publisher) $$a (date_combined)\n",
    "    000000001 433   L $$a (pages)\n",
    "    000000001 451   L $$a (series)\n",
    "    000000001 520   L $$a (thesis)\n",
    "    000000001 540   L $$a (isbn_ean)\n",
    "    000000001 656   L $$a (cover)\n",
    "    000000001 750   L $$a (description)\n",
    "    000000001 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch        \n",
    "    \n",
    "Anmerkung zum Feld 655: die URL wird NACH dem Einspielen in Aleph mit der Datensatz-ID angereichert (siehe Juypter-Notebook \"Link-Anreicherung\"), um einen klaren Bestellink für den Kaufvorschlag zu haben\n",
    "\n",
    "*Hierfür werden immer die Feldbenennung und bestimmte Codierungen VOR den Inhalt - in Klammern de Bezeichnung der entsprechenden Spalte - gesetzt, bzw. erfoderliche Felder komplett neu hinzugefügt.   \n",
    "Am Anfang jeder Zeile braucht Aleph eine 9-Stellige eindeutige Zahl pro Titel.*   \n",
    "\n",
    "*Manchmal ließ sich der Inhalt einer Spalte direkt in die Datei schreiben, manchmal musst die Spalte zuvor über apply aufbereitet werden.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Aufbereiten der neuen Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen = df_aleph_neu_einspielen           #zur vereinfachten Wiederverwertung des alten Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"020\"] = df_aleph_einspielen[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_aleph_einspielen[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung der Personendaten\n",
    "\n",
    "*Da bis zu 3 Personen in einer Spalte zu finden sind, werden diese im Discovery nicht getrennt suchbar, darum werden sie gesplittet. Für die Dublettenkontrolle hat sich das als irrelevant erwiesen, darum erfolgt dieser Schritt erst hier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = df_aleph_einspielen[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= person[0]\n",
    "df_aleph_einspielen[\"contributor_2\"]= person[1]\n",
    "df_aleph_einspielen[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_aleph_einspielen[\"contributor_1\"]= df_aleph_einspielen[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_2\"]= df_aleph_einspielen[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_aleph_einspielen[\"contributor_3\"]= df_aleph_einspielen[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besondere Aufbereitung des Erscheinungsdatum und Erscheinungsjahres\n",
    "\n",
    "*In der Auswahl unserer Titel befinden sich auch im Erscheinen befindliche Titel der kommenden Monate. Diese Information möchten wir gerne im Discovery sichtbar machen. Hierfür bleibt uns nur Aleph-Feld 419c, das dem Erscheinungsjahr vorbehalten ist.   \n",
    "Wunsch ist es: Wenn des Erscheinungsdatum weiter als 10 Tage weg vom heutigen Datum ist, soll das komplette Datum angezeigt werden, ansonsten nur das Erscheinungsjahr.*\n",
    "\n",
    "Zur Umsetzung muss die Spalte \"publication_date\" in ein Datum verwandelt werden und nach den genannten Kriterien unterschiedlich angezeigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = int(time.strftime('%Y%m%d'))\n",
    "df_aleph_einspielen[\"coming_soon\"] = np.where(df_aleph_einspielen[\"publication_date\"].astype(int) > today+10, df_aleph_einspielen[\"publication_date\"], np.nan) #zieht die über 10 Tage raus, brauchen nan für Umwandlung in Datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"publication_date_soon\"] = df_aleph_einspielen[\"coming_soon\"].astype(str).str.replace('00','01')\n",
    "#df_aleph_einspielen[\"publication_date_soon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"year\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=0,stop=4)    #einfaches Zerlegen in die Datumsbestandteile und anschließendes Zusammenfügen\n",
    "df_aleph_einspielen[\"month\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=4,stop=6)\n",
    "df_aleph_einspielen[\"day\"] = df_aleph_einspielen[\"publication_date_soon\"].astype(str).str.slice(start=6,stop=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"full_coming_soon\"] = df_aleph_einspielen[\"year\"]+'-'+df_aleph_einspielen[\"month\"]+'-'+df_aleph_einspielen[\"day\"]\n",
    "df_aleph_einspielen[\"coming_soon\"] = df_aleph_einspielen[\"full_coming_soon\"].astype(str).str.replace('nan--','') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25               \n",
       "71               \n",
       "108              \n",
       "141              \n",
       "163              \n",
       "          ...    \n",
       "471              \n",
       "472              \n",
       "473              \n",
       "475    2021-12-30\n",
       "476              \n",
       "Name: coming_soon, Length: 267, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_einspielen[\"coming_soon\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25           2021\n",
       "71           2021\n",
       "108          2021\n",
       "141          2021\n",
       "163          2021\n",
       "          ...    \n",
       "471          2021\n",
       "472          2021\n",
       "473          2021\n",
       "475    2021-12-30\n",
       "476          2021\n",
       "Name: date_combined, Length: 267, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aleph_einspielen[\"published\"] = np.where(df_aleph_einspielen[\"coming_soon\"] == '', df_aleph_einspielen[\"publication_year\"], '') #Auslesen und Kombinieren der Daten\n",
    "df_aleph_einspielen[\"date_combined\"] = df_aleph_einspielen[\"published\"]+df_aleph_einspielen[\"coming_soon\"]\n",
    "df_aleph_einspielen['date_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"419b\"] = df_aleph_einspielen[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_aleph_einspielen[\"419c\"] = df_aleph_einspielen[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_aleph_einspielen[\"419\"] = df_aleph_einspielen[\"419b\"]+df_aleph_einspielen[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen[\"403\"] = df_aleph_einspielen[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_aleph_einspielen[\"433\"] = df_aleph_einspielen[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_aleph_einspielen[\"451\"] = df_aleph_einspielen[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_aleph_einspielen[\"520\"] = df_aleph_einspielen[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_aleph_einspielen[\"540\"] = df_aleph_einspielen[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_aleph_einspielen[\"656\"] = df_aleph_einspielen[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_aleph_einspielen[\"750\"] = df_aleph_einspielen[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Für das Durchzählen der Titel braucht es eine neue Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier entsteht eine neue Spalte mit Zahlen ab 1 durchgehend gezählt, die für den korrekten Import der Daten in Aleph nötig ist\n",
    "x = df_aleph_einspielen.shape[0]   \n",
    "df_aleph_einspielen[\"id\"] = range(1,x+1)                                                       #Notwendig ist die Zählung ab 1, da Aleph sonst nicht korrekt einließt\n",
    "df_aleph_einspielen[\"id\"] = df_aleph_einspielen[\"id\"].apply(lambda x: f\"{x:09d}\")              #Die Zahl muss 9-Stellig aufgefüllt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Vorbereitungen abgeschlossen, jetzt das Schreiben der Datei im Aleph-Sequential-Format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./output/pda_ges02\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_aleph_einspielen.index:\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' LDR   L -----nM2.01200024------h'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"020\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 030   L $$aaz||rrrza||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 051   L $$am|||||||m|||||||'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 077   L $$aMonographie'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 078   L $$aSchweitzer'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 082   L $$azum Bestellen'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 100   L $$a'+df_aleph_einspielen[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 104   L $$a'+df_aleph_einspielen[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 108   L $$a'+df_aleph_einspielen[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 331   L $$a'+df_aleph_einspielen[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 335   L $$a'+df_aleph_einspielen[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"403\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"419\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"433\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"451\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"520\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"540\"][i]+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' 655   L $$zOrder me$$umailto:bib@mpifg.de?subject=Bestellwunsch'+'\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_aleph_einspielen[\"id\"][i]+' '+df_aleph_einspielen[\"750\"][i]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Daten für Update\n",
    "\n",
    "*es wird anhand der Spalte \"last_modified\" geprüft, ob die Titel seit dem letzten Einspielen ein Update erfahren haben*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-21 14:19:10.045322\n"
     ]
    }
   ],
   "source": [
    "tday = datetime.datetime.now()\n",
    "td = datetime.timedelta(days = 10)\n",
    "u = tday - td\n",
    "df_in_aleph_update['timespan'] = u                                                           # Einfügen einer Spalte mit Datum vor 10 Tagen als Basis für Abfrage zu Update-Notwendigkeit\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update['last_modified'] = pd.to_datetime(df_in_aleph_update.last_modified)          #Umwandlung, da Spalteninhalt object ist\n",
    "df_in_aleph_update['last_modified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       20211118\n",
       "1            NaN\n",
       "2       20211119\n",
       "3       20211019\n",
       "4            NaN\n",
       "          ...   \n",
       "5735         NaN\n",
       "5736    20211119\n",
       "5737         NaN\n",
       "5738         NaN\n",
       "5739         NaN\n",
       "Name: update_true, Length: 5735, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in_aleph_update['update_true'] = np.where(df_in_aleph_update['last_modified'] < df_in_aleph_update['timespan'] , df_in_aleph_update[\"mod_date\"], np.nan) #Abgleich der beiden Datumsspalten zur Entscheidung, ob Update nötig ist\n",
    "df_in_aleph_update['update_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true = df_in_aleph_update[df_in_aleph_update['update_true'].notna()]  # damit habe ich alle Titel, die tatsächlich verändert wurden\n",
    "#df_in_aleph_update_true.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen der Update-Export-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_aleph_update_true[\"020\"] = df_in_aleph_update_true[\"object_id\"].apply(lambda x: f\"020   L $$a{x}\") \n",
    "del df_in_aleph_update_true[\"object_id\"]                                                                      #um das df nicht unnötig anwachsen zu lassen, jeweils alte Spalte löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aufbereitung Personendaten\n",
    "\n",
    "person = df_in_aleph_update_true[\"contributor\"].str.split(';', expand=True)                         #Für saubere Daten, die Autorenangabe splitten und in getrennte Felder schreiben\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= person[0]\n",
    "df_in_aleph_update_true[\"contributor_2\"]= person[1]\n",
    "df_in_aleph_update_true[\"contributor_3\"]= person[2]\n",
    "\n",
    "df_in_aleph_update_true[\"contributor_1\"]= df_in_aleph_update_true[\"contributor_1\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_2\"]= df_in_aleph_update_true[\"contributor_2\"].replace(np.nan, '', regex=True)\n",
    "df_in_aleph_update_true[\"contributor_3\"]= df_in_aleph_update_true[\"contributor_3\"].replace(np.nan, '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             2021\n",
       "2             2021\n",
       "3             2021\n",
       "5             2021\n",
       "7             2021\n",
       "           ...    \n",
       "5729    2021-12-20\n",
       "5731          2021\n",
       "5733          2021\n",
       "5734          2021\n",
       "5736          2021\n",
       "Name: date_combined, Length: 2384, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aufbereitung Erscheinungsdatum \n",
    "\n",
    "df_in_aleph_update_true[\"coming_soon\"] = np.where(df_in_aleph_update_true[\"publication_date\"].astype(int) > today+10, df_in_aleph_update_true[\"publication_date\"], np.nan)\n",
    "\n",
    "df_in_aleph_update_true[\"publication_date_soon\"] = df_in_aleph_update_true[\"coming_soon\"].astype(str).str.replace('00','01')\n",
    "\n",
    "df_in_aleph_update_true[\"year\"] = df_in_aleph_update_true[\"publication_date_soon\"].astype(str).str.slice(start=0,stop=4)    #einfaches Zerlegen in die Datumsbestandteile und anschließendes Zusammenfügen\n",
    "df_in_aleph_update_true[\"month\"] = df_in_aleph_update_true[\"publication_date_soon\"].astype(str).str.slice(start=4,stop=6)\n",
    "df_in_aleph_update_true[\"day\"] = df_in_aleph_update_true[\"publication_date_soon\"].astype(str).str.slice(start=6,stop=8)\n",
    "\n",
    "df_in_aleph_update_true[\"full_coming_soon\"] = df_in_aleph_update_true[\"year\"]+'-'+df_in_aleph_update_true[\"month\"]+'-'+df_in_aleph_update_true[\"day\"]\n",
    "df_in_aleph_update_true[\"coming_soon\"] = df_in_aleph_update_true[\"full_coming_soon\"].astype(str).str.replace('nan--','') \n",
    "\n",
    "df_in_aleph_update_true[\"published\"] = np.where(df_in_aleph_update_true[\"coming_soon\"] == '', df_in_aleph_update_true[\"publication_year\"], '') #Auslesen und Kombinieren der Daten\n",
    "df_in_aleph_update_true[\"date_combined\"] = df_in_aleph_update_true[\"published\"]+df_in_aleph_update_true[\"coming_soon\"]\n",
    "df_in_aleph_update_true['date_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bearbeitung der Felder\n",
    "\n",
    "df_in_aleph_update_true[\"419b\"] = df_in_aleph_update_true[\"publisher\"].apply(lambda x: f\"419   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"419c\"] = df_in_aleph_update_true[\"date_combined\"].apply(lambda x: f\"$$c{x}\")                  \n",
    "\n",
    "df_in_aleph_update_true[\"419\"] = df_in_aleph_update_true[\"419b\"]+df_in_aleph_update_true[\"419c\"]                           #Für die Korrekte Eingabe brauche ich Verlag und Jahr in einer Spalte\n",
    "\n",
    "df_in_aleph_update_true[\"403\"] = df_in_aleph_update_true[\"edition_text\"].apply(lambda x: f\"403   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"433\"] = df_in_aleph_update_true[\"pages\"].apply(lambda x: f\"433   L $$b{x}\")\n",
    "df_in_aleph_update_true[\"451\"] = df_in_aleph_update_true[\"series\"].apply(lambda x: f\"451   L $$b{x}\") \n",
    "df_in_aleph_update_true[\"520\"] = df_in_aleph_update_true[\"thesis\"].apply(lambda x: f\"520   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"540\"] = df_in_aleph_update_true[\"isbn_ean\"].apply(lambda x: f\"540   L $$a{x}\") \n",
    "df_in_aleph_update_true[\"656\"] = df_in_aleph_update_true[\"cover\"].apply(lambda x: f\"656   L $$u{x}\") \n",
    "df_in_aleph_update_true[\"750\"] = df_in_aleph_update_true[\"description\"].apply(lambda x: f\"750   L $$a{x}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben der Ausgabedatei, hier kleine Unterschiede zu den neuen Titeln. Vorhandene ids und bestimmte Felder können nicht verändert sein, brauchen also nicht übernommen zu werden.\n",
    "\n",
    "with open(\"./output/pda_update\", \"w\", encoding=\"utf-8\") as fa:  #durch das Encoding hier, kommen Sonderzeichen richtig rüber\n",
    "    for i in df_in_aleph_update_true.index:\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 100   L $$a'+df_in_aleph_update_true[\"contributor_1\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 104   L $$a'+df_in_aleph_update_true[\"contributor_2\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 108   L $$a'+df_in_aleph_update_true[\"contributor_3\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 331   L $$a'+df_in_aleph_update_true[\"title_sep\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' 335   L $$a'+df_in_aleph_update_true[\"subtitle_all\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"403\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"419\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"433\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"451\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"520\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"540\"][i]+'\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"656\"][i]+'$$3Cover\\n')\n",
    "        fa.write(df_in_aleph_update_true[\"ids\"][i]+' '+df_in_aleph_update_true[\"750\"][i]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschließende Dinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abschließender Eintrag in Log-Datei\n",
    "endtime = time.strftime('%H:%M')\n",
    "\n",
    "with open ('./log/pda_import_log.txt', 'a') as log:                                                  # Da diese Log-Datei nicht unmittelbar gebraucht wird, hier fortlaufendes Schreiben in eine Datei\n",
    "    log.write('\\n                                     beendet ')\n",
    "    log.write(endtime)\n",
    "    log.write(\"\\n============================================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopien bestimmter Daten zur Einsicht bzw. für Prüfzwecke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aleph_einspielen.to_csv('./output/Eingespielte_Titel_'+date+'.csv') \n",
    "df_in_aleph_update_true.to_csv('./output/Update_Titel_'+date+'.csv')\n",
    "\n",
    "df_in_aleph_nicht_einspielen.to_csv('./output/Aleph_loeschen_ebx_vorh_'+date+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Folgende Schritte müssen mit den Dateien ausgeführt werden: \n",
    "\n",
    "\n",
    "1. Einspielen der Datei pda_ges01 als neue Titel in Aleph, hier dann auch Export der urls und Anreicherung mit der Aleph-ID mittels \"mailto_link_skript.ipynb\"\n",
    "2. Einspielen der DAtei pda_update als \"Änderungen bestehender Datensätze in Aleph\"\n",
    "3. Einspielen und löschen der Titel die in ges02_loeschen_1 und ges02_loeschen_2 vorhanden sind"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1999307f9afbc7e27238c8b437c01929975b7965fdd2ec967692fa451253dbf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "4fac973d8b48a5fcf37e7d133428a31fb47ebbd054f5d1feed8c0da486f2af46"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
